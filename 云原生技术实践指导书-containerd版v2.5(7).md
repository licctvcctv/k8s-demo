**第 1 页**

郭  勇

哈尔滨工业大学计算学部软件学院

# 云原生技术实践指导书

---
**第 2 页**

1

1 云原生技术实践基本信息 ......................................................................................................................... 1

1.1 课程目的 .............................................................................................................................................. 1

1.2 要求 ...................................................................................................................................................... 1

1.2.1 题目要求及参考题目 .................................................................................................................. 1

1.2.2 实验要求 ...................................................................................................................................... 3

1.3 实践环境及知识要求 .......................................................................................................................... 3

1.3.1 实践环境 ...................................................................................................................................... 4

1.3.2 编程语言： .................................................................................................................................. 4

1.3.3 相关工具 ...................................................................................................................................... 4

1.3.4 技术栈 .......................................................................................................................................... 4

1.4 实践内容及学时安排 .......................................................................................................................... 5

1.5 基本概念 .............................................................................................................................................. 6

1.5.1 云原生的定义：Cloud Native ..................................................................................................... 6

1.5.2 云原生技术 .................................................................................................................................. 6

2 微服务技术实践 ......................................................................................................................................... 7

2.1 基于SPRINTCLOUD 微服务框架的项目开发 ..................................................................................... 7

2.1.1 本实践课推荐的微服务架构 ....................................................................................................... 7

2.1.2 系统基本框架 .............................................................................................................................. 8

2.1.3 创建SpringCloud 的聚合项目 .................................................................................................... 9

2.2 数据库的创建 .................................................................................................................................... 22

2.2.1 在微服务架构中的分库............................................................................................................. 22

2.2.2 创建数据库及表 ........................................................................................................................ 23

2.3 EUREKA 实现服务注册与发现 ........................................................................................................... 23

2.3.1 为什么要使用服务注册............................................................................................................. 23

2.3.2 Eureka 基本架构： ..................................................................................................................... 24

2.4 基于FEIGN 的服务调用 .................................................................................................................... 26

2.4.1 Feign 简介 ................................................................................................................................... 26

2.4.2 基于OpenFeign 的微服务项目的组成及调用关系 ................................................................. 27

2.4.3 创建服务提供者product-service ............................................................................................... 27

2.4.4 创建服务使用者product-client ................................................................................................. 35

2.4.5 项目测试 .................................................................................................................................... 40

2.5 基于OPENFEIGN 的负载均衡............................................................................................................ 43

2.5.1 负载均衡简介 ............................................................................................................................ 43

2.5.2 负载均衡测试 ............................................................................................................................ 43

2.5.3 负载均衡策略 ............................................................................................................................ 47

---
**第 3 页**

2

2.6 基于OPENFEIGN 的熔断处理............................................................................................................ 47

2.6.1 编写熔断处理代码 .................................................................................................................... 48

2.6.2 测试熔断机制 ............................................................................................................................ 50

2.7 GATEWAY API 网关的实现 ................................................................................................................ 51

2.7.1 API 网关 ...................................................................................................................................... 51

2.7.2 API 网关Gateway 核心概念 ...................................................................................................... 52

2.7.3 Gateway 的实现 .......................................................................................................................... 53

2.7.4 Gateway 过滤器 .......................................................................................................................... 57

2.8 CONFIG 实现配置中心（选做内容） ................................................................................................ 60

2.8.1 配置中心简介 ............................................................................................................................ 60

2.8.2 SpringCloud Config 配置中心 ................................................................................................... 62

3 云原生环境构建 ....................................................................................................................................... 67

3.1 基本概念 ............................................................................................................................................ 68

3.2 基本环境准备 .................................................................................................................................... 68

3.2.1 虚拟机及操作系统安装............................................................................................................. 68

3.2.2 系统设置 .................................................................................................................................... 69

3.3 容器运行时及工具安装(每个节点都装) .......................................................................................... 74

# 3.3.1 安装containerd（所有机器都装，新版本不再使用docker） ............................................... 74

3.3.2 crictl 工具配置 ............................................................................................................................ 75

3.3.3 buildkit 安装 ................................................................................................................................ 75

3.3.4 nerdctl 安装和使用 ..................................................................................................................... 77

3.3.5 命令对照表 ................................................................................................................................ 78

3.4 KUBERNETES 安装 ............................................................................................................................... 80

3.4.1 什么是Kubernetes ..................................................................................................................... 80

3.4.2 本实验中的系统拓扑结构......................................................................................................... 80

3.4.3 安装kubernetes（所有机器都装） .......................................................................................... 81

3.4.4 初始化master 节点（只在主节点Master 上运行） ............................................................... 82

3.4.5 将从节点加入集群（从节点worker1-2 上运行） .................................................................. 84

3.4.6 安装网络插件calico（Master 上安装） .................................................................................. 85

# 3.5 K3S 安装（如果按“3.4.KUBERNETES 安装”进行了安装则跳过此节） .......................................... 91

3.5.1 设置主机名 ................................................................................................................................ 91

3.5.2 配置并更新源 ............................................................................................................................ 91

# 3.5.3 关闭防火墙、交换区、优化内核（所有机器） ..................................................................... 92

3.5.4 开始安装k3s .............................................................................................................................. 93

4 容器及编排技术实践 ............................................................................................................................... 95

---
**第 4 页**

3

4.1 如何部署微服务 ................................................................................................................................ 96

4.2 MYSQL 集群部署 ................................................................................................................................ 96

# 4.2.1 在Kubernetes 下使用容器安装MySQL 数据库的步骤.......................................................... 96

4.2.2 NFS 安装 .................................................................................................................................... 97

4.2.3 MySql 集群安装 ....................................................................................................................... 100

4.3 微服务的环境变量配置文件的编写 .............................................................................................. 105

4.3.1 环境变量配置文件的区分....................................................................................................... 105

4.3.2 项目启动时如何选择配置文件 ............................................................................................... 106

4.3.3 修改微服务配置文件 .............................................................................................................. 106

4.4 微服务打包成JAR 文件 .................................................................................................................. 108

4.4.1 为什么将微服务打包成jar ..................................................................................................... 108

4.4.2 Idea 中打包jar 的基本步骤 ..................................................................................................... 109

4.5 微服务镜像的创建及保存 .............................................................................................................. 109

4.5.1 创建镜像仓库 .......................................................................................................................... 109

4.5.2 创建微服务镜像 ...................................................................................................................... 109

4.5.3 镜像推送到远端镜像仓库........................................................................................................ 111

4.5.4 用脚本批量创建并推送镜像 .................................................................................................... 112

4.6 微服务的编排及部署 ....................................................................................................................... 114

4.6.1 部署文件的语法格式 ............................................................................................................... 114

4.6.2 k8s 中的一些重要概念 .............................................................................................................. 115

4.6.3 创建命名空间 ........................................................................................................................... 118

4.6.4 项目microservice-demo 微服务的编排及部署 ....................................................................... 119

5 K8S 可视化工具部署 ............................................................................................................................. 132

5.1 下载 KUBERNETES DASHBOARD 部署文件并进行配置 ................................................................. 132

5.2 部署 KUBERNETES DASHBOARD ...................................................................................................... 133

5.3 获取令牌并登录DASHBOARD ......................................................................................................... 134

5.3.1 获取访问令牌 .......................................................................................................................... 134

5.3.2 登录Dashboard ........................................................................................................................ 134

# 6 云原生应用的自动化部署和持续集成/持续交付（CI/CD） ............................................................. 136

6.1 GITOPS 及JENKINS 简介 ................................................................................................................... 136

6.2 创建代码仓库 .................................................................................................................................. 138

6.3 部署JENKINS .................................................................................................................................... 138

6.3.1 安装Jenkins ............................................................................................................................. 139

6.3.2 修改Jenkins 启动用户 ............................................................................................................ 141

6.3.3 登录Jenkins ............................................................................................................................. 142

---
**第 5 页**

4

6.3.4 安装插件并修改升级点........................................................................................................... 143

6.4 JENKINS 其它配置 ............................................................................................................................. 149

6.4.1 Jdk 安装及配置 ......................................................................................................................... 149

6.4.2 Maven 安装及配置 ................................................................................................................... 151

6.4.3 配置 Jenkins Agent（可选的） .............................................................................................. 153

6.5 创建并配置任务 .............................................................................................................................. 154

6.5.1 添加凭据 .................................................................................................................................. 154

6.5.2 配置Jenkins 连接 K8S ........................................................................................................... 156

6.5.3 创建新任务 .............................................................................................................................. 159

6.5.4 配置任务 .................................................................................................................................. 160

7 前端开发及部署（选做） ..................................................................................................................... 172

7.1 VUE 项目的创建 ............................................................................................................................... 172

7.1.1 环境准备 .................................................................................................................................. 173

7.1.2 vue 项目的创建及项目结构 ..................................................................................................... 174

7.2 VUE 项目在云原生环境中的部署 .................................................................................................... 180

7.2.1 前端的部署方式的选择........................................................................................................... 180

7.2.2 前端非容器方式部署 .............................................................................................................. 180

7.2.3 前端容器方式的部署 .............................................................................................................. 183

8 附录......................................................................................................................................................... 185

8.1 K8S 卸载重装流程 ........................................................................................................................... 185

8.2 批量导出镜像脚本EXPORT_IMAGES.SH： ...................................................................................... 185

8.3 出错的解决 ...................................................................................................................................... 185

8.3.1 删除k8s .................................................................................................................................... 185

8.3.2 Jenkins 中Git 拉取代码出错 ................................................................................................... 185

8.3.3 常用命令 .................................................................................................................................. 186

8.3.4 镜像仓库操作 .......................................................................................................................... 187

8.3.5 项目基本结构 .......................................................................................................................... 187

8.3.6 k8s 安装时如何重新生成证书？ ............................................................................................ 190

8.3.7 YAML 常见资源对象的字段 ................................................................................................... 190

8.3.8 Feign 服务调用时的错误 ......................................................................................................... 192

8.3.9 Could not get lock /var/lib/dpkg/lock-frontend. It is held by proc ............................................. 194

8.3.10 本指导书所用的镜像版本..................................................................................................... 194

8.3.11 基础镜像设置......................................................................................................................... 196

8.3.12 Eureka 启动异常，提示Freemarker Template Error ............................................................. 196

8.3.13 Ubuntu 关机异常慢的解决方法 ............................................................................................. 196

---
**第 6 页**

5

8.3.14 springcloud gateway 找不到服务 .......................................................................................... 197

8.3.15 如何暂时停止k8s .................................................................................................................. 197

8.3.16 如何重启动k8s ...................................................................................................................... 197

8.3.17 Jenkins 忘记密码|密码重置 ................................................................................................... 198

8.4 其它 .................................................................................................................................................. 199

8.4.1 清除镜像 .................................................................................................................................. 199

8.4.2 buildkitd 帐号配置 .................................................................................................................... 199

8.4.3 nerdctl 登录帐号 ...................................................................................................................... 200

# 8.4.4 配置buildkitd 配置文件，添加镜像仓库使用http 访问 ...................................................... 200

8.4.5 containerd 配置国内镜像源 ...................................................................................................... 200

8.4.6 排错命令 .................................................................................................................................. 201

8.4.7 MySQL8 报错：Public Key Retrieval is not allowed ............................................................... 202

8.4.8 清除pod 日志（不太好用） ................................................................................................... 202

8.4.9 pom 标签 ................................................................................................................................... 202

8.4.10 配置文件属性含义 ................................................................................................................ 203

8.4.11 控制器的作用......................................................................................................................... 203

# 8.4.12 系统启动时很慢提示A start job is running for wait for network to be Configured ............. 204

8.4.13 错误现像ContainerCreating .................................................................................................. 204

8.4.14 IOException : DER input, Integer tag error ............................................................................. 204

8.4.15 镜像源 .................................................................................................................................... 205

---
**第 7 页**

1

# 1 云原生技术实践基本信息

# 1.1 课程目的

通过本课程学习使学生系统的了解云原生技术及相关概念，掌握云原生技术的应用及基础设施

的构成。掌握云原生技术的构成要素，了解云原生技术的最佳实践。

能够运用云原生技术，构建基于云基础设施的应用程序。能够运用自动化工具分析、部署、运

行和维护应用程序。具备使用软件工程工具的能力。

具有工程意识、系统观念和团队精神，能够运用恰当的云原生技术解决工程实际问题。

培养同学的云原生思维和工程实践能力，提高其在云原生时代的竞争力和应用能力。使同学可

以在实际工作中更加熟练地使用云原生技术，提升技术水平和职业价值。

# 1.2 要求

本课程以一个软件项目为主线，覆盖本课云原生技术所有知识点。

# 1.2.1 题目要求及参考题目

同学可以自拟题目，或选择下面的任一题目。要求必须采用云原生技术进行系统开发，不要求

实现系统全部功能，实现1 个用户管理相关的微服务（完成登录鉴权）、和至少2 个业务相关的微服

务即可。

下面是一些备选题目：

# 1.2.1.1 学生课程管理系统

供参考的微服务如下：

学生信息管理微服务：负责学生的注册、登录、个人信息管理等操作。

课程信息管理微服务：负责课程的添加、删除、修改和查询等操作。

选课管理微服务：负责学生选课、退课、课程时间表等操作。

成绩管理微服务：负责录入和查询学生成绩，生成成绩单等操作。

教师管理微服务：负责教师的信息管理、课程分配等操作。

考勤管理微服务：负责学生考勤记录、请假管理等操作。

统计分析微服务：负责对学生选课情况、成绩分布等进行统计分析和报表生成

# 1.2.1.2 图书馆图书管理系统

供参考的微服务如下：

图书信息管理微服务：负责图书的添加、删除、修改和查询等操作。

用户管理微服务：负责用户的注册、登录、权限管理等操作。

借阅管理微服务：负责借阅图书、归还图书、逾期处理等操作。

图书搜索微服务：负责提供图书的搜索功能，包括关键字搜索、分类搜索等。

统计分析微服务：负责对图书借阅情况、用户行为等进行统计分析和报表生成。

推荐系统微服务：负责根据用户的借阅历史和兴趣推荐图书。

订单管理微服务：负责处理用户的购买图书订单、发货、退款等操作。

---
**第 8 页**

2

# 1.2.1.3 员工考勤管理系统

供参考的微服务如下：

员工信息管理微服务：负责员工的注册、登录、个人信息管理等操作。

考勤记录管理微服务：负责记录员工的考勤情况，包括打卡记录、请假记录等。

考勤统计微服务：负责对员工的考勤数据进行统计分析，生成考勤报表等。

请假管理微服务：负责员工请假申请、审批流程等操作。

加班管理微服务：负责员工加班申请、审批流程等操作。

考勤通知微服务：负责发送考勤相关通知和提醒给员工和管理人员。

考勤权限管理微服务：负责管理员工的考勤权限，包括打卡权限、假期权限等。

# 1.2.1.4 客户关系管理系统

供参考的微服务如下：

客户信息管理微服务：负责客户的注册、登录、个人信息管理等操作。

销售管理微服务：负责记录客户的订单、销售情况等。

客户服务微服务：负责处理客户的投诉、建议等客户服务工作。

营销活动微服务：负责管理客户营销活动、促销活动等。

客户反馈微服务：负责收集和分析客户反馈信息，提供给相关部门参考。

客户分析微服务：负责对客户行为、购买习惯等进行统计分析和报表生成。

# 1.2.1.5 仓库库存管理系统

供参考的微服务如下：

商品管理微服务：负责商品的添加、删除、修改和查询等操作。

入库管理微服务：负责记录商品的入库操作、库存更新等。

出库管理微服务：负责记录商品的出库操作、库存更新等。

库存查询微服务：负责提供库存查询功能，包括实时库存查询、历史库存记录等。

库存预警微服务：负责监控库存情况，提供库存预警功能，及时提醒库存不足或过剩情况。

库存统计微服务：负责对库存情况进行统计分析，生成库存报表等。

# 1.2.1.6 餐厅菜单管理系统

供参考的微服务如下：

菜品管理微服务：负责菜品的添加、删除、修改和查询等操作。

订单管理微服务：负责记录顾客的点菜情况、下单、结账等。

库存管理微服务：负责管理菜品的库存情况，包括库存预警、库存补货等。

菜品推荐微服务：负责根据顾客偏好和销售情况推荐菜品。

菜品评价微服务：负责顾客对菜品的评价和反馈收集。

菜品统计微服务：负责对菜品销售情况进行统计分析，生成销售报表等。

# 1.2.1.7 电影票务管理系统

供参考的微服务如下：

电影排片管理微服务：负责电影的排片安排、放映时间管理等。

---
**第 9 页**

3

订票管理微服务：负责顾客的选座订票、支付、取票等操作。

放映管理微服务：负责电影放映过程的管理和监控。

座位管理微服务：负责电影院座位的管理和维护。

电影票统计微服务：负责对电影票销售情况进行统计分析，生成销售报表等。

会员管理微服务：负责电影院会员的注册、积分管理、会员特惠等。

# 1.2.1.8 健身房会员管理系统

供参考的微服务如下：

会员信息管理微服务：负责会员的注册、登录、个人信息管理等操作。

课程预订微服务：负责会员预订健身课程、私教课程等。

器械预订微服务：负责会员预订健身器械、场地等设施。

会员健身数据微服务：负责记录会员的健身数据、健身计划等。

会员活动管理微服务：负责管理健身房的会员活动、促销活动等。

会员反馈微服务：负责收集和分析会员反馈信息，提供给相关部门参考。

# 1.2.1.9 旅行预订管理系统

供参考的微服务如下：

酒店预订微服务：负责旅行者预订酒店、民宿等住宿场所。

机票预订微服务：负责旅行者预订机票、航班信息查询等。

景点门票预订微服务：负责旅行者预订景点门票、导游服务等。

交通接送微服务：负责提供旅行者交通接送服务，包括租车、接送机等。

旅行行程管理微服务：负责旅行者行程安排、导游服务等。

旅行保险微服务：负责提供旅行保险服务、意外保障等。

# 1.2.1.10 人力资源管理系统

供参考的微服务如下：

员工信息管理微服务：负责员工的注册、登录、个人信息管理等操作。

招聘管理微服务：负责招聘流程管理、简历筛选、面试安排等。

培训管理微服务：负责员工培训计划、培训资源管理等。

绩效管理微服务：负责员工绩效考核、绩效评定等。

薪酬管理微服务：负责员工薪酬核算、薪资发放等。

福利管理微服务：负责员工福利管理、员工关怀等。

# 1.2.2 实验要求

以小组为单位（2-3 人一组）完成实践内容。

准时到达实验室，不迟到、不早退。

认真做实验课内容，积极回答问题，保持实验室秩序，不做与课程无关的事。

保持良好的心态，积极进取，独立思考，不断努力，。

按照云原生实践流程进行项目管理和软件开发，在实践中体验云原生技术特点和优势。

# 1.3 实践环境及知识要求

---
**第 10 页**

4

# 1.3.1 实践环境

推荐如下环境：

(1). 云平台：自己搭建三台虚拟机。

(2). 容器平台：containerd、Kubernetes 等

(3). 自动化部署工具：例如Jenkins、GitLab CI/CD 等

(4). 监控和日志系统：例如Prometheus（或Grafana、ELK Stack 等）

# 1.3.2 编程语言：

(1). 脚本语言：例如Bash、Python 等用于自动化部署和运维

(2). 编程语言：例如Java、Golang、Node.js 等用于开发云原生应用程序

(3). 基础设施编排语言：例如YAML、Helm 等用于定义和配置云原生应用的基础设施和资源

(4). 除了以上列举的环境和编程语言，还需要深入了解云原生技术的相关概念和原理，以及相

关工具和框架的使用方法。

同时，需要不断学习和实践，以适应云原生技术领域的快速发展和变化

# 1.3.3 相关工具

(1). 种集成开发环境（IDE）：如IntelliJ IDEA、Visual Studio Code、Eclipse 等，以及适用于特定

编程语言的工具和框架。

(2). 版本控制工具：例如Git、SVN 等，用于管理和协作开发团队的代码。

# 1.3.4 技术栈

项目开发技术栈指的是在软件项目开发过程中所采用的一系列技术、工具和框架的组合。这些

技术和工具涵盖了从前端用户界面开发到后端服务器端开发、数据库管理、测试和部署等各个方面。

一个典型的项目开发技术栈包括以下几个方面：

前端开发技术：包括HTML、CSS、JavaScript 等基础的Web 开发技术，以及现代的前端框架和

库如React、Angular、Vue.js 等。

后端开发技术：包括服务器端编程语言（如Java、Python、Node.js 等）、框架（如Spring Boot、

Django、Express 等）以及服务器端的数据库管理技术（如MySQL、MongoDB、PostgreSQL 等）。

数据存储和管理：包括数据库管理系统、缓存技术、数据存储方案等。

测试和部署：包括自动化测试工具、持续集成/持续部署工具、容器化技术等。

其他辅助工具和服务：例如版本控制工具（如Git）、项目管理工具（如Jira、Trello）、云服务提

供商（如AWS、Azure、Google Cloud）等。

选择合适的项目开发技术栈需要考虑项目的需求、团队成员的技能、项目规模和预算等因素。

不同的项目可能会选择不同的技术栈来满足特定的需求。同时，技术栈也会随着时间的推移而不断

演进和更新，因此需要根据最新的技术趋势和需求来选择合适的技术栈。

下面是一个云原生项目开发的技术栈：

---
**第 11 页**

5

图 1-1 云原生项目开发技术栈

# 1.4 实践内容及学时安排

1） 微服务技术实践（15 学时）

(1). 基于SprintCloud 的微服务框架开发。

(2). 云原生应用项目的服务开发（实现基于 Feign 的服务调用、基于 Feign 的负载均衡的功能、

实现基于 Feign 的服务熔断的功能）

(3). 云原生应用项目的服务的注册及发现

(4). 使用API 网关实现认证及服务的路由

2） 云原生环境的构建（3 学时）

(1). 创建Ubuntu22.0 的虚拟机（2-3 个）、每个虚拟机设置空间大小为50G，在一台虚拟机做为

Master 节点，另外机器做Worker 节点。

(2). 三台虚拟机上安装containerd。

(3). 正确安装k8s 1.28。

(4). 在主节点安装DashBoard

3） 容器编排技术实践（6 学时）

(1). 使用工具创建镜像

(2). 使用Kubernetes 进行服务编排

(3). 服务注册及发现组件的pod 的创建及部署

(4). 网关组件进行pod 的创建及部署

(5). 对具体业务相关服务进行pod 创建及部署

4） 云原生集群应用管理实践及系统验收（8 学时）

(1). 使用GitOps 工具进行CI/CD

(2). 实现应用发布部署的可观测

---
**第 12 页**

6

(3). 实现系统的可监测

# 1.5 基本概念

# 1.5.1 云原生的定义：Cloud Native

Cloud：表示应用程序位于云中，而不是传统的数据中心；

Native：表示应用程序从设计之初即考虑到云的环境，原生为云而设计

是一类技术的统称

通过云原生技术，我们可以构建出更易于弹性扩展的应用程序

这些应用可以被运行在不同的环境中

图 1-2 云原生技术架构

# 1.5.2 云原生技术

云原生技术是一种软件开发和部署的方法论，旨在实现应用的快速开发、部署、扩展和管理。

它利用云计算的特性和势，将应用程序和服务设计为可独立、可扩展、可移植和可弹性伸缩的微服

务，通过容器化、自动化和服务编排等技术，实现应用的快速交付和高效运维。

云原生技术的核心概念包括：

容器化：将应用和服务打包成独立的容器，包括应用代码、运行时环境和依赖项实现跨平台和

跨环境的运行。

自动化：利用自动化工具和平台，实现应用的自动署、配置、监控和扩展，减少人工干预和提

高效率。

微服务架构：将应用程序拆分为多个独立的服务，每个服务负责特定的功能模块，通过接口和

消息传递进行信，实现高内聚和低耦合。

服务编排：通过服务编工具，自动管理和调度多个容器和服务之间的交互和协作，实应用的弹

性伸缩、负载均衡和故恢复。

---
**第 13 页**

7

云原生技术能够帮助企业实现快速响应市场需求、弹性扩展应用、提高开发效率和降低运维成

本，已经成为现代软件开发和部署的趋势和标准。

图 1-3 云原生技术体系及管理要点

# 2 微服务技术实践

# 2.1 基于SprintCloud 微服务框架的项目开发

# 2.1.1 本实践课推荐的微服务架构

本实践课推荐使用SprintCloud的微服务框架，同学们也可以选择其它服务框架或其中某些组件。

基于SprintCloud 的微服务框架，可以使用Eureka、Feign 和Config 实现微服务的开发和使用。

下面是具体的步骤。

表 2-1 本实践中选用的组件

组件名称
作用
所属组件

Eureka
实现服务注册与发现(Netflix)
SpringCloud Netflix

Gateway
实现 API 网关
SpringCloud 原生

Ribbon
实现负载均衡（Feign 内置）
SpringCloud Netflix

Feign
实现基于 Feign 的服务调用
SpringCloud Netflix

Hystrix
实现服务熔断（Feign 内置）
SpringCloud Netflix

Config
实现Config 配置中心
SpringCloud 原生

Spring Cloud、netflex、SpringBoot 的版本的适配

Spring Cloud 与 Netflix 的组件以及SpringBoot 的版本通常是相关联的，但并不是完全一一对

应的。Spring Cloud 的不同版本会集成不同版本的 Netflix 组件，并且Spring Cloud 与SpringBoot 的

版本也有一定的对应关系，因此要注意版本的适配，以确保兼容性和功能完善性。因此，您可以查

看特定版本的 Spring Cloud 文档或官方发布说明，以了解其中包含的 Netflix 组件的版本信息，以

及与SpringBoot 版本的对应关系。

Spring Cloud 官网：https://spring.io/projects/spring-cloud/

---
**第 14 页**

8

Spring Boot 官网：https://spring.io/projects/spring-boot/

Spring cloud  netflix 官网：https://spring.io/projects/spring-cloud-netflix/

下面是一些版本的适配关系，最新版本的需要大家通过上面的官网查询：

⚫
Spring Cloud 版本与 Netflix 版本对应关系：

Spring Cloud 2020.0.x 对应 Netflix 2.2.x

Spring Cloud 2020.0.x 对应 Netflix 2.1.x

Spring Cloud 2019.0.x 对应 Netflix 2.0.x

Spring Cloud 2019.0.x 对应 Netflix 1.4.x

Spring Cloud 2019.0.x 对应 Netflix 1.3.x

⚫
Spring Cloud 版本与 Spring Boot 版本对应关系：

Spring Cloud 2020.0.x 对应 Spring Boot 2.4.x

Spring Cloud 2020.0.x 对应 Spring Boot 2.3.x

Spring Cloud 2019.0.x 对应 Spring Boot 2.2.x

Spring Cloud 2019.0.x 对应 Spring Boot 2.1.x

Spring Cloud 2019.0.x 对应 Spring Boot 2.0.x

# 2.1.2 系统基本框架

基于SprintCloud的微服务框架使用Eureka、Feign和Config进行微服务的开发和使用非常方便，

可以让开发者更加专注于业务逻辑的开发。

本课实践所用系统架构如图所示。

图 2-1 项目逻辑架构

---
**第 15 页**

9

下面详细说明具体过程。

# 2.1.3 创建SpringCloud 的聚合项目

使用IDEA 开发工具，创建SpringCloud 的基于Maven 的聚合项目，这种项目是在一个父项目

下管理多个子项目，这些子项目可以是微服务、模块、组件等，通过Maven 的聚合功能可以统一管

理这些子项目的依赖、版本、打包等配置。

# 2.1.3.1 聚合项目及其作用

创建聚合项目的目的是为了更好地管理和组织多个相关联的项目，可以统一管理它们的依赖关

系，统一打包发布，方便项目的构建和部署。同时，聚合项目也可以提高团队协作效率，统一管理

项目的版本和发布流程。另外，聚合项目还可以方便地进行跨项目的代码重用和共享，提高代码的

复用性和可维护性。

创建聚合工程的要点（重点理解）：

⚫
该聚合项目（父级项目、顶级项目）本身也是一个maven 项目，它必须有自己的pom

⚫
它的打包方式必须是：pom

⚫
引入新的元素：modules—module（模块：每个模块其实也是一个项目）

⚫
版本：聚合模块的版本要和被聚合模块的版本一致

⚫
relative path：每个module 名称都是一个当前pom 的相对目录

⚫
目录名称：为了方便快速定位，模块所处的目录应当与其artifactId 一致(maven 约定而不是

硬性要求)，总之，模块所处的目录必须要和聚合模块中的模块目录保持一致。

⚫
聚合模块减少的内容：聚合模块的内容仅仅是一个pom.xml 文件，它不包含src/main/java

和src/test/java 等目录，因为他只是用来将其他模块整合构建成一个整体的工具，本身并没

有实质的内容。

⚫
聚合模块和子模块的目录：它们可以是父子级，也可以是平行结构（推荐）。默认是父子级

结构，但是不建议使用，因为父子级是一种嵌套关系，子模块要建在父模块里面，维护起

来很容易紊乱；推荐使用平行结构，是因为维护起来一目了然，条理比较清晰，当然同时

要在pom 文件的中修改相应的目录路径配置。

⚫
如果聚合模块对某一个子模块进行了删除操作，那么一定要在聚合模块的pom.xml 文件中

的modules 选项中将对应的子模块删除掉。

# 2.1.3.2 聚合项目pom 结构

Maven 在设计时，借鉴了 Java 面向对象中的继承思想，提出了 POM 继承思想。当一个项目

包含多个模块时，可以在该项目中再创建一个父模块，并在其 POM 中声明依赖，其他模块的 POM

可通过继承父模块的 POM 来获得对相关依赖的声明。对于父模块而言，其目的是为了消除子模块

POM 中的重复配置，其中不包含有任何实际代码，因此父模块 POM 的打包类型（packaging）必须

是 pom。

---
**第 16 页**

10

图 2-2 聚合项目依赖关系示意图

当一个项目需要同时继承父工程的pom 并引入其他工程的依赖时，可以通过以下示例来说明。

假设有一个父工程A，它的pom.xml 文件如下：

<project>
<modelVersion>4.0.0</modelVersion>
<groupId>com.example</groupId>
<artifactId>parent-project</artifactId>
<version>1.0.0</version>
<packaging>pom</packaging>
<dependencies>
<!-- 父工程的依赖 -->
<dependency>
<groupId>org.springframework</groupId>
<artifactId>spring-core</artifactId>
<version>5.3.8</version>
</dependency>
</dependencies>
<modules>
<module>child-project</module>
</modules>
</project>

然后有一个子工程B，它的pom.xml 文件如下：

<project>
<modelVersion>4.0.0</modelVersion>
<parent>
<groupId>com.example</groupId>
<artifactId>parent-project</artifactId>
<version>1.0.0</version>
</parent>
<artifactId>child-project</artifactId>
<dependencies>
<!-- 引入其他工程的依赖 -->
<dependency>
<groupId>com.example</groupId>
<artifactId>other-project</artifactId>

---
**第 17 页**

11

<version>1.0.0</version>
</dependency>
</dependencies>
</project>

在上面的示例中，子工程B 通过<parent>标签继承了父工程A 的pom，并且在自己的
<dependencies>标签中引入了其他工程的依赖。这样子工程B 就同时继承了父工程的配置并引入了其
他工程的依赖。

下面具体讲解如何创建聚合项目，如何在聚合项目中创建微服务模块。

# 2.1.3.3 创建父项目

在微服务项目中，通常父项目中会包含一些共享的配置、依赖管理、插件配置等内容。

起动Idea ,依次点击：File->New->Project，如下图所示，选择项目类型，创建父项目。

图 2-3 创建项目

按着提示输入必要的信息，生成项目。作为父项目不需要编写业务代码，因此删除项目中的src。

# 2.1.3.4 配置父项目Pom 文件

父项目中的pom 内容一般包括：

公共的依赖管理和版本控制：父项目中可以定义和管理所有子项目共享的依赖库的版本，以确

---
**第 18 页**

12

保所有子项目使用相同版本的依赖。

公共的插件配置：父项目中可以定义和配置一些通用的插件，如编译插件、测试插件、打包插

件等，以确保所有子项目使用相同的构建规则。

公共的配置文件：父项目中可能包含一些通用的配置文件，如日志配置、数据库配置等，以确

保所有子项目使用相同的配置。

公共的工具类和代码库：父项目中可能包含一些通用的工具类和代码库，以便子项目之间进行

代码的共享和重用。

初始依赖 如下：

下面是父项目的pom 文件的内容。包含的子项目（即聚合项目中的编写的每个微服务或其它组

件）都列在 <modules> </modules>中，如下所示，目前只有一个eureka-service 子项目。

<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-
v4_0_0.xsd">
<modelVersion>4.0.0</modelVersion>
<groupId>cn.microservice-demo</groupId>
<artifactId>microservice-demo</artifactId>
<version>1.0.0</version>
<!--新增包含的子项目-->
<modules>
<module>eureka-service</module>
<module>product-service</module>
</modules>
<name>microservice-demo</name>
<packaging>pom</packaging>
<description>简单的微服务案例</description>
<properties>
<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
<project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
<maven.compiler.encoding>UTF-8</maven.compiler.encoding>
<springboot.version>2.0.9.RELEASE</springboot.version>
<springcloud.version>Finchley.SR2</springcloud.version>
<maven.compiler.source>1.8</maven.compiler.source>
<maven.compiler.target>1.8</maven.compiler.target>
<mybatis.version>2.0.1</mybatis.version>
<hutool.version>4.5.13</hutool.version>
</properties>
<profiles>
<profile>
<id>dev</id>
<activation>
<activeByDefault>true</activeByDefault>
</activation>
<properties>
<profileActive>dev</profileActive>
</properties>
</profile>

---
**第 19 页**

13

<profile>
<id>fat</id>
<properties>
<profileActive>fat</profileActive>
</properties>
</profile>
</profiles>
<!-- 管理依赖 -->
<dependencyManagement>
<dependencies>
<!-- spring Boot 依赖 -->
<dependency>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-dependencies</artifactId>
<version>${springboot.version}</version>
<type>pom</type>
<scope>import</scope>
</dependency>
<!-- spring Cloud 依赖 -->
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-dependencies</artifactId>
<version>${springcloud.version}</version>
<type>pom</type>
<scope>import</scope>
</dependency>
<!-- 统一依赖管理 -->
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-config-server</artifactId>
<version>2.0.2.RELEASE</version>
<type>pom</type>
<scope>import</scope>
</dependency>
<!--加入hystrix 的依赖 -->
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-netflix-hystrix</artifactId>
<version>2.0.2.RELEASE</version>
<type>pom</type>
<scope>import</scope>
</dependency>
<!-- spring cloud config 客户端 -->
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-config</artifactId>
<version>2.0.2.RELEASE</version>
<type>pom</type>
<scope>import</scope>

---
**第 20 页**

14

</dependency>
<!--spring boot web 模块 -->
<dependency>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-starter-web</artifactId>
<version>${springboot.version}</version>
<exclusions>
<!--排除tomcat 依赖 ,使用underom -->
<exclusion>
<artifactId>spring-boot-starter-tomcat</artifactId>
<groupId>org.springframework.boot</groupId>
</exclusion>
</exclusions>
</dependency>
</dependencies>
</dependencyManagement>
<dependencies>
<!--eureka 客户端 -->
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>
</dependency>
<!-- feignclient 远程调用 -->
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>
<!--Lombok -->
<dependency>
<groupId>org.projectlombok</groupId>
<artifactId>lombok</artifactId>
<scope>provided</scope>
</dependency>
</dependencies>
<build>
<finalName>${project.name}</finalName>
<resources>
<resource>
<directory>src/main/resources</directory>
<filtering>true</filtering>
</resource>
</resources>
<pluginManagement>
<plugins>
<plugin>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-maven-plugin</artifactId>
<version>${springboot.version}</version>
<executions>
<execution>
<goals>
<goal>repackage</goal>

---
**第 21 页**

15

</goals>
</execution>
</executions>
</plugin>
</plugins>
</pluginManagement>
<plugins>
<plugin>
<artifactId>maven-compiler-plugin</artifactId>
<version>3.8.0</version>
<configuration>
<target>${maven.compiler.target}</target>
<source>${maven.compiler.source}</source>
<encoding>UTF-8</encoding>
<skip>true</skip>
</configuration>
</plugin>
<plugin>
<groupId>pl.project13.maven</groupId>
<artifactId>git-commit-id-plugin</artifactId>
<version>2.2.5</version>
</plugin>
</plugins>
</build>
<repositories>
<!--阿里云主仓库，代理了maven central 和jcenter 仓库 -->
<repository>
<id>aliyun</id>
<name>aliyun</name>
<url>https://maven.aliyun.com/repository/public</url>
<releases>
<enabled>true</enabled>
</releases>
<snapshots>
<enabled>false</enabled>
</snapshots>
</repository>
<!--阿里云代理Spring 官方仓库 -->
<repository>
<id>spring-milestones</id>
<name>Spring Milestones</name>
<url>https://maven.aliyun.com/repository/spring</url>
<releases>
<enabled>true</enabled>
</releases>
<snapshots>
<enabled>false</enabled>
</snapshots>
</repository>
</repositories>
</project>

# 2.1.3.5 创建子项目Eureka

在一个父项目下管理多个子项目（Module），一般每个微服务对应一个子项目，下面以eureka 为

---
**第 22 页**

16

例创建微服务。

每个微服务项目通常包含以下内容：

(1). 微服务的具体实现代码：包括业务逻辑、数据访问、服务接口等。

(2). 服务配置文件：包括服务的配置信息，如端口号、数据库连接信息等。

(3). 服务的依赖管理：包括服务所依赖的第三方库和其他微服务的依赖管理。

(4). 服务的测试代码：包括单元测试、集成测试等。

Eureka 项目基本结构如下所示，其中省略了测试部分的代码。

图 2-4 eureka-service 微服务模块结构

下面是eureka-service 微服务的创建过程：

⚫
右键点击项目名称–>New–>Module

图 2-5 创建子项目（服务模块）

⚫
选中Spring Initializr–>Next

–>Group={主工程的GroupId}–>Aritifact={当前模块的ArtifactId}、

–>Next–>Next–>Finish

---
**第 23 页**

17

图 2-6 选择类型并输入信息

这样一个主项目和子项目就创建好了，可以删去多余的文件，比如父项目的src 文件夹可以删

除。

图 2-7 项目结构示意

---
**第 24 页**

18

# 2.1.3.6 配置子项目pom 文件

要让子项目使用父项目的POM 内容，可以在子项目的POM 文件中使用<parent>标签来引用父

项目的POM。具体步骤如下：

在子项目的POM 文件中，使用<parent>标签指定父项目的坐标（groupId、artifactId、version）：

<parent>
<groupId>com.example</groupId>
<artifactId>parent-project</artifactId>
<version>1.0.0</version>
</parent>

在父项目的POM 文件中定义需要共享的配置、依赖管理等内容，例如：

<project>
<groupId>com.example</groupId>
<artifactId>parent-project</artifactId>
<version>1.0.0</version>
<packaging>pom</packaging>
<!-- 共享的依赖管理 -->

<dependencies>
...
</dependencies>

<!-- 共享的插件配置 -->

<build>
<plugins>
...
</plugins>
</build>

<!-- 其他共享配置 -->

...
</project>

通过以上设置，子项目就可以继承父项目的配置、依赖管理等内容。当子项目构建时，Maven 会

自动使用父项目的POM 内容，从而实现共享配置和依赖管理。

下面是本例子中的eureka-service 的pom 文件的内容：

<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-
v4_0_0.xsd">
<modelVersion>4.0.0</modelVersion>
<parent>
<artifactId>microservice-demo</artifactId>
<groupId>cn.microservice-demo</groupId>
<version>1.0.0</version>
</parent>

---
**第 25 页**

19

<artifactId>eureka-service</artifactId>
<dependencies>
<!--SpringCloud eureka-server -->
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>
</dependency>
<!--web 模块 -->
<dependency>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-starter-web</artifactId>
</dependency>
<!--undertow 容器 -->
<dependency>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-starter-undertow</artifactId>
</dependency>
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-config</artifactId>
<exclusions>
<exclusion>
<groupId>*</groupId>
<artifactId>*</artifactId>
</exclusion>
</exclusions>
</dependency>
</dependencies>
<build>
<plugins>
<plugin>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-maven-plugin</artifactId>
</plugin>
</plugins>
</build>
</project>

# 2.1.3.1 编写配置文件application.yml

修改项目配置文件，添加相应内容，配置文件在eureka-service/src/resources 下面，默认生成的是

application.properties，可以使用该文件进行配置，也可以创建一个yaml 文件application.yml 进行配

置。

application.properties 和application.yml 都是用来配置应用程序属性的文件，application.properties

是基于键值对的属性文件，而application.yml 是基于YAML 格式的属性文件，使用缩进和冒号来表

示层级关系。这两种配置文件，选择使用哪种取决于个人偏好和项目需求。这里我们选用yaml 文件

进行配置。

---
**第 26 页**

20

application.yml 文件配置

server:
port: 8888
spring:
application:
name: eureka-server
eureka:
instance:
hostname: 127.0.0.1
prefer-ip-address: true
server:
enable-self-preservation: false
eviction-interval-timer-in-ms: 4000
wait-time-in-ms-when-sync-empty: 0
responseCacheUpdateIntervalMs: 1
use-read-only-response-cache: false
renewal-percent-threshold: 0.49
client:
register-with-eureka: true
serviceUrl:
defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/
fetch-registry: false

这个yaml 配置文件包含了一些配置信息，它们被分成了不同的部分。

# 1. server 部分包含了端口号的配置，指定了服务的端口号为8888。

# 2. spring部分包含了Spring应用的配置信息。application.name指定了应用的名称为eureka-server。

# 3. eureka 部分包含了Eureka 注册中心的配置信息。

⚫
instance 部分指定了当前实例的主机名为127.0.0.1，并且prefer-ip-address 设置为true。

⚫
client 部分指定了是否向Eureka 注册中心注册自己，并且设置了默认的服务URL 为

http://${eureka.instance.hostname}:${server.port}/eureka/。fetch-registry 设置为false，表示从不从注册

中心获取注册信息。

⚫
在该部分的server 下面为了使服务能尽快注册成功，加入了一些设置，否则默认情况下，

一般要启动30-40 秒后，服务才能被发现并使用。

总的来说，这个配置文件指定了应用的端口号、名称，以及与Eureka 注册中心相关的配置信息。

# 2.1.3.2 启动子项目

在主启动类上加上注解 @EnableEurekaServer

package cn.simplemicroservice.eurekaservice;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;
@SpringBootApplication
@EnableEurekaServer
public class EurekaServiceApplication {

---
**第 27 页**

21

public static void main(String[] args) {
SpringApplication.run(EurekaServiceApplication.class, args);
}
}

启动主配置类，查看输出窗口是否有错误，如果没有错误，则可以在浏览器中打开连接。

图 2-8 Eureka 微服务启动运程

下图是在浏览器中打开连接后显示的Eureka 的界面，由于在配置文件中设置了register-with-

eureka: true，所以看到Eureka 将自己也注册到服务器上了。

---
**第 28 页**

22

图 2-9 Eureka 登录界面

至此Eureka 创建完成，其它微服务可以按此步骤编写即可。

# 2.2 数据库的创建

为了更好的讲解微服务中的业务实现，我们创建一个简单的数据库，并输入一些数据。

# 2.2.1 在微服务架构中的分库

在微服务架构中，通常会采用分库的方式来使用数据库。这是因为微服务架构的特点决定了单

一数据库往往无法满足微服务系统的需求，而分库可以更好地支持微服务架构的弹性、可扩展性和

独立性。

以下是一些采用分库的方式来使用数据库的原因：

(1). 数据隔离：不同微服务之间的数据应该是隔离的，通过分库可以将不同微服务的数据存储

在不同的数据库中，避免数据泄露和冲突。

(2). 性能和扩展性：单一数据库可能无法支撑整个系统的高并发和大规模数据存储需求，通过

分库可以将数据分散存储，提高系统的性能和扩展性。

(3). 独立性：每个微服务都应该有自己的数据库，通过分库可以确保每个微服务都有独立的数

据存储，提高了微服务的独立性和自治性。

(4). 数据管理：通过分库可以更好地管理数据，例如可以根据业务需求对数据进行分片、分区

等操作，更好地支持数据的管理和维护。

需要注意的是，分库也会带来一些挑战，例如分布式事务、数据一致性、跨库查询等问题需要

---
**第 29 页**

23

特别注意和处理。因此，在采用分库的方式使用数据库时，需要综合考虑系统的需求和特点，合理

设计分库策略，同时也要考虑如何解决分库带来的一些挑战。

# 2.2.2 创建数据库及表

这里我们使用mysql 8.0.26 数据库，创建一个数据库和一个表，后面的微服务会用到它们，现在

创建好或者在后面编写product-services 微服务时再创建也可以。

数据库名：tb_product

表名：product

下面是创建数据库tb_product 及product 表的sql 语句。

create
CREATE DATABASE `tb_product`;
use  `tb_product`;
CREATE TABLE `product` (
`id` int NOT NULL AUTO_INCREMENT,

`product_name` varchar(100) DEFAULT NULL COMMENT '商品名称',

`price` double(15,3) DEFAULT NULL COMMENT '商品价格',

PRIMARY KEY (`id`)
)
ENGINE=InnoDB
AUTO_INCREMENT=5
DEFAULT
CHARSET=utf8mb4
COLLATE=utf8mb4_0900_ai_ci;

插入数据的语句：

INSERT INTO `tb_product`.`product` (`product_name`,`price`) VALUES ('上衣','100.00');

INSERT INTO `tb_product`.`product` (`product_name`,`price`) VALUES ('裤子','50.00');

INSERT INTO `tb_product`.`product` (`product_name`,`price`) VALUES ('毛衣','200.00');

INSERT INTO `tb_product`.`product` (`product_name`,`price`) VALUES ('帽子','30.00');

INSERT INTO `tb_product`.`product` (`product_name`,`price`) VALUES ('鞋','200.00');

# 2.3 Eureka 实现服务注册与发现

# 2.3.1 为什么要使用服务注册

前面我们已经创建了eureka-services 模块，现在我们具体说一下它的原理及实现细节。

在实际开发中，一个分布式系统中会有成千上万个微服务，此时微服务之间的调用就会出现一

些问题。比如：

服务消费者调用服务提供者时，请求url 中直接写死的。在服务不多时可以这样，但存在大量服

务时，手动维 护服务列表就会给维护和更新带来很大问题。

如果新增微服务时，如何及时通知其他微服务。微服务宕机后，如何及时下线。

微服务注册中心的出现，就是为了更好更方便的管理应用中的每一个服务，是各个分布式节点

之间的纽带。

---
**第 30 页**

24

图 2-10 Eureka 基本原理

三个角色之间的关系如下：

(1). 微服务（包括服务提供者、服务消费者）在启动时，将自己的地址等信息注册到注册中心，

注册中心存储这 些注册信息。

(2). 服务消费者从注册中心查询服务提供者的地址，并通过该地址调用服务提供者的接口。

(3). 各个微服务与注册中心使用一定机制（例如心跳）通信。如果注册中心与某微服务长时间

无法通信，就会注 销该服务实例。

(4). 微服务地址发生变化（例如服务增加或IP 变动等）时，会重新注册到注册中心。这样，服

务消费者就无需人工 修改提供者的地址了。

# 2.3.2 Eureka 基本架构：

Eureka Server：Eureka 服务端，提供服务注册和发现

Eureka Client：Eureka 客户端，包括服务提供者和服务消费者，当然服务提供者和服务消费者都

是相对的，服务提供者为了实现其服务也可能要调用其它服务此时它又是一个消费者。

Eureka 基本流程：

(1). 各个微服务启动时，会向Eureka Server 注册自己的信息（例如网络信息）， Eureka Server

会存储各个服务的信息；

(2). 微服务启动后，会周期性地向Eureka Server 发送心跳（默认周期为30 秒）以续约自己的信

息。如果Eureka Server 在一定时间内没有接收到某个微服务节点的心跳，Eureka Server 将会注销该

微服务节点（默认90 秒）；

(3). 每个Eureka Server 同时也是Eureka Client，多个Eureka Server 之间通过复制的方式完成服

务注册表的同步；

(4). Eureka Client 会缓存Eureka Server 中的信息。即使所有的Eureka Server 节点都宕掉，服务

消费者依然可以使用缓存中的信息找到服务提供者。

综上所述，Eureka 通过心跳检测做健康检查和客户端缓存等机制，提高了系统的灵活性、可伸

缩性和可用性。

# 2.3.2.1 修改续约时间

我们知道，微服务启动后，会周期性地向Eureka Server 发送心跳（默认周期为30 秒）以续约自

---
**第 31 页**

25

己的信息。如果Eureka Server 在一定时间内没有接收到某个微服务节点的心跳，Eureka Server 将会

注销该微服务节点（默认周期90 秒）；

在日志中可以看到续约时间：

# 启动心跳执行器：续订间隔为：30

Starting heartbeat executor: renew interval is: 30

如果认为默认周期时间过长，那么可以通过如下配置来修改续约时间：

eureka:
... ...
instance:

prefer-ip-address: true #使用ip 地址注册

instance-id: ${spring.cloud.client.ip-address}:${server.port}
lease-renewal-interval-in-seconds: 5 #续约时间间隔（秒）

lease-expiration-duration-in-seconds: 15 #续约到期时间（秒）

lease-renewal-interval-in-seconds：续约时间间隔（默认30 秒）。表示eureka client 发送心跳给server

端的频率。

lease-expiration-duration-in-seconds：续约到期时间（默认90 秒）。表示eureka server 至上一次收

到client 的心跳之后，等待下一次心跳的超时时间，在这个时间内若没收到下一次心跳，则将移除该

instance。

# 2.3.2.2 Eureka 自我保护机制

Eureka 自我保护机制：

默认情况下，Eureka 在90 秒内没有接收到某个微服务节点的心跳，就会认为该微服务节点已宕

机，并将其从 服务列表中剔除。

但是，当网络分区故障发生时，微服务与Eureka Server 之间无法正常通信，以上行为就会非常

危险：因为微服务本身是健康的，此时不应该注销这个微服务。

Eureka 通过自我保护机制来解决这个问题。当Eureka Server 的某节点发现在短时间内丢失过多

客户端时（可能发生了网络分区故障），Eureka 会将当前的实例注册信息保护起来，不在注销任何微

服务。

具体来说：如果在15 分钟内超过85%的客户端节点都没有正常的心跳，那么Eureka 就认为客

户端与注册中心出现了网络故障，Eureka Server 自动进入自我保护机制（在单机状态下很容易进入

自我保护机制）。此时， Eureka 会将当前的实例注册信息保护起来，同时提示这个警告。

当网络故障修复后，该Eureka Server 节点会自动退出自我保护机制。

---
**第 32 页**

26

图 2-11 Eureka 告警示意图

在开发过程中，我们可以不设置自我保护机制，通过设置eureka.server.enable-self-preservation 来

关闭自我保护机制。

#eureka server 配置

eureka:
client:
...
server:

enable-self-preservation: false #关闭自我保护机制

重新启动，就会显示自我保护机制已经关闭

图 2-12 重启动后的信息提示

自我保护模式是一种应对网络异常的安全保护措施。它的架构哲学是：宁可同时保留所有微服

务（健康的微服务和不健康的微服务都会保留），也不盲目注销任何健康的微服务。使用自我保护模

式，可以让Eureka 集群更加 的健壮、稳定。

# 2.4 基于Feign 的服务调用

# 2.4.1 Feign 简介

Feign 是Netflix 开发的声明式，模板化的HTTP 客户端。它可以使我们快捷、优雅的调用HTTP

API。它使得编写Web 服务客户端变得更加简单。在Spring Cloud 中，Feign 可以与Eureka、Ribbon

等组件集成，实现服务的发现和负载均衡。通过Feign，开发人员可以使用简单的接口来调用远程服

务，而不需要关心底层的HTTP 通信细节。Feign 还支持对请求和响应进行自定义，以及对请求参数

的编码和解码。因此，Feign 在微服务架构中被广泛应用于服务之间的通信。

---
**第 33 页**

27

如果使用的RestTemplate 实现REST API 调用，代码大致如下：

restTemplate.getForObject("http://provider-server/user/getUserById/"+userId,CommonResult.class);

由代码可知，我们是使用拼接字符串的方式构造URL 的。此种方式，在有多个请求参数时就非

常麻烦。此时可以使用Feign 来帮助我们更快捷、更优雅的调用HTTP API。

在Spring Cloud 中使用Feign 进行远程调用时，能获得与调用本地方法一样的编码体验，开发者

完全感知不到这是远程方法，更感知不到这是个HTTP 请求。

SpringCloud 对Feign 进行了增强，也就是OpenFeign。OpenFeign 支持SpringMVC 注解，并整

合了Ribbon 和Eureka，从而让Feign 也具有负载均衡的功能，后面我们将使用OpenFeign。

# 2.4.2 基于OpenFeign 的微服务项目的组成及调用关系

在前后端分离模式下，前端通过HTTP 请求调用后端的API 接口，然后后端的Controller 会调

用FeignClient 来实现对其他服务的调用。简而言之，前端通过HTTP 请求调用后端的API，后端通

过FeignClient 来调用其他服务。这种方式能够将服务调用的逻辑封装在后端，使得前端只需要关注

与后端的交互，而后端负责处理服务之间的调用。

调用关系是这样的：

⚫ 前端发起请求到后端服务的API 接口。

⚫ 后端服务的Controller 接收到请求后，调用FeignClient 的接口方法。

⚫ FeignClient 根据接口定义和注解，将请求发送到其他服务的控制器。

⚫ 其他服务的控制器接收到请求后，再调用具体的服务进行处理。

即前端发送请求到后端服务的API 接口，然后后端服务调用FeignClient，再由FeignClient 调用

其他服务的控制器，最终其他服务的控制器再调用具体的服务

因此对于每个微服务，需要编写具体的业务服务，还要编写服务的API 接口。

# 2.4.3 创建服务提供者product-service

# 2.4.3.1 创建项目

按前面2.1 讲的方法创建一个“产品服务”的微服务,模块名为product-service。在该微服务中使用

OpenFeign 实现微服务调用。

如下图所示创建子项目，并输入相应信息，其中Group 与父项目保持一致。

---
**第 34 页**

28

图 2-13 创建ProductService 微服务项目

product-service 中的pom 参考用前面的2.1 中子项目的pom 文件进行改写，并加入OpenFeign

的依赖。然后在父项目pom 中的<modules>加入新创建模块。

<!--新增包含的子项目-->
<modules>
<module>eureka-service</module>
<module>product-service</module>
</modules>

# 2.4.3.2 项目基本结构

项目结构如下，后面将分别说明每个类的创建过程。

---
**第 35 页**

29

在product-service 项目中的pom 文件中添加OpenFeign 的依赖：

<!--SpringCloud 整合的openFeign -->

<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>

product-service 项目完整的pom 文件如下：

<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-
v4_0_0.xsd">
<modelVersion>4.0.0</modelVersion>
<parent>
<artifactId>microservice-demo</artifactId>
<groupId>cn.microservice-demo</groupId>
<version>1.0.0</version>
</parent>
<artifactId>product-service</artifactId>
<dependencies>
<!--web 模块 -->
<dependency>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-starter-web</artifactId>
</dependency>
<!--undertow 容器 -->

---
**第 36 页**

30

<dependency>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-starter-undertow</artifactId>
</dependency>
<!--SpringCloud 整合的openFeign -->
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-config</artifactId>
<exclusions>
<exclusion>
<groupId>*</groupId>
<artifactId>*</artifactId>
</exclusion>
</exclusions>
</dependency>
<dependency>
<groupId>org.mybatis.spring.boot</groupId>
<artifactId>mybatis-spring-boot-starter</artifactId>
<version>1.1.1</version>
</dependency>
<dependency>
<groupId>org.mybatis</groupId>
<artifactId>mybatis</artifactId>
<version>3.5.10</version>
</dependency>
<dependency>
<groupId>org.mybatis</groupId>
<artifactId>mybatis-spring</artifactId>
<version>2.0.1</version>
</dependency>
<dependency>
<groupId>mysql</groupId>
<artifactId>mysql-connector-java</artifactId>
</dependency>
</dependencies>
<build>
<plugins>
<plugin>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-maven-plugin</artifactId>
</plugin>
</plugins>
</build>
</project>

# 2.4.3.3 编辑主类ProductServiceApplication

它使用了MapperScan 和SpringBootApplication 注解。

---
**第 37 页**

31

@MapperScan 注解的作用是告诉Spring 在指定的包中寻找MyBatis 的映射器接口，并将它们注

册为Spring Bean，以便在需要时可以通过依赖注入来使用这些映射器接口。

@SpringBootApplication 注解的作用是标识这是一个Spring Boot 应用程序的入口类，并且会自

动配置Spring 应用程序上下文、Spring MVC 和其他必要的配置。它还会自动扫描指定包及其子包中

的组件，以便将它们注册为Spring Bean，并启用自动配置功能。scanBasePackages 参数用于指定需

要扫描的包。

在main 方法中，使用SpringApplication.run 方法启动了ProductServiceApplication 这个Spring

Boot 应用程序。

package microservicedemo.productservice;
import org.mybatis.spring.annotation.MapperScan;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
@SpringBootApplication(scanBasePackages = {"microservicedemo.productservice.controller"})
@MapperScan(basePackages="microservicedemo.productservice.mapper")
public class ProductServiceApplication {
public static void main(String[] args) {
SpringApplication.run(ProductServiceApplication.class, args);
}
}

# 2.4.3.4 创建控制器类ProductController

在微服务架构中，控制器类通常是指用于处理和响应外部请求的类，主要负责接收请求、调用

相应的服务逻辑（业务逻辑）、并返回处理结果的类。控制器类一般是微服务中的入口点，用于处理

来自外部的HTTP 请求、消息队列的消息等。

控制器类的主要功能和特点包括：

(1). 请求处理：负责接收外部请求，解析请求参数，验证请求合法性，并调用相应的服务逻辑

进行处理。

(2). 业务逻辑调用：调用微服务内部的业务逻辑服务，执行具体的业务逻辑操作，例如数据查

询、数据处理、数据更新等。

(3). 结果返回：将处理结果封装成响应数据，返回给请求方，通常以HTTP 响应或者消息队列

消息的形式返回结果。

(4). 异常处理：处理请求过程中可能发生的异常，进行异常捕获、处理和返回错误信息。

在典型的基于HTTP 的微服务架构中，控制器类通常基于RESTful 风格的API 设计，使用HTTP

协议进行请求和响应。控制器类可以通过注解或者配置路由规则来定义请求的处理逻辑，接收参数，

并调用相应的服务进行处理。

按下面步骤创建一个控制器类ProductController。

---
**第 38 页**

32

图 2-14 new 一个类

图 2-15 输入类名

代码如下，其中注解的作用如下：

@AllArgsConstructor：这是Lombok 库中的注解，用于自动生成一个包含所有参数的构造函数。

它可以减少编写重复代码的工作量，特别是在需要为类的所有字段创建构造函数时非常有用。

@RestController：这是Spring 框架中的注解，用于标识一个类为RESTful 风格的控制器。它结

合了@Controller 和@ResponseBody 注解的功能，表示这个类的所有方法的返回值都会直接写入

HTTP 响应体中，而不会通过视图解析器进行渲染。

@Slf4j：这是Lombok 库中的注解，用于自动生成一个名为"log"的日志记录器变量，可以在类

中直接使用这个变量进行日志记录，而无需手动创建Logger 对象。这可以简化日志记录的代码，提

高代码的可读性和可维护性。

@GetMapping("/findByProductId/{productId}") 是Spring 框架中用于处理HTTP GET 请求的注

---
**第 39 页**

33

解。它表示当收到一个路径为"/findByProductId/{productId}"的GET 请求时，将会调用对应的方法来

处理这个请求。其中，{productId}是一个路径变量，表示在这个路径中会接收一个名为"productId"的

参数，并将其传递给处理方法进行处理。

package microservicedemo.productservice.controller;
import lombok.AllArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import microservicedemo.productservice.mapper.ProductMapper;
import microservicedemo.productservice.po.Product;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.PathVariable;
import org.springframework.web.bind.annotation.RestController;
import java.util.List;
/**
* 商品的服务控制层
*/
@AllArgsConstructor
@RestController
@Slf4j
public class ProductController {
private final ProductMapper productMapper;
/**
* 根据商品id 查询商品
*/
@GetMapping("/findByProductId/{productId}")
public Product findByProductId(@PathVariable Long productId) {
Product product = productMapper.findByProductId(productId);
log.info("-------------OK   /findByProductId/{productId}--------------------");
return product;
}
/**
* 查询所有商品
*/
@GetMapping("/queryAllProduct")
public List<Product> findByProductId() {
List<Product> productList = productMapper.queryAllProduct();
log.info("-------------OK   queryAllProduct--------------------");
return productList;
}
}

# 2.4.3.5 创建mapper 类

"数据层mapper"类通常是指用于实现数据访问对象（DAO）或者数据访问层（DAL）的接口类。

这些类主要负责将应用程序的领域对象（实体对象）与数据库之间进行映射和交互，包括数据的读

取、写入、更新和删除等操作。

创建mapper 接口过程如下：

---
**第 40 页**

34

mapper 类对数据库的操作可以使用配置文件的方式，也可以使用注解的方式，我们这里使用了

注解的方式。

package microservicedemo.productservice.mapper;
import microservicedemo.productservice.po.Product;
import org.apache.ibatis.annotations.Param;
import org.apache.ibatis.annotations.Select;
import java.util.List;
public interface ProductMapper {
@Select("select p.product_name as productName,p.price as price from product p where id =
#{productId}")
Product findByProductId(@Param("productId") Long productId);
@Select("select p.id,p.product_name as productName,p.price as price from product p")
List<Product> queryAllProduct();
}

这个名为ProductMapper 的接口，用于定义与产品相关的数据库操作方法。该接口使用了MyBatis

注解来定义SQL 查询语句，并提供了两个方法：

findByProductId：根据产品ID 查询产品信息，使用@Select 注解指定了查询语句，并使用@Param

注解指定了参数名称。

queryAllProduct：查询所有产品信息，同样使用了@Select 注解指定了查询语句。

这个接口的作用是定义了查询产品信息的方法，可以通过调用这些方法来实现对产品信息的数

据库操作。

# 2.4.3.6 创建实体类Product

创建商品类Product，商品类要与数据库中的字段保持一致，由于使用了lombok，因此不需要与

构造函数及getter 和setter 等方法。

package microservicedemo.productservice.po;
import lombok.Data;
/**
* 商品的实体
*/
@Data
public class Product {
private Long id;
private String productName;
private Double price;

---
**第 41 页**

35

private Long stock;
}

# 2.4.3.7 编写配置文件application.yaml

product-service 的配置文件如下，

server:
port: 8010
spring:
application:
name: product-service
datasource:
url: jdbc:mysql://127.0.0.1:3306/tb_product?characterEncoding=utf-8&useSSL=false
username: root
password: root
driver-class-name: com.mysql.jdbc.Driver
eureka:
instance:
prefer-ip-address: true
client:
register-with-eureka: true
fetch-registry: true
service-url:
defaultZone: http://localhost:8888/eureka

配置文件是一个YAML 格式的文件，用于配置一个Spring Boot 应用的服务器端口、数据源、

Eureka 服务注册和发现等。

(1). server 部分配置了服务器的端口号为8010。

(2). spring 部分配置了应用的名称为"product-service"。

(3). datasource 部分配置了数据库连接的URL、用户名、密码和驱动类。

(4). eureka 部分配置了Eureka 服务的实例选项，prefer-ip-address 配置为true 表示使用ip 地址

作为Eureka 实例的标识。

(5). eureka.client 部分配置了Eureka 客户端的注册和发现选项，register-with-eureka 和fetch-

registry都配置为true表示该应用将注册自己到Eureka服务并从Eureka服务中获取其他服务的信息。

(6). eureka.client.service-url.defaultZone
配置了
Eureka
服务的
URL ，默认为

"http://localhost:8888/eureka"。

# 2.4.4 创建服务使用者product-client

# 2.4.4.1 项目基本结构及pom

再创建一个模块，用于使用OpenFeign 调用product-service 中提供的服务，项目名称为：product-

client。项目结构如下：

---
**第 42 页**

36

图 2-16

pom 文件内容如下：

<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-
instance"
xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-
4.0.0.xsd">
<modelVersion>4.0.0</modelVersion>
<parent>
<artifactId>microservice-demo</artifactId>
<groupId>cn.microservice-demo</groupId>
<version>1.0.0</version>
</parent>
<artifactId>product-client</artifactId>
<name>product-client</name>
<description>product-client</description>
<dependencies>
<!--web 模块一定要放在前面 -->
<dependency>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-starter-web</artifactId>
</dependency>
<!--undertow 容器 -->
<dependency>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-starter-undertow</artifactId>
</dependency>
<!--SpringCloud 整合的openFeign -->
<dependency>

---
**第 43 页**

37

<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-config</artifactId>
<exclusions>
<exclusion>
<groupId>*</groupId>
<artifactId>*</artifactId>
</exclusion>
</exclusions>
</dependency>
</dependencies>
<build>
<plugins>
<plugin>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-maven-plugin</artifactId>
</plugin>
</plugins>
</build>
</project>

# 2.4.4.2 创建启动类ProductClientApplication

启动类ProductClientApplication 的内容如下，

其中：

@SpringBootApplication(scanBasePackages = {"cn.microservicedemo.productclient.controller"})：这

是Spring Boot 注解，用于标识这是一个Spring Boot 应用程序的入口类。scanBasePackages 参数指定

了需要扫描的包，表示Spring Boot 应用程序会扫描cn.microservicedemo.productclient.controller 包及

其子包中的组件，并将其注册为Spring Bean。

@EnableEurekaClient：这是一个Spring Cloud 注解，用于标识这是一个Eureka 客户端应用程序。

当应用程序启动时，它会尝试注册到Eureka 服务注册中心，并从中获取其他服务的信息。

@EnableFeignClients("cn.microservicedemo.productclient.Client")：这是一个Spring Cloud 注解，

用于启用Feign
客户端功能。参数指定了需要扫描的包，表示
Spring
会扫描

cn.microservicedemo.productclient.Client 包及其子包中的Feign 客户端接口，并将其注册为Feign 客户

端。Feign 是一个声明式的Web 服务客户端，可以简化HTTP API 的调用。

package cn.microservicedemo.productclient;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.netflix.eureka.EnableEurekaClient;
import org.springframework.cloud.openfeign.EnableFeignClients;
@SpringBootApplication(scanBasePackages = {"cn.microservicedemo.productclient.controller"})
@EnableEurekaClient
@EnableFeignClients("cn.microservicedemo.productclient.Client")

---
**第 44 页**

38

public class ProductClientApplication {
public static void main(String[] args) {
SpringApplication.run(ProductClientApplication.class, args);
}
}

# 2.4.4.3 创建Feign 调用接口ProductServiceClient

商品服务远程调用接口ProductServiceClient，用于调用远程服务。

注解：@FeignClient(name="product-service")用于指明调用的服务名。

package cn.microservicedemo.productclient.Client;
import cn.microservicedemo.productclient.model.Product;
import org.springframework.cloud.openfeign.FeignClient;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestParam;
import java.util.List;
/**
* 商品服务远程调用客户端
*/
@FeignClient(name="product-service")
public interface ProductServiceClient {
/**
* 根据商品id 获取商品对象
* @param productId
* @return
*/
@GetMapping("/findByProductId/{productId}")
Product findByProductId(@RequestParam(value = "productId") Long productId);
/**
* 获取所有商品集合
* @return
*/
@GetMapping("queryAllProduct")
List<Product> queryAllProduct();
@GetMapping("queryAllt")
List<Product> queryAl();
}

# 2.4.4.4 创建控制器类ProductServiceClientController

控制器类ProductServiceClientController，这个控制器类在Feign 调用接口模块中的作用是作为

客户端的入口点，负责接收和处理客户端的请求，并通过调用Feign 接口来实现对远程服务的调用。

其中的log.info 语句主要是为调试和查看结果用的。

package cn.microservicedemo.productclient.controller;

---
**第 45 页**

39

import cn.microservicedemo.productclient.Client.ProductServiceClient;
import cn.microservicedemo.productclient.model.Product;
import lombok.AllArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.PathVariable;
import org.springframework.web.bind.annotation.RestController;
import java.util.List;
@AllArgsConstructor
@RestController
@Slf4j
public class ProductServiceClientController {
@Autowired
private final ProductServiceClient prodServiceClient;
/**
* 根据商品id 查询商品
*/
@GetMapping("/findByProductId/{productId}")
public Product findByProductId(@PathVariable Long productId) {
Product product = prodServiceClient.findByProductId(productId);
log.info("-------------In client  findByProductId---------------");
return product;
}
/**
* 查询所有商品
*/
@GetMapping("/queryAllProduct")
public List<Product> findByProductId() {
List<Product> productList = prodServiceClient.queryAllProduct();
log.info("--------------In client  queryAllProduct--------------");
return productList;
}
}

# 2.4.4.5 创建商品类Product

这里的商器Product 类要与前面的一样，也可以将共有的类放在一个公共模块中。

package microservicedemo.productserviceapi;
import lombok.Data;
/**
* 商品的实体
*
* @author me
*/
@Data
public class Product {
private Long id;
private String productName;

---
**第 46 页**

40

private Double price;
private Long stock;
}

启动服务，测试服务调用，同时也可以测试负载均衡（OpenFeign 也封装了Ribbon 的负载均衡

功能）。

# 2.4.4.1 编写配置文件application.yaml

这个YAML 格式的配置文件，用于配置一个名为"product-client"的服务。

(1). 该服务将在端口8018 上运行。

(2). 配置文件还包括了一些关于Eureka 服务器的配置，该服务器用于服务发现和负载均衡。

(3). 配置中指定了Eureka 的一些属性，包括Eureka 服务器的地址和一些与服务注册和发现相

关的参数。该配置文件还指定了该服务将注册到Eureka 服务器，并且将定期从Eureka 服务器获取最

新的服务注册信息。

配置文件内容如下：

server:
port: 8018
spring:
application:
name: product-client
ribbon:
eureka:
enable: true
eureka:
instance:
prefer-ip-address: true
client:
register-with-eureka: true
#下面一定要写true
fetch-registry: true
initial-instance-info-replication-interval-seconds: 10
registry-fetch-interval-seconds: 5
service-url:
defaultZone: http://localhost:8888/eureka

# 2.4.5 项目测试

到目前为止项目microservice-demo 的总体结构如下：

图 2-17 项目结构

下面我们将项目启动并进行测试，启动的项目包括：

(1). 注册服务器eureka-service:用于服务的注册。

---
**第 47 页**

41

(2). 微服务product-service:提供服务的程序。

(3). 微服务调用接口程序product-client:调用服务的程序。

在idea 中先启动eureka-sevice，然后依次启动product-service、product-client。如下图所示，可

以看到启动后的程序及端口。

图 2-18 程序启动后Idea 中看到的信息

点击上图红框中的EurekaServiceApplication:8888，或在浏览器中输入localhost:8888，打开Eureka

控制台界面，可以看到注册到服务器上的服务。

---
**第 48 页**

42

图 2-19 Eureka 中显示已经注册的微服务

在浏览器上输入：localhost:8018/findByProductId/1,如果输入库中有该商器，则会在浏览器中看

到返回的输入，如下图所示。

f

图 2-20 显示的结果

其中/findByProductId/1 对应的是product-client 中控制器ProductServiceClientController 的

@GetMapping("/findByProductId/{productId}")。完整代码如下，这里调用了使用OpenFeign 实现的服

务访问接口prodServiceClient，prodServiceClient 调用product-service 提供的服务。

@GetMapping("/findByProductId/{productId}")
public Product findByProductId(@PathVariable Long productId) {

---
**第 49 页**

43

Product product = prodServiceClient.findByProductId(productId);
log.info("--------------In client  findByProductId----------------");
return product;
}

# 2.5 基于OpenFeign 的负载均衡

# 2.5.1 负载均衡简介

负载均衡是一种基础的网络服务，其原理是通过运行在前面的负载均衡服务，按照指定的负载

均衡算法，将流量分配到后端服务集群上，从而为系统提供并行扩展的能力。

负载均衡有两种：客户端负载均衡与服务端负载均衡

⚫
服务端负载均衡

比如nginx，先发送请求到负载均衡服务器或者软件，然后通过负载均衡算法，在多个服务器之

间选择一个进行访问；即在服务器端再进行负载均衡算法分配。

⚫
客户端负载均衡

客户端会有一个服务器地址列表，在发送请求前通过负载均衡算法选择一个服务器，然后进行

访问，这是客户端负载均衡；即在客户端就进行负载均衡算法分配

Ribbon 就是一个典型的客户端负载均衡器。Ribbon 会获取所有服务地址，然后根据负载均衡算

法，获取本次请求的服务地址。

OpenFeign 是一个客户端负载均衡工具，它可以让你像调用本地方法一样调用远程服务。它内部

使用了Ribbon 来实现负载均衡。

# 2.5.2 负载均衡测试

搭建服务提供者集群是指将多台服务器或计算机组成一个集群，用于提供特定的服务或应用程

序。这些服务器可以共同处理用户的请求，提供高可用性、负载均衡和容错能力，从而提高服务的

稳定性和性能。

要在一台机器上体会负载均衡，可以在一台机器上启动一个服务的多个实例，构成服务提供者

集群，下面是具体过程。

Spring Cloud Eureka 默认情况下会开启负载均衡。当服务向 Eureka 注册时，Eureka 会自动将

负载均衡请求分发给多个实例，从而实现负载均衡。因此，开发人员无需额外配置即可享受到 Eureka

默认的负载均衡功能。

我们前面创建的项目其实已经具备了负载均衡功能，下面我们来进行验证。

# 2.5.2.1 搭建服务提供者集群

搭建服务提供者集群通常涉及到网络配置、服务器部署、负载均衡、故障恢复等技术，旨在实

现服务的高效运行和可靠性。常见的服务提供者集群包括Web 服务器集群、数据库集群、应用服务

器集群等。

我们可以在idea 集成开发环境中进行负载均衡的测试，下面是在一台机器上创建两个product-

service 服务实例的过程。在Idea 中打开service 窗口，如图所示选择“Copy Configuration...”

---
**第 50 页**

44

图 2-21 复制服务ProductMgrApplication 配置

如图所示，输入Name 并添加“VM options：”指定新的服务端口，这里指定的端口为8011。

图 2-22 编辑配置信息

启动后在idea 中的service 窗口看到的信息：

---
**第 51 页**

45

图 2-23 启动了两个相同的服务

此集群中有两个product-service 微服务，两个微服务的功能一致，服务名也一致，但端口是不一

样的。下面是在eureka 服务器中看到的信息，其中括号中的2 表示该服务有2 个。

图 2-24 Eureka 上同时注册了两个PRODUCT-SERVICE 服务

# 2.5.2.2 负载均衡的实现

回到项目product-client，找到ProductServiceClient 接口，由于接口加上有@FeignClien 注解，因

此已经实现了负载均衡，代码如下。

package cn.microservicedemo.productclient.Client;
import cn.microservicedemo.productclient.model.Product;
import org.springframework.cloud.openfeign.FeignClient;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestParam;

---
**第 52 页**

46

import java.util.List;
/**
* 商品服务远程调用客户端
*/
@FeignClient(name="product-service")
public interface ProductServiceClient {
/**
* 根据商品id 获取商品对象
* @param productId
* @return
*/
@GetMapping("/findByProductId/{productId}")
Product findByProductId(@RequestParam(value = "productId") Long productId);
/**
* 获取所有商品集合
* @return
*/
@GetMapping("queryAllProduct")
List<Product> queryAllProduct();
@GetMapping("queryAllt")
List<Product> queryAl();
}

使用@FeignClient 注解，虽然可以让Feign 自动帮我们完成服务之间的调用，但是默认使用的是

轮询（Round Robin）负载均衡策略，并没有进行任何特殊的定制。因此，如果希望使用不同的负载

均衡策略，可以在@FeignClient 注解上指定对应的配置类，比如使用@RibbonClient 注解来自定义

Ribbon 的配置，这样就可以实现指定的负载均衡策略了。同时，用户也可以使用Spring Cloud 提供

的其他负载均衡组件来实现负载均衡。总之，@FeignClient 注解只是一个方便的工具，用户需要自

己定制和配置具体的负载均衡策略。

# 2.5.2.3 测试负载均衡

整个项目在Idea 中启动后，可以在Idea 的后台日志观查调用的是哪一个服务。前面我们已经在

在product-service 服务的Controler 控制器中的findByProductId 方法中加入一条日志信息：

log.info("-------------OK   /findByProductId/{productId}--------------------");

这样在调用该服务时，就会显示这条信息。

在浏览器上输入：http://localhost:8018/findByProductId/1 。先将ProductMtrApplication 和

ProductMtrApplication1 的控制台信息清空，然后在浏览器上多次刷新输入的链接，观察控制台的输

出。

如图所示，分别是ProductMtrApplication 和ProductMtrApplication1 的输出，一共发了5 次服务

请求，每发一次服务请求就会在控制台输出一信息，可以看到两个控制台分别输出的信息，表明多

次服务请求被分配均匀的分给了两个服务提供者，说明实现了负载均衡。

---
**第 53 页**

47

图 2-25 查看位于端口8010 的product-service 服务的输出

图 2-26 查看位于端口8011 的product-service 服务的输出

# 2.5.3 负载均衡策略

Ribbon 内置了多种负载均衡策略：

(1). 轮询策略（com.netflix.loadbalancer.RoundRobinRule）

(2). 随机策略（com.netflix.loadbalancer.RandomRule）

(3). 重试策略（com.netflix.loadbalancer.RetryRule）：在一个配置时间段内（超时时间），当选择

服务实例不成功，则一直尝试选择一个可用的服务实例。

(4). 权重策略（com.netflix.loadbalancer.WeightedResponseTimeRule）：会计算每个服务的权重，

越高的被调用的可能性越大。

(5). 最佳策略（com.netflix.loadbalancer.BestAvailableRule）：遍历所有的服务实例，过滤掉故障

实例，并返回请求数最小的实例。

(6). 可用过滤策略（com.netflix.loadbalancer.AvailabilityFilteringRule）：过滤掉故障和请求数超过

阈值的服务实例，再从剩下的实力中轮询调用。

负责均衡策略是可以通过配置文件进行修改的。

# 2.6 基于OpenFeign 的熔断处理

基于OpenFeign 的熔断是一种微服务架构中的熔断机制，用于防止服务雪崩。当服务出现故障

或响应时间过长时，熔断器会自动断开与该服务的连接，避免影响整个系统的正常运行。

总的来说，熔断的主要作用包括：

(1). 避免故障的传播，保护系统免受外部服务故障的影响。

---
**第 54 页**

48

(2). 提高系统的可用性和稳定性，减少系统雪崩的风险。

(3). 提供快速失败的响应，减少系统资源的占用。

(4). 在外部服务故障恢复后，重新恢复对外部服务的调用，恢复正常的系统功能。

在使用OpenFeign 时，我们可以通过配置Hystrix 来实现熔断机制。Hystrix 是一个开源的熔断

框架，它可以帮助我们处理分布式系统中的延迟和容错问题。

# 2.6.1 编写熔断处理代码

首先需要加入如下依赖，由于前面在父项目中已经加入了该依赖，所以就不需要再加入到

product-client 模块中了。

<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-netflix-hystrix</artifactId>
</dependency>

在product-client 模块中的client 包中添加ProductServiceFallback 类，实现ProductServiceClient

接口，用于OpenFeign 自动触发熔断机制时的响应处理。

ProductServiceFallback 代码如下，触发熔断机制，这里只是简单的返回了null，在生产环境中，

可以将方法的返回值进行处理，而不是返回null，这样前端可以通过收到的回应信息的内容进行更

进一步的处理。

package cn.microservicedemo.productclient.Client;
import cn.microservicedemo.productclient.model.Product;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Component;
import java.util.List;
@Slf4j
@Component
public class ProductServiceFallback implements ProductServiceClient {
@Override
public Product findByProductId(Long productId) {
log.info("findByProductId callback");
return null;
}
@Override
public List<Product> queryAllProduct() {
log.info("queryAllProduct callback");
return null;
}
@Override
public List<Product> queryAl() {
return null;
}
}

修改ProductServiceClient 的FeignClient 注解，加上“fallback =ProductServiceFallback.class”。

完整代码如下所示。

---
**第 55 页**

49

package cn.microservicedemo.productclient.Client;
import cn.microservicedemo.productclient.hystrix.ProductServiceFallback;
import cn.microservicedemo.productclient.model.Product;
import org.springframework.cloud.openfeign.FeignClient;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestParam;
import java.util.List;
/**
* 商品服务远程调用客户端
*/
@FeignClient(name="product-service",fallback = ProductServiceFallback.class)
public interface ProductServiceClient {
/**
* 根据商品id 获取商品对象
*/
@GetMapping("/findByProductId/{productId}")
Product findByProductId(@RequestParam(value = "productId") Long productId);
/**
* 获取所有商品集合
*/
@GetMapping("queryAllProduct")
List<Product> queryAllProduct();
@GetMapping("queryAllt")
List<Product> queryAl();
}

注意要确保ProductServiceFallback 类已经正确实现，并且能被Spring 容器扫描到。比如我们把

把ProductServiceFallback 放在Client 包下面，如下图所示，则需要在启动类的注解

@SpringBootApplication 中加入“cn.microservicedemo.productclient.Client”，即：

@SpringBootApplication(scanBasePackages =
{"cn.microservicedemo.productclient.controller","cn.microservicedemo.productclient.Client"})

否则会在启动product-client 时有找不到该ProductServiceFallback 的错误。

图 2-27 项目结构

在配置文件中开启熔断机制，配置文件完整代码如下，其中对于hystrix 仅进行了简单的配置，

其它配置使用了默认值，实际使用时可以进行更详细的配置，比如熔断阀值等。

---
**第 56 页**

50

server:
port: 8018
spring:
application:
name: product-client
ribbon:
eureka:
enable: true
eureka:
instance:
prefer-ip-address: true
client:
register-with-eureka: true
#下面一定要写true
fetch-registry: true
initial-instance-info-replication-interval-seconds: 10
registry-fetch-interval-seconds: 5
service-url:
defaultZone: http://localhost:8888/eureka
feign:
hystrix:
enabled: true    #在feign 中开启hystrix 熔断机制

# 2.6.2 测试熔断机制

下面我们测试熔断机制，当服务出现故障或响应时间过长时，熔断器会自动断开与该服务的连

接，避免影响整个系统的正常运行。因此我们可以在product-service 中的服务中加上延时，使服务响

应时间超过设置的熔断阀值，这样就会激活熔断机制，进行熔断。

下面我们在product-service 的ProductController 类中的方法findByProductId 中加上延时，完整

代码如下：

@GetMapping("/findByProductId/{productId}")
public Product findByProductId(@PathVariable Long productId) throws InterruptedException {
Product product = productMapper.findByProductId(productId);
Thread.sleep(2000);
log.info("-------------OK   /findByProductId/{productId}---------");
return product;
}

然后在浏览器中输入http://localhost:8018/findByProductId/1 进行测试，观查浏览器的返回值和

idea 中的日志信息。

我们会发现，浏览器中没有任何信息，在idea 开发环境中的查看ProductClientApplication 的日

志信息中可以看到如下信息：

图 2-28 熔断机制显示的信息

---
**第 57 页**

51

可以看到已经激活了熔断机制，所显示的信息是熔断处理函数中我们加入的日志显示信息。

# 2.7 Gateway API 网关的实现

# 2.7.1 API 网关

# 2.7.1.1 什么是API 网关

问题的提出：访问不同的微服务API，一般会有不同的网络地址（IP 和端口），客户端在访问这

些微服务API 时必须 记住几十甚至几百个地址，这对于客户端来说太复杂也难以维护。而且，如果

让客户端直接访问各个微服务API， 就可能会有很多问题：

客户端会请求多个不同的服务，需要维护不同的请求地址，增加开发难度。存在跨域请求时，

每个微服务都需要独立设置加大身份认证的难度。

难以重构，随着项目的迭代，可能需要重新划分微服务。如果客户端直接与微服务通信，那么

重构将会很难实施。

某些微服务可能使用了防火墙/浏览器不友好的协议，直接访问会有一定的困难。

因此，我们需要一个微服务网关，介于客户端与服务器之间的中间层，所有的外部请求都会先

经过微服务网关。客户端只需要与网关交互，只知道一个网关地址即可。

图 2-29 API 网关示意图

综上所述，API 网关 = 路由 + 过滤器

过滤器在网关中可以完成一系列的横切功能，例如权限校验、跨域设置、负载均衡、日志、限

流以及监控 等等，这些都可以通过过滤器完成（其实路由转发也是通过过滤器实现的）。

---
**第 58 页**

52

API 网关的主要功能包括：

(1). 安全性：API 网关可以提供安全认证和授权机制，确保只有经过认证的用户可以访问API，

并且可以限制用户对API 的访问权限。

(2). 流量控制：API 网关可以控制流量，包括限制并发访问量、限制请求频率、限制数据传输量

等，以保护后端服务不受过载影响。

(3). 监控和分析：API 网关可以监控API 的使用情况，包括请求量、响应时间、错误率等指标，

并提供分析报告，帮助开发者优化API 的性能和稳定性。

(4). 路由和转发：API 网关可以根据请求的路径或参数，将请求路由到不同的后端服务，实现

请求的转发和分发。

(5). 缓存：API 网关可以缓存经常请求的数据，减少对后端服务的请求，提高响应速度和降低

成本。

(6). 转换和适配：API 网关可以对请求和响应进行转换和适配，包括数据格式转换、协议转换、

版本适配等，以满足不同客户端的需求。

总的来说，API 网关的主要功能是提供统一的入口和管理接口，保护后端服务，提高性能和安

全性，简化开发和维护工作。

# 2.7.1.2 常见API 网关

Nginx：使用反向代理和负载均衡可实现对API 服务器的高可用。

Zuul：Netﬂix 开源，使用java 开发，功能丰富。

Spring Cloud Gateway：Spring Cloud 提供的新一代网关，用于替代Zuul。

# 2.7.2 API 网关Gateway 核心概念

Spring Cloud Gateway 是 Spring 官方开发的网关，旨在为微服务架构提供一种简单而有效的统

一的 API 路由管理方式，统一访问接口。

Spring Cloud Gateway 作为 Spring Cloud 生态系中的网关，目标是替代 Netﬂix 的zuul1.x，并

且没有对zuul2.x 以上版本进行集成。Gateway 不仅提供统一的路由方式，并且基于 Filter 链的方式

提供了网关基本的功能，例如：路由、熔断、过滤、限流等。

图 2-30 Spring Cloud Gateway 主要功能

---
**第 59 页**

53

路由（route）   路由是网关最基础的部分。路由信息由一个ID、一个目的URI、一组断言和一

组Filter 组成。如果断言为真，则说明请求URL 和配置的路由匹配。

断言（predicates）就是路由的条件。开发人员可以匹配HTTP 请求中的所有内容（例如请求头

或请求参 数），如果请求与断言相匹配则进行路由。

过滤器（ﬁlter） Gateway 中的Filter 分为两种类型，分别是Gateway Filter 和Global Filter。过滤

器Filter 可以对请求和响应进行处理。

# 2.7.3 Gateway 的实现

# 2.7.3.1 编写gateway-service

在父工程下，创建 Maven Module ，名称为gateway-service，创建工程时可以直接选择依赖，如

下图所示，如果不选也可在生成工程后可以直接在pom 文件中手动加入Gateway 依赖。

在pom.xm 文件中Gateway 依赖如下。

<dependencies>

<!--加入gateway 依赖 -->

<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-gateway</artifactId>
</dependency>

注意创建项目时pom 文件中会会自动添加一些依赖，有可能会因为依赖的版本适配问题导致项

---
**第 60 页**

54

目无法运行，因此要进行一些修改，下面是修改好的pom 文件，可以下面的pom 文件覆盖项目中的

文件。

<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-
instance"
xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-
4.0.0.xsd">
<modelVersion>4.0.0</modelVersion>
<parent>
<artifactId>microservice-demo</artifactId>
<groupId>cn.microservice-demo</groupId>
<version>1.0.0</version>
</parent>
<artifactId>gateway-service</artifactId>
<name>gateway-service</name>
<description>gateway-service</description>
<dependencies>
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-gateway</artifactId>
</dependency>
</dependencies>
<build>
<plugins>
<plugin>
<groupId>org.springframework.boot</groupId>
<artifactId>spring-boot-maven-plugin</artifactId>
</plugin>
</plugins>
</build>
</project>

另外要在父工程的pom 文件的  <modules></modules>中加入“gateway-service”，加入后的内容

如下：

<!--新增包含的子项目-->
<modules>
<module>eureka-service</module>
<module>product-service</module>
<module>product-client</module>
<module>gateway-service</module>
</modules>

生成的主启动类如下

package gatewayservice.gatewayservice;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
@SpringBootApplication
public class GatewayServiceApplication {
public static void main(String[] args) {

---
**第 61 页**

55

SpringApplication.run(GatewayServiceApplication.class, args);
}
}

创建application.yml 配置文件

server:
port: 9999
spring:
application:
name: @artifactId@
cloud:
gateway:
discovery:
locator:
#开启以服务id 去注册中心上获取转发地址
enabled: true
##小写serviceId
lower-case-service-id: true
routes:
- id: product-client
uri: lb://product-client
filters:
- StripPrefix=1
predicates:
- Path=/product/**
eureka:
instance:
prefer-ip-address: true
client:
register-with-eureka: true
fetch-registry: true
service-url:
defaultZone: http://127.0.0.1:8888/eureka

上面这个 Spring Cloud Gateway 配置文件，主要包含了应用的端口号、应用名称和路由配置。

注意：如果在k8s 集群中要将service-url.defaultZone 改为注册服务eureka 的服务名，不能再用

本机IP。比如服务名为eureka-service，则service-url 为：defaultZone: eureka-service:8888/eureka 。

如果无法通过域名解析，可有下面命令看一下coredns 的configmap 文件是否正确。

kubectl -n kube-system edit configmap coredns

下面对配置文件中的主要部分进行说明：

server.port：指定应用的端口号为 9999。

# 1.
spring.application.name：指定应用名称为 Maven 中的 artifactId。使用 `@artifactId@` 占位

符表示，Spring 启动时会自动替换为实际的 artifactId 值，这里也可以直接写“gateway-service”。

# 2.
spring.cloud.gateway.discovery.locator.enabled：启用服务发现功能，设置为 true 表示从注册

中心上获取转发地址。

# 3.
spring.cloud.gateway.discovery.locator.lower-case-service-id：将服务 ID 转换成小写，避免在

路由规则中区分大小写。

# 4.
spring.cloud.gateway.routes：定义路由规则列表。每个路由规则都有一个唯一的 ID（例如

---
**第 62 页**

56

product-client），以及要转发到的目标 URI（例如 lb://product-client）。其中 “lb” 表示负载均衡，这

里将请求转发到名为 product-client 的服务上，并进行了一个 StripPrefix 过滤器操作，去掉了请求

路径中的一级前缀。Predicates 则用来匹配 HTTP 请求， Filters 则用来对请求进行过滤或增强。

这里“lb”所指示的负载均衡与前面提到的product-client 中的负载均衡是不完全一样的。这里是

在调用product-client 之前进行的负载均衡，而在product-client 中还有一级负载均衡。

在Gateway 中实现负载均衡的关键是使用服务的名称来进行路由。当Gateway 接收到客户端的

请求时，它会根据请求中指定的服务名称来进行路由。当Gateway 启动时，它会从服务注册中心获

取所有可用的服务信息，包括服务的名称、实例的地址等。然后，Gateway 会使用负载均衡策略（如

轮询、随机等）从可用的服务实例中选择一个来处理请求。当有新的服务实例加入或下线时，Gateway

会动态地更新可用服务列表，以确保请求能够均衡地分发到多个服务上。

虽然两者都实现了负载均衡的功能，但它们是在不同的层次上进行的。Feign 是在应用程序内部

的服务调用中实现负载均衡，而Gateway 是在整个网关层面上实现负载均衡。

通过 Spring Cloud Gateway 的这些配置，可以对请求进行转发、负载均衡、路由和过滤等处理，

从而提升系统的可靠性和扩展性。其中负载均衡和过滤，我们后面再具体讲解。

# 2.7.3.2 gateway 路由测试

启动工程，如下图所示，将所有服务都启动。

图 2-31 idea 中所有服务启动

注册服务器中显示的信息：

---
**第 63 页**

57

图 2-32 注册服务器中显示的服务

由于我们在配置文件中对于服务“product-client”配置了路径“- Path=/product/**”所以在发请求时

要加入“product”。

打开浏览器输入http://localhost:9999/product/queryAllProduct，可以收到从数据库返回的结果，内

容如下，说明路由没有问题。在实际项目开发时，前端（即web 端）可以通过axios 等前后端通信方

式，通过类似这样的接口http://localhost:9999/product/queryAllProduct 与后端通信。

图 2-33 后端返回的结果

这样我们在访问的时候，通过在gateway-service 配置文件application.yml 中的“routes”下添加可

访问的服务，就可以只使用一个网关端口，调用所有的微服务，这个端口一般是外网可访问的，而

其它端口则不对外开放，只在系统内部各服务之间开放。

# 2.7.4 Gateway 过滤器

Spring Cloud Gateway 除了具备请求路由功能之外，也支持对请求的过滤。

---
**第 64 页**

58

# 2.7.4.1 Gateway 过滤器类型

Spring Cloud Gateway 的 Filter 可分为两种：

局部过滤器（GatewayFilter 接口），是针对单个路由的过滤器。

全局过滤器（GlobalFilter 接口），作用于所有路由，不需要单独配置。开发者可以通过全局过滤

器实现一些共通功能，并且全局过滤器也是开发者使用比较多的过滤器。

Spring Cloud Gateway 内部也是通过一系列的内置全局过滤器对整个路由转发进行处理。

图 2-34 全局过滤器

# 2.7.4.2 统一鉴权（过滤器实例）

下面创建一个统一鉴权认证过滤器，也就是对用户是否有访问权限的验证。这个鉴权过程可以

在网关层统一检验， 检验的标准就是：请求中是否携带token 凭证以及token 的正确性。

在gateway_service 工程中添加ﬁlter 包，创建AuthFilter 类

package gatewayservice.gatewayservice;
import org.springframework.cloud.gateway.filter.GatewayFilterChain;
import org.springframework.cloud.gateway.filter.GlobalFilter;
import org.springframework.http.HttpStatus;
import org.springframework.stereotype.Component;
import org.springframework.web.server.ServerWebExchange;
import reactor.core.publisher.Mono;
@Component
public class AuthFilter implements GlobalFilter{
@Override
public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {
//获取请求参数中的token
String token = exchange.getRequest().getQueryParams().getFirst("token");
System.out.println(token);

---
**第 65 页**

59

if(!token.equals("1")){
//响应http 状态码（401）
exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED);
//请求结束
return exchange.getResponse().setComplete();
}
//继续执行过滤器链中的下一个资源
return chain.filter(exchange);
}
}

这段代码是一个 Spring Cloud Gateway 中的全局过滤器，主要是对请求进行鉴权操作。

# 1. import reactor.core.publisher.Mono：引入 Reactor 的 Mono 类，用于实现异步非阻塞编程模型。

# 2. @Component：声明该类为 Spring 组件，将其注册到 Spring 容器中。

# 3. AuthFilter implements GlobalFilter：实现了 GatewayFilter 接口的 AuthFilter 全局过滤器，用

于对所有请求进行拦截和处理。

# 4. filter(ServerWebExchange exchange, GatewayFilterChain chain)：重写 filter 方法，用于进行过

滤逻辑处理。其中，ServerWebExchange 表示一个 HTTP 请求-响应交换对象，GatewayFilterChain 表

示一个过滤器链，可以通过它来执行下一个过滤器或者业务处理器。

# 5. exchange.getRequest().getQueryParams().getFirst("token")：从 HTTP 请求参数中获取名为

"token" 的参数值。

# 6. exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED)：设置 HTTP 响应状态码

为 401（Unauthorized）。

# 7. return chain.filter(exchange)：通过 GatewayFilterChain 对象来调用下一个过滤器或者业务处理

器，也就是放行请求。

总体来说，这段代码中的 AuthFilter 过滤器可以用于对需要鉴权的请求进行过滤和处理，如果

请求参数中没有包含 token 参数，则返回 401 错误码，否则放行请求继续处理。

测试：http://127.0.0.1:9999/product/queryAllProduct?token=1

由于我们在过滤器中只放行token 为1 的请求，因此上面的链接可以获取信息。

如果输入的地址没有?token=1 或者token 不等于1，那么，会提示401 错误页面，如下图所示。

---
**第 66 页**

60

在实际项目开发时token 不是固定值，一般是动态生成的。在Gateway 中，可以通过以下步骤

实现用户认证和使用Token 进行服务访问：

# 1. 用户发送请求到Gateway。

# 2. Gateway 接收到请求后，首先进行用户认证。

# 3. 如果用户认证成功，Gateway 会生成一个Token，并将Token 返回给客户端。

# 4. 客户端收到Token 并保存。

# 5. 客户端将Token 携带在请求中，发送请求到Gateway。

# 6. Gateway 接收到请求后，请求会经过预先配置的过滤器链，对Token 验证。

# 7. 如果Token 验证通过，过滤器将请求转发给后端服务进行处理。

# 8. 后端服务根据业务逻辑返回相应的结果给Gateway。

# 9. Gateway 将后端服务的响应返回给客户端。

# 10. 如果Token 验证不通过，则返回设置好的错误信息。

需要注意的是，用户认证和Token 验证的实现方式可以根据实际需求和技术栈进行选择。上述

步骤提供了一种基本的流程，具体的实现方式可能会有所不同。

至此微服务开发方法讲解完毕，下面将讲解如何将其部署在云原生环境中。

# 2.8 Config 实现配置中心（选做内容）

# 2.8.1 配置中心简介

# 2.8.1.1 什么是配置中心

对于传统单体应用而言，常使用配置文件来管理所有配置。比如Springboot 的application.yml 文

件。但是在微服务架构中全部手动修改的话会很麻烦而且不易维护。微服务的配置管理一般有以下

需求：

➢
集中配置管理：一个微服务架构中会有成百上千个微服务，所以集中配置管理是很重要

的。

➢
不同环境不同配置：有些配置在不同环境（开发、生产、测试）中是不同的。

---
**第 67 页**

61

➢
运行期间动态调整： 比如：运行中会根据各个微服务的负载情况动态调整数据源连接池

大小等。

➢
配置修改后可自动更新：配置内容修改后，微服务可以自动更新配置。

综上所述，对于微服务架构来说，一套统一的、通用的管理配置中心是不可缺少的重要组成部

分。

# 2.8.1.2 基本原理

配置服务器：在配置服务器中，配置信息被存储在一个或多个远程仓库中，如Git 仓库。配置服

务器通过HTTP 或其他协议提供接口，使客户端能够获取配置信息。

配置客户端：在应用中引入Spring Cloud Config 客户端依赖后，应用会自动连接配置服务器并

获取配置信息。客户端在启动时会向配置服务器发起请求，获取当前应用的配置信息，并将其缓存

在本地。

配置刷新：配置客户端可以通过调用特定的接口或发送事件来触发配置的重新加载。当配置信

息发生变化时，客户端会向配置服务器发起请求，获取最新的配置信息，并更新本地缓存。这样，

应用就可以动态地获取最新的配置信息，而无需重启。

总结起来，Spring Cloud Config 通过将配置信息存储在远程仓库中，实现了配置的集中管理和动

态刷新。配置服务器提供配置信息的存储和访问接口，而配置客户端则负责从配置服务器获取配置

信息并在应用中使用。这样，就能够方便地管理和更新应用的配置信息，提高系统的灵活性和可维

护性。

# 2.8.1.3 常用配置中心

Spring Cloud Config：2014 年9 月开源，Spring Cloud 生态组件，可以和Spring Cloud 体系无缝

整合。

Apollo（阿波罗）：2016 年5 月开源，是携程框架部门研发的开源配置管理中心。能够集中管理

应用在不同环境、不同集群的配置,配置修改后能够实时的推送到应用端,并且有关于权限管理、流程

治理等功能。Nacos：2018 年6 月开源，是阿里开源的配置中心。Nacos 提供了一组简单易用的特性

集，帮助您实现动态服务发现、服务配置管理、服务及流量管理。本实践中我们使用SpringCloud 自

带的Config 配置组件，实现配置管理。

---
**第 68 页**

62

图 2-35 配置中心基本工作过程

# 2.8.2 SpringCloud Config 配置中心

SpringCloud Config 项目是一个解决分布式系统的配置管理方案。它包含了Client 和Server 两个

部分，server 默认使用git 来提供配置文件的存储、以接口的形式将配置文件的内容提供出去，client

通过接口获取数据、并依据此数据初始化自己的应用。

# 2.8.2.1 搭建远程GIT 仓库

SCC Server 默认使用Git 存储配置文件，当然也可以使用SVN。这里使用Git 作为学习环境。

如果直接使用GitHub 的话，国内用户经常会遇到无法连接或者网速过慢的问题。所以，这里我

们使用国内的Git 托管服务：码云。

访问码云官网：https://gitee.com/ 按照码云官网的要求注册用户并登陆，新建仓库。

图 2-36 搭建GIT 仓库

添加仓库信息：添加仓库名称，选择公开即可，然后点击最下方按钮“创建”

初始化readme 文件第一个红框内就是当前仓库地址。此时可以点击第二个红框，初始化readme

文件。

如果要对仓库管理进行管理，可以在左上角的“管理”选项中，对本仓库进行清理、删除等操作。

# 2.8.2.2 上传配置文件

将需要进行集中管理的配置文件，上传到上面创建好的仓库中。

---
**第 69 页**

63

上传文件命名规则：

{application}-{profile}.yml （或：{application}-{profile}.properties）

application 为应用名称、profile 指的开发环境（用于区分开发环境（dev），测试环境（test）、生

产环境（prod）等）。

这里我们将服务提供者product-service 的配置文件改名为：product-dev.yml；然后上传到远程Git

仓库中。

注意：上传配置文件的字符编码集要正确（本项目中一律使用utf-8），否则不能正确识别。如下

图所示，上传了product-dev.yml 文件。product-dev.yml 内容为：

server:
port: 18010
spring:
profiles:
active: dev
application:
name: product-service
datasource:
url: jdbc:mysql://127.0.0.1:3306/tb_product?characterEncoding=utf-8&useSSL=false
username: root
password: root
driver-class-name: com.mysql.jdbc.Driver
eureka:
instance:
prefer-ip-address: true
client:
register-with-eureka: true
fetch-registry: true
service-url:
defaultZone: http://localhost:8888/eureka

上传成功后，在gitee 上可以看到该文件。

图 2-37 git 仓库中的配置文件

---
**第 70 页**

64

# 2.8.2.3 创建配置管理服务端

在父工程下，创建 Maven Module 子工程（工程名：config-service；Packaging：jar）

在pom.xml 文件中添加依赖

<dependencies>
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-config</artifactId>
</dependency>
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-config-server</artifactId>
</dependency>
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>
</dependency>
</dependencies>
创建主启动类

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.config.server.EnableConfigServer;
import org.springframework.cloud.netflix.eureka.EnableEurekaClient;
@SpringBootApplication

@EnableEurekaClient// 启动 eureka 客户端

@EnableConfigServer// 启动 config 服务端

public class ConfigServerApplication {
public static void main(String[] args) {
SpringApplication.run(ConfigServerApplication.class, args);
}
}

在resources 目录下创建bootstrap.yml 文件，并添加git 相关信息，uri 就是gitee 中项目的https

协议的url ，注意将“.git”去掉。如果项目为私有则需要账号的password，如为开源共享的项目，则

不需要密码。

server:
port: 7009
spring:
application:

name: config-service # 为当前商品服务命名

profiles:
active: dev
cloud:
config:
server:
git:

---
**第 71 页**

65

username: frankgy@126.com
password:

uri: https://gitee.com/frankgy/serviceconfig #要读取的远程仓库的配置文件的地址。

default-label: master # 指定分支，不指定则默认master

eureka:
client:

service-url: # 配置服务注册地址，与 eureka-server 中暴露地址保持一致

defaultZone: http://localhost:8888/eureka
instance:

prefer-ip-address: true  # 是否使用 IP 地址注册，默认 false

# instance-id: product-service  # 实例 id，服务的唯一标识

instance-id: ${spring.cloud.client.ip-address}:${server.port} # 如果想在控制页面看到服务地址与端口，可

以将 instance-id 这样配置

lease-renewal-interval-in-seconds: 5  # 发送心跳的间隔，单位秒，默认 30

lease-expiration-duration-in-seconds: 10 # 续约到期时间，单位秒，默认90

启动该服务，在服务列表中看到配置服务已经启动，端口为7009.

图 2-38 服务列表中的配置服务

测试：启动项目，测试配置服务是否可正常运行，在浏览器地址栏中输入：

http://localhost:7009/product-dev.yml。

product-dev.yml 就是Git 仓库中上传的文件。如图所示返回了gitee 上仓库上的product-dev.yml。

---
**第 72 页**

66

图 2-39 返回的配置文件内容

# 2.8.2.4 修改product-service

product-service 相对于配置服务程序config-service 来说，它是客户端，因为product-service 需要

通过config-service 的配置服务从gitee 上获取配置。修改pproduct-service 工程，在pom.xml 文件中

添加SCC 的依赖。

<!--加入spring-cloud-config 的依赖 -->

<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-starter-config</artifactId>
</dependency>
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-config-client</artifactId>
</dependency>

将product-service 原来的application.yml 分别合并到文件application-dev.yml 和application-

k8s.yml 中，然后上传到远端配置文件仓库。创建bootstrap.yml 文件，内容如下，其中激活环境profile

可根据需要指定为dev 或k8s。本地application.yml 删除或何留都可以，因为bootstrap.yml 有了如下

配置后，本地的application.yml 将不再起作用。

spring:
cloud:
config:
discovery:

service-id: config-service  #告诉当前客户端 统一配置中心的服务端服务id

enabled: true #开启客户端，根据服务id 到注册中心获取配置信息

label: master # 指定分支

name:  product # 指定应用名称

profile: k8s # 指定激活环境

---
**第 73 页**

67

bootstrap.yml 用来在程序引导时执行，应用于早期配置信息读取。也就是说：bootstrap.yml 配置

文件在application.yml 配置文件之前被加载。

bootstrap.yml 用于系统级别的参数配置，这些参数一般是不会变动的。application.yml 用于定义

应用级别的参数配置。

bootstrap.yml 的典型应用场景就是：配置信息一般是从 config server 中加载。为了取得配置信

息，需要一些提早的引导配置。因此，把 config server 信息放在 bootstrap.yml 中，用来加载在这个

时期真正需要的配置信息。

重启测试，在gitee 上的product-dev.yml 中服务端口设置的是18010，重启动product-service，查

看是否正常。如图所示，ProductServiceApp 的端口变成了18010，即使用的是gitee 上的配置。

图 2-40 配置更改后的Product Service 的端口变化情况

可以看到，即使没有了application.yml 文件，微服务也可以正常工作。

需要手动重启并且等待一段时间。因为config server 需要从git 上下载application.yml 配置文件，

并且放在微服务的缓存中。

# 3 云原生环境构建

本章主要内容：

注：如果使用本课程提供的Ubuntu 镜像，则“1.基本环境准备”和“2.镜像工具安装”都装好，

可以在此镜像基础上创建三个链接克隆，再完成“3.2.2.1 配置静态IP、3.2.2.2 设置主机名”。 然

后完成“3.4 Kubernetes 安装”及以后的内容即可。

---
**第 74 页**

68

图 3-1 本章内容

# 3.1 基本概念

# 1.什么是云原生环境？

是指在云计算基础设施上构建和运行应用程序的一种方法。

它强调了应用程序的可移植性、弹性和可扩展性。

# 2.云原生环境的核心内容

微服务架构：将应用程序拆分为多个小型、独立的服务，每个服务专注于特定的功能，并可以

独立部署和扩展。

容器化：将应用程序和其依赖项打包到容器中，以实现应用程序的可移植性和隔离性。

自动化：通过自动化工具和流程，实现应用程序的部署、扩展和管理，以提高效率和可靠性。

弹性和可伸缩性：能够根据需求自动扩展和缩减资源，以适应不同的工作负载。

敏捷开发和持续交付：采用敏捷开发方法和持续集成/持续交付（CI/CD）流程，实现快速迭代和

发布。

# 3.2 基本环境准备

基本环境包括三个结点，即三台机器。一个master 两个worker 结点。

(1) 创建Ubuntu22.04.1 LTS 的虚拟机（3 个）。每台虚拟机设置空间为50G，一台虚拟机做为

Master 节点，另外两台机器做Worker(node)节点。

(2) 正确安装k8s 1.28。

# 3.2.1 虚拟机及操作系统安装

(1).下载虚拟机平台：VMware-workstation-full-16.0.0-16894299 并安装。

(2).下载ubuntu 22.04.1 LTS 或更高版本操作系统，下面是22.04.3 的链接。

---
**第 75 页**

69

https://cdimage.ubuntu.com/releases/22.04/release/ubuntu-22.04.3-live-server-arm64.iso

(3).在虚拟机平台上安装一个ubuntu 22.04，然后再克隆两个ubuntu 22.04。克隆方法如下图所示，

也可再克隆一个备用，以免在安装k8s 过程中出现问题，可以用备用的再克隆，这样可以避免重新

安装ubuntu 浪费大量时间。

图 3-2 克隆虚拟机

# 3.2.2 系统设置

# 3.2.2.1 确定虚拟机静态IP 网段及网关

打开虚拟网络编辑器，

---
**第 76 页**

70

如下图所示，查看你虚拟机静态IP 所在网段，按此网段配置你的三台虚拟机的IP。

点击更改设置，查看虚拟机的网关：

点击“更改设置”后，进入如下界面，再次点击“NAT 设置”：

---
**第 77 页**

71

点击“NAT 设置”后，出现如下界面，红框中的就是你虚拟机要用的网关。

通过上面操作知道了虚拟机的IP 地址和网关后，就可以进入下一步操作，进行IP 地址的配置。

---
**第 78 页**

72

# 3.2.2.2 配置静态IP

在虚拟机上启动三个ubuntu，使用vim 命令编辑网络配置文件 /etc/netplan/00-installer-

config.yaml，该文件用于配置网络接口的设置。建议使用root 用户或具有sudo 权限的用户进行操作

配置文件示例如下，

network:
ethernets:
ens33:
addresses:
- 192.168.79.133/24
routes:
- to: default
via: 192.168.79.2
nameservers:
addresses: [114.114.114.114, 8.8.8.8]
dhcp4: false
dhcp6: false
version: 2

如果是ubuntu 18.04 则配置文件如下：

network:
ethernets:
ens33:
addresses:
- 192.168.79.133/24
routes:
- to: 0.0.0.0/0
via: 192.168.79.2
on-link: true
nameservers:
addresses: [114.114.114.114, 8.8.8.8]
dhcp4: false
dhcp6: false
version: 2
三台虚拟机的IP 地址分别配置为

# 192.168.79.133/24
192.168.79.134/24
192.168.79.135/24
设置后每台都要运行netplan apply，这样才能立即生效。

# 3.2.2.3 设置主机名

# 192.168.79.133/24
k8s-master
192.168.79.134/24
k8s-worker1

---
**第 79 页**

73

# 192.168.79.135/24
k8s-worker2

分别在三台机器上执行各自的命令。

sudo hostnamectl set-hostname k8s-master    # 192.168.79.133 机器上执行。

sudo hostnamectl set-hostname k8s-worker1   #192.168.79.134 上执行

sudo hostnamectl set-hostname k8s-worker2   # 192.168.79.135 上执行

将下面信息同时写入三台机器的hosts。

# 192.168.79.133/24
k8s-master
192.168.79.134/24
k8s-worker1
192.168.79.135/24
k8s-worker2

操作方法（在每台机器上都要执行）：

cat >> /etc/hosts <<EOF
192.168.79.133/24
k8s-master
192.168.79.134/24
k8s-worker1
192.168.79.135/24
k8s-worker2
EOF

# 3.2.2.4 关闭防火墙和交换区（所有机器）

执行下面的命令关闭防火墙和交换区：

# 关闭防火墙

sudo ufw disable

# 关闭交换区

sudo swapoff -a

#永久关闭交换区

sed -i '/swap/s/^\(.*\)$/#\1/g' /etc/fstab

# 3.2.2.5 将桥接的IPv4 流量传递到iptables 的链（所有机器）

执行如下命令：

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward = 1
EOF

执行下面命令：

vim /etc/sysctl.conf

将下面两条加入/etc/sysctl.conf 文件最后面。

net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1

---
**第 80 页**

74

然后执行如下命令：

echo 1 > /proc/sys/net/ipv4/ip_forward
modprobe br_netfilter
sysctl --system

命令说明：

# 1. cat <<EOF > /etc/sysctl.d/k8s.conf - 这个命令会将后续输入的内容（直到遇到EOF 为止）写入

到/etc/sysctl.d/k8s.conf 文件中。

# 2. net.bridge.bridge-nf-call-ip6tables = 1 - 这个设置允许桥接网络调用IPv6 的iptables。

# 3. net.bridge.bridge-nf-call-iptables = 1 - 这个设置允许桥接网络调用IPv4 的iptables。

# 4. net.ipv4.ip_forward = 1 - 这个设置允许IPv4 数据包转发。

# 5. modprobe br_netfilter - 这个命令用于加载br_netfilter 内核模块，这是用于桥接网络的Netfilter

模块。

# 6. echo 1 > /proc/sys/net/ipv4/ip_forward - 这个命令将1 写入/proc/sys/net/ipv4/ip_forward 文件中，

从而启用IPv4 数据包转发。

# 3.2.2.6 设置时间同步（所有机器）

sudo apt install -y chrony
sudo systemctl restart chrony
chrony sources

timedatectl set-timezone Asia/Shanghai  #设置时区

# 3.3 容器运行时及工具安装(每个节点都装)

Docker 是创建镜像常用的工具，但在已经安装了Kubernetes 1.28 和containerd 的系统上安装

Docker 可能会导致冲突和兼容性问题，因此我们选择其它构建工具。这里我们选用buildkit,buildkit

是从Docker 从公司开源出来的下一代镜像构建工具，支持OCI 标准的镜像构建，项目地址是：

https://github.com/moby/buildkit。

# 3.3.1 安装containerd（所有机器都装，新版本不再使用docker）

在 Kubernetes（k8s）中，containerd 是一个负责管理容器生命周期的容器运行时。其主要作用

包括以下几点：

容器生命周期管理：containerd 负责管理容器的生命周期，包括创建、运行、暂停、重启、销毁

容器等操作。它与 Kubernetes 的 kubelet 一起协同工作，确保容器按照预期的方式在集群中运行。

容器镜像管理：containerd 也负责管理容器镜像，支持拉取、保存、删除镜像等操作。它可以与

容器仓库（如Docker Hub、Harbor）集成，使得在 Kubernetes 中使用镜像更加方便。

容器资源隔离：containerd 使用 Linux 的命名空间和控制组（cgroup）技术来实现容器间的资源

隔离，确保容器之间互相独立运行且不会相互干扰。

---
**第 81 页**

75

网络管理：containerd 负责配置容器的网络，包括分配IP 地址、端口映射等操作，以确保容器

之间可以互相通信，以及与外部网络连接。

总的来说，containerd 是 Kubernetes 中关键的组件之一，通过实现容器生命周期管理、镜像管

理、资源隔离和网络管理等功能，使得 Kubernetes 能够高效地管理和运行容器化应用程序。

执行下面命令安装containerd

sudo  apt-get update
sudo apt  install containerd

# 3.3.2 crictl 工具配置

配置文件 /etc/crictl.yaml 是用于设置 crictl 工具与容器运行时（containerd）和镜像服务之间的

通信配置。

因此，将这些配置写入 /etc/crictl.yaml 文件中，可以为 crictl 工具提供正确的配置，使其能够

与容器运行时和镜像服务进行通信，并执行相应的操作。

cat > /etc/crictl.yaml <<EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
EOF

runtime-endpoint: 指定容器运行时的 Unix 套接字路径。在这个例子中，容器运行时是

containerd，其套接字路径为 unix:///run/containerd/containerd.sock。

image-endpoint: 指定镜像服务的 Unix 套接字路径。在这个例子中，镜像服务也是 containerd，

其套接字路径同样为 unix:///run/containerd/containerd.sock。

timeout: 设置与容器运行时和镜像服务之间的通信超时时间（以秒为单位）。在这个例子中，超

时时间设置为 10 秒。

这些配置项的目的是让 crictl 工具能够正确找到并与 containerd 运行时和镜像服务通信，以便

进行容器和镜像的管理操作。通过正确配置这些参数，crictl 工具可以与容器运行时进行交互，例如

创建、删除、调试容器等，并与镜像服务进行交互，例如拉取、推送镜像等。

# 3.3.3 buildkit 安装

我们在进行微服务部署时，要进行镜像的创建和管理，对于使用containerd 的k8s 一般我们不使

用docker 工具，而是使用buildkit、nerdctl 等工具。

buildkit 由两部分组成：

buildkitd（服务端）：负责镜像构建，目前支持runc 和containerd 作为镜像构建环境，默认是runc

buildkitctl（客户端）：负责解析Dockerfile 文件，并向服务端buildkitd 发出构建请求。

buildktit 安装

⚫
下载安装包

wget https://github.com/moby/buildkit/releases/download/v0.12.4/buildkit-v0.12.4.linux-amd64.tar.gz

⚫
解压并将文件放在指定的目录

mkdir -p ./buildkit
tar xf buildkit-v0.12.4.linux-amd64.tar.gz -C ./buildkit/

---
**第 82 页**

76

mv ./buildkit/bin/* /usr/local/bin/
rm -rf ./buildkit

准备service 文件

⚫
编辑文件/lib/systemd/system/buildkit.socket

vim  /lib/systemd/system/buildkit.socket

将下面内容加入文件中

#######################################
[Unit]
Description=BuildKit
Documentation=https://github.com/moby/buildkit
[Socket]
ListenStream=%t/buildkit/buildkitd.sock
SocketMode=0660
[Install]
WantedBy=sockets.target

⚫
编辑文件/lib/systemd/system/buildkit.service

vim  /lib/systemd/system/buildkit.service

将下面内容加入文件中

#########################################
[Unit]
Description=BuildKit
Requires=buildkit.socket
After=buildkit.socket
Documentation=https://github.com/moby/buildkit
[Service]
ExecStart=/usr/local/bin/buildkitd --oci-worker=false --containerd-worker=true
[Install]
WantedBy=multi-user.target

启动服务

sudo systemctl enable buildkit.service
sudo systemctl start buildkit.service

验证安装是否成功

输入buildctl -version ，如果安装成功会显示版本号等信息。

---
**第 83 页**

77

图 3-3 验证buildctl 版本

# 3.3.4 nerdctl 安装和使用

如果习惯使用buildkit 也可以不装nerdctl。但buildkit 的命令与docker 工具的命令差异较大，

如果熟悉docker 工具，可以安装nerdctl，这是一个较新的containerd 工具，兼容Docker 命令行工具，

比ctr 覆盖更全面，另外还支持docker-compose(不包括swarm)以及一些可选的高级特性。

# 3.3.4.1 安装

安装过程如下，首先创建一个临时目录，进入该目录，将文件下载到该目录，解压文件，将解

压后的nerdctl 拷贝到 /usr/local/bin 中，然后删除临时目录。

下载文件

wget https://github.com/containerd/nerdctl/releases/download/v1.7.2/nerdctl-1.7.2-linux-amd64.tar.gz
#创建临时目录用于解压文件

mkdir -p nerdctl

#解压

tar -xf nerdctl-1.7.2-linux-amd64.tar.gz -C ./nerdctl/

#将工具考入系统目录

cp ./nerdctl/nerdctl /usr/local/bin/

#删除临时目录

rm -r nerdctl
要特别说明的是：nerdctl 操作的是 containerd 而非 docker，所以 nerdctl images 和 docker

images 看到的内容不同，它只是用法保持了 docker cli 的习惯，实质上操作的是 containerd。

修改默认命名空间：

mkdir -p /etc/nerdctl
vim /etc/nerdctl/nerdctl.toml

加入如下内容：

namespace = "k8s.io"
debug = false
debug_full = false
insecure_registry = true

nerdctl 有namespace 概念，默认使用的namespace 可以在/etc/nerdctl/nerdctl.toml 指定！！！

namespace = "k8s.io"如果不指定，要想显示机器上的镜像需要加上命名空间这个参数，比如显示

命名空间“ k8s.io”下的镜像，命令如下：

nerdctl -n k8s.io ps -a

---
**第 84 页**

78

# 3.3.4.2 验证

输入nerdctl -v 如果显示类似如下的信息表示安装成功。

图 3-4 查看nerdctl 版本

主要命令（注意在安装了containerd 后下面的命令才能正常运行）：

nerdctl 是一个与Docker 兼容的CLI 工具，用于管理容器和镜像。下面是一些nerdctl 的主要命

令和示例说明：

nerdctl run: 运行一个新的容器。

示例：nerdctl run --name mynginx -p 8080:80 nginx（运行一个名为mynginx 的容器，将容器的80

端口映射到主机的8080 端口，并使用nginx 镜像）

nerdctl ps: 列出当前正在运行的容器。

示例：nerdctl ps（列出所有正在运行的容器）

nerdctl images: 列出本地的镜像。

示例：nerdctl images（列出所有本地的镜像）

nerdctl pull: 从远程仓库拉取镜像。

示例：nerdctl pull nginx（从Docker Hub 拉取最新的nginx 镜像）

nerdctl stop: 停止正在运行的容器。

示例：nerdctl stop mynginx（停止名为mynginx 的容器）

nerdctl rm: 删除已停止的容器。

示例：nerdctl rm mynginx（删除名为mynginx 的容器）

nerdctl exec: 在正在运行的容器中执行命令。

示例：nerdctl exec mynginx ls /usr/share/nginx/html（在名为mynginx 的容器中执行命令，列出

nginx 容器内的目录）

# 3.3.5 命令对照表

Docker、nerdctl、crictl 和ctr 都是与容器相关的工具，具体作用如下：

Docker: Docker 是一个开源的容器化平台，用于开发、打包、部署和运行应用程序。它提供了容

器的构建、管理和编排等功能，使得开发人员可以更加方便地使用容器技术进行应用程序的开发和

部署。

nerdctl: nerdctl 是一个与 Docker 兼容的命令行工具，用于管理容器和镜像。它提供了类似于

Docker 的命令行接口，并且支持 Docker Compose，但是 nerdctl 是一个纯粹的容器运行时，相比于

---
**第 85 页**

79

Docker，nerdctl 更加轻量级。

crictl: crictl 是一个用于与 Kubernetes CRI（Container Runtime Interface）兼容的容器运行时进行

交互的命令行工具。它可以用于管理容器、镜像和容器运行时的状态等操作。

ctr: ctr 是一个用于与 containerd 容器运行时进行交互的命令行工具。containerd 是一个开源的

容器运行时，ctr 可以用于管理容器、镜像、快照等操作。

总的来说，这些工具都是用于管理容器和镜像的命令行工具，它们提供了对容器运行时和容器

管理的支持，可以用于在不同的容器平台上进行容器的管理和操作。

表 3-1 docker、nerdctl、crictl 及ctr 主要命令对照表

docker
nerdctl
crictl
ctr
备注

docker run
nerdctl run
crictl run
ctr run
-
docker exec -it
NAME|ID sh

nerdctl exec -it
NAME|ID bash

crictl exec -it
container-id sh

ctr t exec -t --
exec-id
CONTAINER sh

进入容器

docker ps
nerdctl ps
crictl ps
ctr c ls
-
docker start
NAME|ID

nerdctl start
crictl start
ctr t start
-

docker stop
NAME|ID

nerdctl stop
crictl stop
-
-

docker stats
NAME|ID

nerdctl stats
crictl stats
-
容器资源使用
情况
docker inspect
NAME|ID

nerdctl inspect
crictl inspect
container-id

ctr c info
容器对象信息

docker rm
NAME|ID

nerdctl rm
crictl rm
ctr c rm
-

docker cp
nerdctl cp
-
-
-

docker save
nerdctl save
-
-
-

docker load
nerdctl load
-
-
-

docker commit
nerdctl commit
-
-
-

docker images
nerdctl images
crictl images
ctr i ls
-

docker logs
nerdctl logs
crictl logs
-
-

docker build
nerdctl build
-
-
-

docker rmi
nerdctl rmi
crictl rmi
ctr i rm
-

docker pull
nerdctl pull
crictl pull
ctr i pull
-

docker tag
nerdctl tag
-
ctr i tag
-

---
**第 86 页**

80

# 3.4 Kubernetes 安装

# 3.4.1 什么是Kubernetes

Kubernetes 是一个开源的容器编排和管理平台，用于自动化部署、扩展和管理容器化应用程序。

它提供了一个可靠的、可扩展的平台，用于在大规模的集群中运行和管理容器化应用程序。

Kubernetes 的设计目标是简化应用程序的部署和管理，提供高可用性、弹性和可伸缩性。它通过

使用容器化技术（如Docker）来实现应用程序的隔离和可移植性，并提供丰富的功能和工具来管理

容器化应用程序的生命周期。

Kubernetes 的主要特点和功能包括：

自动化部署：Kubernetes 可以自动化地部署容器化应用程序，并根据应用程序的需求自动调度

容器到合适的节点上运行。

自动化扩展：Kubernetes 可以根据应用程序的负载和资源需求自动扩展或缩减容器的数量，以

满足变化的工作负载。

自动化故障恢复：Kubernetes 具有自动检测和恢复故障的能力，它可以重新启动失败的容器或

迁移容器到其他可用节点上。

自动化负载均衡：Kubernetes 提供内置的负载均衡功能，可以将流量均匀地分发到运行中的容

器上，以确保应用程序的高可用性和性能。

配置和存储管理：Kubernetes 提供了对应用程序配置和存储的集中管理，可以动态地管理应用

程序的配置文件和存储卷。

自动化监控和日志：Kubernetes 集成了监控和日志收集工具，可以帮助用户实时监控应用程序

的健康状态和性能指标。

通过这些功能，Kubernetes 使得容器化应用程序的部署、扩展和管理变得更加简单、可靠和高

效。它已成为云原生应用程序部署的事实标准，并被广泛应用于各种规模的生产环境中。

# 3.4.2 本实验中的系统拓扑结构

以下是在 Ubuntu 22.0 上安装 Kubernetes 1.28 的详细步骤，前面已经对系统中的防火墙、交换

区、IP 等进行了设置，下面我们安装一个 master 节点和两个 worker 节点。

安装后的系统拓扑结构如下图所示：

docker push
nerdctl push
-
ctr i push
-

docker login
nerdctl login
-
-
-

docker logout
nerdctl logout
-
-
-

-
nerdctl
namespace ls

-
ctr ns ls
查看containerd
命名空间

---
**第 87 页**

81

图 3-5 系统拓扑结构

# 3.4.3 安装kubernetes（所有机器都装）

(1). 安装基础环境

#安装一些必要的软件和工具，包括ca-certificates、curl、software-properties-common 和apt-

transport-https 等。

sudo apt-get install -y ca-certificates curl software-properties-common apt-transport-https curl

从https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg 下载公钥文件，并将该公钥添加到系

统的APT 密钥库中。这个公钥文件用于验证从Kubernetes 官方仓库下载的软件包的真实性和完整

性。

sudo curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -
# 执行配置k8s 阿里云源

sudo vim /etc/apt/sources.list.d/kubernetes.list

#加入以下内容

deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main

(2). 执行更新

apt update

---
**第 88 页**

82

#查看kubeadm kubelet kubectl 有哪些版本，以及版本安装时的具体名称，例如1.28.2 是错误的，

要是1.28.2-00

apt-cache madison kubeadm kubelet kubectl

#安装指定版本的kubeadm kubelet kubectl

sudo apt update && \

sudo apt-get -y install kubelet=1.28.2-00 kubeadm=1.28.2-00 kubectl=1.28.2-00  # 安装的时候需要

指定版本，否则会安装最新版本

sudo apt-mark hold kubelet kubeadm kubectl  #阻止软件自动更新

systemctl start kubelet
systemctl enable kubelet

查看安装的情况以及版本

kubectl version --client && kubeadm version

(3). 所有结点都执行

mkdir /etc/containerd
containerd config default > /etc/containerd/config.toml
#注意上面的文件是没有的，需要手动创建

#最终把这个文件里面的sandbox_image 改成下面的形式

sandbox_image = "registry.aliyuncs.com/google_containers/pause:3.9"

#文件中的SystemdCgroup 改成如下

SystemdCgroup = true

#重新启动服务

systemctl daemon-reload
systemctl restart containerd.service

# 3.4.4 初始化master 节点（只在主节点Master 上运行，但其它结点不要关机）

执行下面命令或获取镜像后再执行下面命令（获取镜像方法见下面，如果使用本课程提供的

ubuntu 系统，容器镜像文件已经导入了）。

其中--apiserver-advertise-address=192.168.79.133 里的IP 是master 结点的IP，要改为你的master

结点的IP。

（下面命令为一行）

kubeadm
init
--apiserver-advertise-address=192.168.79.133
--image-
repository=registry.aliyuncs.com/google_containers
--kubernetes-version=v1.28.2
--service-
cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=all

如果提示：“/proc/sys/net/bridge/bridge-nf-call-iptables does not exist”这个错误，则需要在这个结

点上重新运行一下：

modprobe br_netfilter

---
**第 89 页**

83

sysctl --system
上面的命令当本地不存在所需要的镜像时会自动将镜像仓库中的镜像下载到本地，然后再进行

安装，如果无法访问镜像仓库或网速太慢，都会因超时而导至安装失败。因此可以先下载镜像再安

装。

可使用下面方法先将镜像下载到所有机器中（注意是所有的结点master 和worker 结点都要下

载）。

# 1.获取镜像文件：先从已经安装的k8s 的机器上或镜像仓库把镜像文件导出。（如果已经有导出

的镜像文件了可省略此步）。

同学们可以到这里下载导出的镜像文件：https://pan.hit.edu.cn/l/8FsAU9 提取码:(cfqz) ，进入这

个文件夹“k8s1.28images”下载所需要的文件，因此同学们不需要执行下面的导出命令了。

导出命令如下：

nerdctl save -o coredns.tar  registry.aliyuncs.com/google_containers/coredns:v1.10.1
nerdctl save -o etcd.tar  registry.aliyuncs.com/google_containers/etcd:3.5.9-0
nerdctl save -o kube-apiserver.tar  registry.aliyuncs.com/google_containers/kube-apiserver:v1.28.2
nerdctl save -o kube-controller-manager.tar   registry.aliyuncs.com/google_containers/kube-
controller-manager:v1.28.2

nerdctl save -o kube-proxy.tar  registry.aliyuncs.com/google_containers/kube-proxy:v1.28.2
nerdctl save -o kube-scheduler.tar   registry.aliyuncs.com/google_containers/kube-scheduler:v1.28.2
nerdctl save -o pause.tar  registry.aliyuncs.com/google_containers/pause:3.9

注意，镜像名要与机器中的一致。

导出的文件以.tar 为扩展名。

# 2.导入镜像

将镜像的所有.tar 文件拷贝到 master 结点和worker 结点，然后在master 结点和所有worker 结

点运行如下命令，将镜像导入机器。

nerdctl load -i   coredns.tar
nerdctl load -i   etcd.tar
nerdctl load -i   kube-apiserver.tar
nerdctl load -i   kube-controller-manager.tar
nerdctl load -i   kube-proxy.tar
nerdctl load -i   kube-scheduler.tar
nerdctl load -i   pause.tar

# 3.查看镜像

执行nerdctl  images 命令，查看每台机器上是否有如下镜像

---
**第 90 页**

84

如果已经有了，再执行下面命令，这样很快就会安装完毕:

kubeadm
init
--apiserver-advertise-address=192.168.79.133
--image-
repository=registry.aliyuncs.com/google_containers
--kubernetes-version=v1.28.2
--service-
cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=all

其中--apiserver-advertise-address=192.168.79.133 里的IP 是master 结点的IP，要改为你的master

结点的IP。

如果安装成功后会提示如下类似的信息：

图 3-6 k8s 安装成功后的信息

如果有新结点要加入集群可以用下面命令查看Join 用的token:

kubeadm token create --print-join-command

按照提示执行（k8s-master 上执行）

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf  $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
export KUBECONFIG=/etc/kubernetes/admin.conf

在其它结点上进行如下操作：

mkdir -p $HOME/.kube

然后将k8s-master 结点上的$HOME/.kube/目录下的config 文件 拷贝到其糨结结点的目录

$HOME/.kube。

# 3.4.5 将从节点加入集群（从节点worker1-2 上运行）

kubeadm join 192.168.79.133:6443 --token fw7hu4.war4zon4v2277g2h --discovery-token-ca-cert-
hash
sha256:dcdaa1124ed92228ceb5251d00a8897dc53ec8c8f937710cc92e1fb361f620c9
--ignore-
preflight-errors=all

其中红色的部分替换成安装成功时显示的信息。

---
**第 91 页**

85

加入后在Master 结点上用下面命令查看加入情况

kubectl get nodes

执行后显示如下信息，表明k8s-worker1 和k8s-worker2 已经加入集群，注意第2 列可能显示的

是NotReady，可以继续后面的操作，当安装完calico 并成功运行后，会变成Ready

图 3-7 查看个结点情况

# 3.4.6 安装网络插件calico（Master 上安装）

（1）安装calico

打开https://docs.tigera.io/calico/latest/about，找到如下位置.

图 3-8 Calico 下载界面

在master 上安装Operator ,运行:

kubectl create -f
https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml

如果无法链接到上面的文件，就要通过代理或镜像将文件先下载到Master 上，然后在Master 机

器上直接运行（本课程已经在网盘上提供了tigera-operator.yaml 及下关的文件，可以在指定的网盘下

载）:

kubectl create -f tigera-operator.yaml

tigera-operator.yaml 文件是用于部署和管理 Calico 网络策略和安全解决方案的配置文件。

Tigera Operator 是 Tigera 公司开发的一个 Kubernetes Operator，用于简化 Calico 和 Tigera Secure

的部署和管理。

运行时会显示如下信息：

---
**第 92 页**

86

图 3-9 运行部署文件时显示的信息

可以通过看下命令看一下命名空间是否有相应的Pod。

kubectl get ns

图 3-10 显示的命名空间

kubectl get pods -n tigera-operator

图 3-11 查看calico 的tigera-operator 是否已经启动

可以看到pod 是处于运行状态，说明Operator 安装成功。

然后进行资源定制，因为下面链接中的配置文件中的集群的pod 网络配置的是192.168.0.0/16 与

我们安装时pod 网络不一致（我们用的是 --pod-network-cidr=10.244.0.0/16），因此需要修改配置然后

才能安装calica。

取出如下页面中的链接，

https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/custom-resources.yaml

将该链接的custom-resources.yaml 文件下载到Master 机器上，然后进行修改。

---
**第 93 页**

87

图 3-12 资源定制文件的链接

主要是要改一下CIDR，将默认的IP 范围改为10.244.0.0/16,我们部署时使用这个IP 范围。CIDR

（Classless Inter-Domain Routing，无类域间路由选择）是用于定义 Pod 网络地址范围的一种标准。

CIDR 地址块用于指定 Kubernetes 集群中每个节点和 Pod 的 IP 地址范围。

CIDR 地址通常表示为一个 IP 地址，后面跟着一个斜杠和一个数字，例如 192.168.0.0/16。这

个数字表示了网络地址的前缀长度，用于确定可用的 IP 地址范围。在 Kubernetes 中，CIDR 地址

块用于划分 Pod 网络地址范围，确保每个 Pod 都有一个唯一的 IP 地址，并且能够进行正确的路

由和通信。

custom-resources.yaml 文件如下图所示的这一行是已经改完的。

图 3-13 custom-resources.yaml 中修改后的cidr

然后运行下面命令，运行时间在几分钟，这个操作也比较慢，因此也可以采用前面的方法先将

---
**第 94 页**

88

镜像下载到所有机器上，然执行下面命令：

kubectl  create -f custom-resources.yaml

# 1.获取镜像文件：先从已经安装的calico 的机器上或镜像仓库把镜像文件导出。（如果已经有导

出的镜像文件了可省略此步）。

同学们可以从下面的网盘或QQ 群文件中下载导出的镜像文件。

https://pan.hit.edu.cn/l/8FsAU9 提取码:(cfqz) ，进入这个文件夹“calicoimages”下载所需要的文件，

因此同学们不需要执行下面的导出命令了。

导出命令如下：

nerdctl save -o calico_apiserver.tar  calico/apiserver:v3.27.0
nerdctl save -o calico_cni.tar  calico/cni:v3.27.0
nerdctl save -o calico_csi.tar  calico/csi:v3.27.0
nerdctl save -o calico_kube-controllers.tar  calico/kube-controllers:v3.27.0
nerdctl save -o calico_node-driver-registrar.tar  calico/node-driver-registrar:v3.27.0
nerdctl save -o calico_node.tar  calico/node:v3.27.0
nerdctl save -o calico_pod2daemon-flexvol.tar  calico/pod2daemon-flexvol:v3.27.0
nerdctl save -o calico_typha.tar  calico/typha:v3.27.0
nerdctl save -o tigera_operator.tar  quay.io/tigera/operator:v1.32.3

注意，镜像名要与机器中的一致。

导出的文件以.tar 为扩展名。

# 2.导入镜像

将镜像的所有.tar 文件拷贝到 master 结点和worker 结点，然后在master 结点和所有worker 结

点运行如下命令，将镜像导入机器。

nerdctl load -i    calico_apiserver.tar
nerdctl load -i    calico_cni.tar
nerdctl load -i    calico_csi.tar
nerdctl load -i    calico_kube-controllers.tar
nerdctl load -i    calico_node-driver-registrar.tar
nerdctl load -i    calico_node.tar
nerdctl load -i    calico_pod2daemon-flexvol.tar
nerdctl load -i    calico_typha.tar
nerdctl load -i   tigera_operator.tar

# 3.查看镜像

执行nerdctl  images 命令，查看每台机器上是否有如下镜像

---
**第 95 页**

89

如果已经有了上面的镜像，再执行下面命令，这样很快就会安装完毕:

kubectl  create -f custom-resources.yaml

通过如下命令查看安装状态：

watch kubectl get pods -n calico-system

图 3-14 查看calico-system 的pod 状态

安装成功后如下图所示

图 3-15 正常运行的calico-system 的pod

至此k8s 安装完毕，初安装后的状态如下：

图 3-16 k8s 安装完成后的pod 状态

---
**第 96 页**

90

其他说明：

#需要将相关进程加入到开机自启

使用下面命令打开文件 ~/.profile

vim ~/.profile

将 export KUBECONFIG=/etc/kubernetes/admin.conf 写入到该文件最后一行

图 3-17 加入命令

然后执行：

source ~/.profile

systemctl enable kubelet  #kubelet 开机自启

如果从worker节点使用kubectl get pod 报错，可以将下面语句加入worker节点的文件  ~/.profile

的最后面。

export KUBECONFIG=/etc/kubernetes/kubelet.conf

然后运行：

source ~/.profile
kubectl get pods -A

所有pod 都处于running 状态，表明pod 安装成功。

图 3-18 pod 安装成功的状态

使用下面命令查看node 状态：

kubectl get nodes -A

---
**第 97 页**

91

图 3-19 结点的状态

显示所有node 的status 都是ready，则表明网络插件已经正常运行。

# 3.5 K3s 安装（如果按“3.4.Kubernetes 安装”进行了安装则跳过此节）

如果不想安装k8s（3.4.Kubernetes 安装）也可以选择安装k3s 两者选其中之一，但推荐按装k8s。

# 3.5.1 设置主机名

# 192.168.79.133/24
k3s-master
192.168.79.134/24
k3s-worker1
192.168.79.135/24
k3s-worker2

分别在三台机器上执行各自的命令。

sudo hostnamectl set-hostname k3s-master    # 192.168.79.133 机器上执行。

sudo hostnamectl set-hostname k3s-worker1   #192.168.79.134 上执行

sudo hostnamectl set-hostname k3s-worker2   # 192.168.79.135 上执行

将下面信息同时写入三台机器的/etc/hosts 中

# 192.168.79.133/24
k3s-master
192.168.79.134/24
k3s-worker1
192.168.79.135/24
k3s-worker2

操作方法（在每台机器上都要执行）：

cat >> /etc/hosts <<EOF
192.168.79.133/24
k3s-master
192.168.79.134/24
k3s-worker1
192.168.79.135/24
k3s-worker2
EOF

# 3.5.2 配置并更新源

sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak
sudo vim /etc/apt/sources.list

用下面内容替原sources.list 中的内容

deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse

---
**第 98 页**

92

deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse

然后执行：

sudo apt-get update
sudo apt-get upgrade

# 3.5.3 关闭防火墙、交换区、优化内核（所有机器）

sudo ufw disable

优化内核参数

编辑/etc/sysctl.conf 将下面两行加入该文件中

net.ipv4.ip_forward = 1
net.ipv4.conf.all.proxy_arp = 1

使新配置生效

sysctl -p /etc/sysctl.conf

安装selinux-utils

apt install selinux-utils
# 关闭交换区

sudo swapoff -a

#永久关闭交换区

sed -i '/swap/s/^\(.*\)$/#\1/g' /etc/fstab

查看是否已经关闭

cat  /etc/fstab
# /etc/fstab: static file system information.
#
# Use 'blkid' to print the universally unique identifier for a
# device; this may be used with UUID= as a more robust way to name devices
# that works even if disks are added and removed. See fstab(5).
#
# <file system> <mount point>   <type>  <options>       <dump>  <pass>
# / was on /dev/ubuntu-vg/ubuntu-lv during curtin installation
/dev/disk/by-id/dm-uuid-LVM-
cqBroz4h7zMStZQGAZv41xiCb4vpakxU1D4gt7nOhKl9hID2yP0f1LriNWp3wKdq / ext4 defaults 0 1

# /boot was on /dev/sda2 during curtin installation
/dev/disk/by-uuid/05595787-2ace-4079-9118-a98adf54b051 /boot ext4 defaults 0 1
#/swap.img      none    swap    sw      0       0

---
**第 99 页**

93

# 3.5.4 开始安装k3s

k3s 集群默认安装最新版本,其中集成了containerd、Flannel、CoreDNS 等组件。

国内脚本部署安装，如需二进制安装参考官网安装手册

# 3.5.4.1 安装master 结点上的组件

使用国内镜像源进行安装命令如下：

curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \
INSTALL_K3S_MIRROR=cn \
K3S_TOKEN=12345 sh -s - \
--system-default-registry=registry.cn-hangzhou.aliyuncs.com

运行后如果成功会显示如下信息：

（如果想卸载上述安装执行/usr/local/bin/k3s-uninstall.sh）

查看是否安装成功

sudo k3s -v
k3s version v1.29.3+k3s1 (8aecc26b)
go version go1.21.8
sudo kubectl get nodes
k3s-master   Ready      control-plane,master   18h   v1.29.3+k3s1

提取加入worker 结点用的token：

sudo cat /var/lib/rancher/k3s/server/node-token

运行后可以看到类似如下的token：

K1035fde72690c4f522bb73e87dd7ad70089afeb7a169a0603c32cf229d3f511d50::server:12345

为了解决应用从docker.io 下载镜像的问题，部署完k3s 后新增配置文件registries.yaml，

cat > /etc/rancher/k3s/registries.yaml <<EOF
mirrors:

---
**第 100 页**

94

docker.io:
endpoint:
- "https://registry.cn-hangzhou.aliyuncs.com/"
- "https://mirror.ccs.tencentyun.com"
quay.io:
endpoint:
- "https://quay.tencentcloudcr.com/"
registry.k8s.io:
endpoint:
- "https://registry.aliyuncs.com/v2/google_containers"
gcr.io:
endpoint:
- "https://gcr.m.daocloud.io/"
k8s.gcr.io:
endpoint:
- "https://registry.aliyuncs.com/google_containers"
ghcr.io:
endpoint:
- "https://ghcr.m.daocloud.io/"
EOF

然后重新启动k3s：

systemctl restart k3s

# 3.5.4.2 将worker 加入k3s 集群

为将worker 加入k3s 集群，通常需要在worker 节点上运行一个命令，以连接到k3s 服务器并加

入集群。以下是加入k3s 集群的基本格式：

curl
-sfL
tps://rancher-mirror.rancher.cn/k3s/k3s-install.sh
|
INSTALL_K3S_MIRROR=cn
K3S_URL=https://<k3s-server-ip>:6443 K3S_TOKEN=<node-token> sh -

要加入集群，您需要替换以下内容：

<k3s-server-ip>: 您的k3 的master 的IP 地址或域名。

<node-token>: 已在服务器上生成的用于worker 节点加入的令牌。

请确保您在添加worker 节点时使用正确的服务器IP 地址和节点令牌。

curl
-sfL
https://rancher-mirror.rancher.cn/k3s/k3s-install.sh
|
INSTALL_K3S_MIRROR=cn
K3S_URL=https://192.168.79.133:6443
K3S_TOKEN=K1035fde72690c4f522bb73e87dd7ad70089afeb7a169a0603c32cf229d3f511d50::serve
r:12345 sh -

其中192.168.79.133 是master 结点的IP

查看集群状态，运行下面命令：

sudo kubectl get nodes

显示如下信息，则说明安装成功，集群已经建立起来了。

---
**第 101 页**

95

NAME          STATUS   ROLES                  AGE   VERSION
k3s-master    Ready    control-plane,master   15m   v1.29.6+k3s1
k3s-worker1   Ready    <none>                 19s   v1.29.6+k3s1
k3s-worker2   Ready    <none>                 4s    v1.29.6+k3s1

k3s 也是systemd 起的也可以查看

sudo systemctl status k3s

● k3s.service - Lightweight Kubernetes

Loaded: loaded (/etc/systemd/system/k3s.service; enabled; vendor preset: enabled)

注意为了与前面k8s 中执行nerdctl 命令的方式一致首先执行下面命令

alias nerdctl='nerdctl --address /run/k3s/containerd/containerd.sock --namespace k8s.io'

为了使系统启动后上面的别名仍然有效，需要将其加到启动配置文件~/.bashrc 中：

vim ~/.bashrc

注：如果不进行上面别名的设置就需要像下面这样执行命令，nerdctl 命令与前面的k8s 执行命

令的方法不同，需要用如下方式执行nerdctl 命令。查看所有镜像

nerdctl -H /run/k3s/containerd/containerd.sock --namespace=k8s.io 具体命令

或者

nerdctl --address /run/k3s/containerd/containerd.sock --namespace k8s.io ps

例如查看镜像：

nerdctl -H /run/k3s/containerd/containerd.sock --namespace=k8s.io images

执行下面命令查看master 上pod 是否正常：

kubectl get pods -A

# 4 容器及编排技术实践

容器是一种轻量级的、可移植的软件包装技术，它将应用程序及其所有依赖项（例如库、配置

---
**第 102 页**

96

文件等）打包在一起，以便可以在不同的计算环境中运行。容器提供了一种可隔离的运行环境，使

应用程序在不同的计算环境中具有一致的行为。

容器编排技术则是指将多个容器组合在一起，形成一个完整的应用程序。容器编排技术可以管

理和协调多个容器的运行，包括容器的部署、扩展、监控和更新等操作。容器编排技术可以确保容

器之间的相互通信和协作，并提供高可用性、弹性和可扩展性等特性。

常见的容器编排技术包括Docker Swarm、Kubernetes 和Apache Mesos 等。这些技术提供了一套

工具和API，使开发人员和系统管理员能够轻松地管理和编排容器化的应用程序。容器编排技术可

以帮助企业实现快速部署和扩展应用程序的目标，提高应用程序的可靠性和性能。

我们将使用Kubernetes 进行容器编排。

# 4.1 如何部署微服务

要在k8s 中部署一个微服务，通常需要涉及以下几个步骤：

# 1.数据库集群的安装部署。安装部署微服务所用的数据库这里我们部署的是Mysql 集群。

# 2.微服务的环境变量配置文件的编写：用于定义微服务在k8s 集群中运行时所需的环境变量，如

数据库连接信息、日志级别等。

# 3. 微服务打包成Jar 文件。

# 4. 微服务镜像的创建和保存。编写Dockerfile 用于构建Docker 镜像的文件，其中定义了微服务

的运行环境和依赖。这部分前面已经讲解过了。通过这类文件生成每个微服务的镜像。

# 5. 其他配置文件：根据微服务的实际需求，可能还需要编写其他配置文件，如配置文件、证书

文件等。

# 6. 微服务的编排和部署。编写Kubernetes 部署文件，包括Deployment、Service、Ingress 等资源

的配置文件，用于定义微服务的部署和暴露方式。使用部署文件进行微服务部署。

# 4.2 Mysql 集群部署

将 MySQL 部署到 Kubernetes 上时，使用 NFS 存储会更加可靠。那么接下来，我们详细介绍

如何使用 NFS 存储来部署 MySQL 容器。

# 4.2.1 在Kubernetes 下使用容器安装MySQL 数据库的步骤

在Kubernetes 上采用容器方式安装MySQL 时，需要创建PV（PersistentVolume）、PVC

（PersistentVolumeClaim）、ConfigMap、Secret、StatefulSet 和Service 等组件，它们之间的关系如下：

PV（PersistentVolume）和PVC（PersistentVolumeClaim）：PV 是Kubernetes 中的持久化存储资

源，而PVC 是用户对PV 的申请。PV 是由集群管理员创建和管理的，而PVC 是用户在Kubernetes

中声明对PV 的需求。PVC 可以动态地绑定到PV，用于提供MySQL 数据库的持久化存储。

ConfigMap：ConfigMap 用于将配置数据以键值对的形式存储在Kubernetes 集群中，然后将其挂

载到MySQL 容器中。通过ConfigMap，可以将MySQL 的配置文件或配置参数存储在Kubernetes 中，

使得MySQL 容器能够读取配置信息。

Secret：Secret 用于存储敏感数据，如密码、密钥等。在MySQL 安装过程中，可以将MySQL 的

---
**第 103 页**

97

用户名和密码等敏感信息存储在Secret 中。然后，将Secret 挂载到MySQL 容器中，以供MySQL 访

问，确保敏感信息的安全性。

StatefulSet：StatefulSet 是Kubernetes 中用于管理有状态应用程序的资源对象。在MySQL 的安

装过程中，可以使用StatefulSet 来创建多个MySQL 实例，为每个实例分配唯一的网络标识和持久化

存储。StatefulSet 确保MySQL 实例的顺序部署、伸缩和升级，以及稳定的网络标识和存储。

Service：Service 是Kubernetes 中用于暴露应用程序的网络服务的一种资源对象。在MySQL 的

安装过程中，可以创建一个Service 来MySQL 实例提供网络访问。Service 将MySQL 实例暴露给其

他应用程序或用户，使其能够通过网络访问MySQL 数据库。

综上所述，PV、PVC、ConfigMap、Secret、StatefulSet 和Service 等组件在Kubernetes 中协同工

作，实现了MySQL 数据库的容器化部署、持久化存储、配置管理、敏感信息保护、有状态应用管理

和网络访问等功能。这些组件共同构成了MySQL 数据库在Kubernetes 上的完整部署和运行环境。

# 4.2.2 NFS 安装

NFS，全称为 Network File System，是一种网络文件系统协议，用于实现不同操作系统之间的共

享文件访问。它最初由 Sun Microsystems 开发，现在已经成为一个开放的标准化协议。

NFS 可以让多个计算机通过网络共享文件，类似于本地文件系统的操作方式，使得不同计算机

之间的文件共享变得更加简单和高效。通过 NFS 协议，客户端可以像访问本地文件一样访问远程

服务器上的文件，而无需了解底层的网络细节和具体实现方式。NFS 支持文件读写、锁定、权限控

制等功能，并支持多种操作系统的文件系统类型，例如 Linux、Unix、Mac OS X 等。

在 Kubernetes 集群中，NFS 通常被用于数据持久化方案，特别是在有状态应用容器化时，将数

据存储到共享的 NFS 存储卷中，这样即使容器重启或者 Pod 被重新调度到其他节点上，数据也能

够保留下来。因此，Kubernetes 中的数据库应用（如 MySQL）通常都会使用 NFS 存储来确保数据

的可靠性和持久性。

# 4.2.2.1 在master 结点安装nfs-server

在存话数据库文件的机器上执行（如果数据库话在master 结点上，则在该节点上运行）：

apt-get install -y nfs-kernel-server

#在master(192.168.79.133)机器创建nfs 目录作为共享文件目录:

mkdir -p /nfs/data

mkdir -p /nfs/data/mysql

#修改权限 给文件夹增加读写权限

chmod a+rw /nfs/data && chmod a+rw /nfs/data/mysql

配置NFS 服务目录，编辑nfs 的配置文件/etc/exports，

vim /etc/exports

在尾部新增内容如下：

---
**第 104 页**

98

/nfs/data *(rw,sync,no_subtree_check,no_root_squash,insecure)
/nfs/data/mysql *(rw,sync,no_subtree_check,no_root_squash,insecure)

图 4-1 加入内容

说明:

➢
*表示任何IP 都可以访问.

➢
rw 是读写权限.

➢
sync 是同步权限，

➢
no_subtree_check 表示如果输出目录是一个子目录，nfs 服务器不检查其父目录的权限.

➢
no_root_squash 登入 NFS 主机使用分享目录的使用者，如果是 root 的话，那么对于这个

分享的目录来说，他就具有 root 的权限

让配置生效

exportfs -r 命令的作用是重新导出所有已经在/etc/exports 文件中定义的共享目录，以便新的配置

生效。sudo 表示以超级用户权限运行命令，exportfs 是用于管理NFS 共享的命令，-r 表示重新导出

所有的共享目录。

sudo exportfs -r

#查看结果

sudo exportfs

显示类似如下的信息：

/nfs/data      <world>
运行如下命令，启动rpcbind、nfs-server 服务

sudo systemctl restart rpcbind && systemctl enable rpcbind

sudo systemctl restart nfs-server && systemctl enable nfs-server

查看RPC 服务的注册情况

sudo rpcinfo -p localhost | grep 'nfs'
100003    3   tcp   2049  nfs
100003    4   tcp   2049  nfs2

运行:

sudo showmount -e 192.168.79.133

---
**第 105 页**

99

这个命令的作用是显示指定主机（192.168.79.133）上共享的所有NFS 文件系统。通过运行

showmount 命令，可以查看远程主机上当前共享的所有NFS 文件系统，以便远程客户端可以访问这

些共享。

它显示了IP 地址为192.168.79.133 的主机上的NFS 共享目录列表：

Export list for 192.168.79.133:
/nfs/data *

到这里，NFS 服务端就准备好了，接下来我们准备客户端。

# 4.2.2.2 在所有结点安装NFS 客户端

在所有机器上执行如下命令，安装客户端应用。

sudo apt-get install -y nfs-common

在客户端创建挂载点，然后将服务器的文件夹挂载到本地

sudo mkdir -p /nfs/data

sudo mount 192.168.79.133:/nfs/data  /nfs/data

查看挂载情况：

sudo df -Th | grep '192.168.79.133'

# 192.168.79.133:/nfs/data nfs4       40G   20G   18G  54% /data/test/nfs

可以看到，40G 空间，这个空间是nfs 服务端的空间。此时已经挂载成功。

NFS 的测试

在客户端目录 /data/test 下创建一个文件夹并存入文件

sudo mkdir -p /nfs/data/test

sudo cat > /nfs/data/test/test1.txt <<EOF

我是从客户端192.168.79.134 写入的

EOF

在服务端目录 /nfs/data/test 查看

sudo ls /nfs/data/test
test1.txt
sudo cat /nfs/data/test/test1.txt

我是从客户端192.168.79.134 写入的

可以看到文件已经写入到共享目录了。

注：

如果要取消挂载，可执行命令：sudo umount /data/test/nfs

---
**第 106 页**

100

查看文件：sudo ls /data/test/nfs/

可以看到客户端文件已经消失。

注意：取消挂载后客户端文件消失服务端文件还存在。

# 4.2.3 MySql 集群安装

# 4.2.3.1 创建 PV

在 Kubernetes 上使用 NFS，您需要创建一个 PersistentVolume，它会描述 NFS 服务器的存储

空间。在此示例中，我们将在 NFS 服务器中创建一个名为 /nfs/data/mysql 的目录，并将该目录映射

到一个 PersistentVolume。

执行如下命令创建目录：

sudo mkdir -p /nfs/data/mysql

我们创建一个名为 mysql-pv.yaml 的文件，其中包含了如下配置：

apiVersion: v1

kind: PersistentVolume

metadata:

name: mysql-pv

labels:

volume-type: mysql

spec:

capacity:

storage: 10Gi

accessModes:

- ReadWriteMany

storageClassName: my-storage-class       # 修改存储类名称

nfs:

server: 192.168.79.133 # 这里填写真实的NFS 服务器IP 地址

path: /nfs/data/mysql   # 这里填写共享目录路径

其中，server 字段需要填写NFS 服务器的IP 地址，path 字段需要填写NFS 共享目录的路径。

然后，使用以下命令创建 PersistentVolume：

sudo kubectl create -f mysql-pv.yaml

---
**第 107 页**

101

可以通过下面命令查看创建结果,可以年到名为mysql-pv 的pv 创建成功：

sudo kubectl get pv

图 4-2 显示创建的pv

# 4.2.3.2 创建pvc

PVC 和PV 之间没有依靠ID、名称或者label 匹配，而是靠容量和访问模式，PVC 的容量和访

问模式需要是某个PV 的子集才能自动匹配上。注意：PVC 和PV 是一对一的，也即一个PV 被一个

PVC 自动匹配后，不会再被其它PVC 匹配了，即使PVC 需求能够完全满足。

创建一个 PersistentVolumeClaim，它将请求上面创建的 PersistentVolume。创建一个名为 mysql-

pvc.yaml 的文件，其中包含如下配置：

mysql-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: mysql-pvc
namespace: default
spec:
accessModes:
- ReadWriteMany
resources:
requests:
storage: 10Gi

volumeName: mysql-pv                    # 显式引用 PersistentVolume

storageClassName: my-storage-class      # 指定使用的存储类名称

selector:
matchLabels:
volume-type: mysql

其中，metadata.name 字段指定了PVC 的名称，spec.accessModes 字段指定了PVC 的访问模式，

这里是ReadWriteMany，表示多个Pod 可以同时挂载这个PVC。spec.resources.requests.storage 字段

指定了PVC 的存储大小，这里是10GB。

使用以下命令创建 PersistentVolumeClaim：

sudo kubectl create -f mysql-pvc.yaml

可以通过下面命令查看创建结果,可以年到名为mysql-pv 的pv 创建成功：

sudo kubectl get pvc

---
**第 108 页**

102

# 4.2.3.3 创建ConfigMap

接下来需要创建一个ConfigMap 来存储MySQL 的配置文件。创建ConfigMap 的配置文件mysql-

configmap.yaml 如下：

apiVersion: v1
kind: ConfigMap
metadata:
name: mysql-config
data:
my.cnf: |
[mysqld]
bind-address=0.0.0.0
innodb_buffer_pool_size=512M
character-set-server=utf8mb4

collation-server=utf8mb4_unicode_ci

使用以下命令创建 PersistentVolumeClaim：

kubectl create -f mysql-configmap.yaml

使用下面命令可以查看创建结果

kubectl get cm

图 4-3 查看配置文件

# 4.2.3.4 创建secret

Kubernetes Secret 是用于存储和管理敏感信息（如密码、密钥和证书）的Kubernetes API 对象。

它可以用于安全地存储敏感数据，并使应用程序可以在不暴露敏感信息的情况下访问它们。与

ConfigMap 不同，Secret 的数据是加密的，并且只有授权的用户和应用程序才能访问它们。在

Kubernetes 中，Secret 通常用于存储以下类型的敏感数据。

下面这个配置文件mysql-secret.yaml 定义了一个名为mysql-secret 的Secret 对象，它包含了

MySQL 的root 用户密码信息，以base64 编码的形式存储在mysql-root-password 字段中。在使用这

个Secret 时，需要将mysql-root-password 字段的值解码为原始密码。其中“cm9vdA==”是“root”的

base64 编码，即原始密码为root。

apiVersion: v1
kind: Secret
metadata:
name: mysql-secret
type: Opaque

---
**第 109 页**

103

data:
#base64_encoded_password
mysql-root-password: cm9vdA==

使用以下命令创建 statefulset：

sudo kubectl create -f mysql-secret.yaml
使用下面命令可以查看创建结果:

sudo kubectl get secret

# 4.2.3.5 创建StatefulSet

创建一个StatefulSet来部署MySQL8.0.26集群。创建StatefulSet的配置文件mysql-statefulset.yaml

如下：

apiVersion: apps/v1
kind: StatefulSet
metadata:
name: mysql
spec:
serviceName: mysql
replicas: 1
selector:
matchLabels:
app: mysql
template:
metadata:
labels:
app: mysql
spec:
containers:
- name: mysql
image: mysql:8.0.26
ports:
- name: mysql
containerPort: 3306
env:
- name: MYSQL_ROOT_PASSWORD
valueFrom:
secretKeyRef:
name: mysql-secret

---
**第 110 页**

104

key: mysql-root-password
volumeMounts:
- name: mysql-data
mountPath: /var/lib/mysql
- name: mysql-config
mountPath: /etc/mysql/conf.d/my.cnf
subPath: my.cnf
volumes:
- name: mysql-data
persistentVolumeClaim:
claimName: mysql-pvc
- name: mysql-config
configMap:
name: mysql-config

其中，mysql-secret 是一个 Secret，用于存储 MySQL 的用户密码。您可以使用以下命令创建名

为 mysql-secret 的 Secret：

使用以下命令创建 statefulset：

sudo kubectl create -f mysql-statefulset.yaml

可以使用下面命令查看创建结果：

sudo kubectl get statefulsets --all-namespaces

# 4.2.3.6 创建 MySQL 服务

创建 MySQL 服务，它将 MySQL Pod 暴露到 Kubernetes 集群。创建名为 mysql-service.yaml 的

服务文件，其中包含以下配置：

apiVersion: v1
kind: Service
metadata:
name: mysql
spec:
type: NodePort
ports:
- name: mysql
port: 3306
targetPort: 3306
selector:
app: mysql

使用以下命令创建服务：

---
**第 111 页**

105

sudo kubectl create -f mysql-service.yaml

# 4.2.3.7 数据库测试

输入命令查看mysql 的pod 名称，可以看到名称为mysql-0：

sudo kubectl get pod -A

图 4-4 查看部署好的mysql pod

使用下面命令进入pod

sudo kubectl exec -it  mysql-0  -- /bin/bash
进入的执行下面命令，然后输入密码root

mysql -u root -p

执行下面命令，可以显示出数据库。

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
4 rows in set (0.02 sec)

显示以上信息说明数据库已经安装完成。

# 4.3 微服务的环境变量配置文件的编写

# 4.3.1 环境变量配置文件的区分

在IDEA 项目中，配置文件通常会被分为application、application-dev 等不同的文件，这是为了

方便在不同的环境下进行配置的管理和切换。

application.properties/application.yml：这是主要的配置文件，包含了应用程序的通用配置信息，

例如数据库连接、日志配置等。这个文件是所有环境都会加载的。

application-dev.properties/application-dev.yml：这是开发环境下的配置文件，包含了开发环境特定

的配置信息，例如开发环境的数据库连接、调试模式等。在开发阶段，开发人员可以根据自己的需

---
**第 112 页**

106

求进行配置。

application-prod.properties/application-k8s.yml：这是生产环境下的配置文件，包含了生产环境特

定的配置信息，例如生产环境的数据库连接、性能优化配置等。在部署到生产环境时，可以使用这

个配置文件。

通过将配置文件分为不同的环境，可以方便地在不同的环境中切换配置信息，避免了在不同环

境中手动修改配置文件的麻烦。同时，也可以确保在不同环境中应用程序的配置是正确的，提高了

配置的可靠性和可维护性。

# 4.3.2 项目启动时如何选择配置文件

可以通过设置spring.profiles.active 属性来选择使用哪个配置文件。具体步骤如下：

打开application.properties 或application.yml 文件。

添加以下内容：

对于application.properties 文件，添加spring.profiles.active=dev 或spring.profiles.active=prod，表

示选择使用开发环境或生产环境的配置文件。

对于application.yml 文件，添加spring: profiles: active: dev 或spring: profiles: active: k8s，表示选

择使用开发环境或生产环境的配置文件。

根据需要，将开发环境的配置信息填写到application-dev.properties 或application-dev.yml 文件中，

将生产环境的配置信息填写到application-prod.properties 或application-prod.yml 文件中。

运行项目时，IDEA 会根据spring.profiles.active 属性的值自动选择加载对应的配置文件。例如，

如果将spring.profiles.active=dev 设置为开发环境，那么IDEA 会加载application-dev.properties 或

application-dev.yml 文件中的配置信息。

通过这种方式，可以根据需要自动选择加载不同环境的配置文件，方便进行开发和部署。

现在我们将idea 中每个微服务下面的再加两个配置文件，分别为application-dev.yml 和

application-k8s.yml，分别用于开发环境时使用和部署到k8s 集群使用。

把application-dev.yml 和application-k8s.yml 中共有的部分放在application.yml 中。

# 4.3.3 修改微服务配置文件

由于部署到k8s 集群与本地开发所使用的配置信息不完全相同，因此要将配置文件分为本地开

发用的和部署到k8s 集群用的。

比如将微服务部署到集群中时涉及到连接地址的地方要使用微服务的URL。比如微服务要向

eureka 注册，需要将微服务的applycation.yml 中的eureka 服务的url 改为域名方式。

完整的service 域名解析是：<servicename>.<namespace>.svc.<clusterdomain> 其中

servicename 为service 名称

namespace 为service 所处的命名空间

clusterdomain 是k8s 集群设计的域名后缀，默认为cluster.local

一般在生产环境中，我们可以直接简写为<servicename>.<namespace>即可，后面的部分保持默

认即可。如果pod 与svc 是在同一个命名空间，那么直接写svc 的名字即可，如 <servicename>。

比如，product-service 要向Eureka 注册，如果eureka 所在的命名空间为“stockmgr”，端口为8888，

则在product-service 的配置文件application.yaml 中eureka 的service-url 可以写成如下形式。

---
**第 113 页**

107

eureka:
client:

service-url: # 配置服务注册地址，与 eureka-server 中暴露地址保持一致

defaultZone: http://eureka-service.stockmgr.svc.cluster.local:8888/eureka

下面以eureka-service 为例说明配置文件的重新编写：

原来的application.yml 文件如下

server:
port: 8010
spring:
application:
name: product-service
datasource:
url: jdbc:mysql://127.0.0.1:3306/tb_product?characterEncoding=utf-8&useSSL=false
username: root
password: root
driver-class-name: com.mysql.jdbc.Driver
eureka:
instance:
prefer-ip-address: true
client:
register-with-eureka: true
fetch-registry: true
service-url:
defaultZone: http://localhost:8888/eureka

现在将application-dev.yml 和application-k8s.yml 的公共部分保留在application.yml 中，不同的

部分放在各自的文件中。

application.yml 文件内容，其中文件中的“active: k8s”表示项目启动后使用application.yml 和

application-k8s.yml 配置文件。

server:
port: 8010
spring:
profiles:
active: k8s
application:
name: product-service

application-dev.yml 文件内容

spring:
application:
name: product-service
datasource:
url: jdbc:mysql://127.0.0.1:3306/tb_product?characterEncoding=utf-8&useSSL=false
username: root
password: root
driver-class-name: com.mysql.jdbc.Driver

---
**第 114 页**

108

eureka:
instance:
prefer-ip-address: true
client:
register-with-eureka: true
fetch-registry: true
service-url:
defaultZone: http://localhost:8888/eureka

application-k8s.yml 文件内容

spring:
datasource:
url: jdbc:mysql://mysql.default:3306/tb_product?characterEncoding=utf-8&useSSL=false
username: root
password: root
driver-class-name: com.mysql.jdbc.Driver
eureka:
instance:
prefer-ip-address: true #如果为false 可能出错找不到服务的错误
client:
register-with-eureka: true
fetch-registry: true
service-url:
defaultZone: http://eureka-service.stockmgr.svc.cluster.local:8888/eureka
instance:
prefer-ip-address: true   #使用ip 地址向eureka server 进行注册
instance-id: ${spring.cloud.client.ip-address}:${server.port}   #设置eureka 控制台中显示的注册
信息
lease-renewal-interval-in-seconds: 5         #续约间隔时间
lease-expiration-duration-in-seconds: 15     #续约到期时间

# 4.4 微服务打包成Jar 文件

# 4.4.1 为什么将微服务打包成jar

通常我们将微服务打包成 JAR 文件，这是一种常见的部署方式，有以下几个优点：

便于部署：将微服务打包成 JAR 文件可以将应用程序、依赖库和配置文件等资源打包在一起，

形成一个独立的可执行文件。这样可以简化部署过程，只需要将 JAR 文件拷贝到目标环境，通过命

令启动即可，不需要额外安装依赖库或配置环境。

便于传输：JAR 文件是一个可执行的二进制文件，可以方便地在不同环境之间传输。无需担心

环境差异导致的兼容性问题，只要目标环境支持 Java 运行环境即可。

便于版本管理：JAR 文件可以包含版本信息，便于管理不同版本的微服务。通过版本控制工具

管理 JAR 文件，可以轻松地进行版本回滚、升级和发布。

便于扩展：JAR 文件可以包含所有需要的依赖库和资源，使得微服务具有较好的独立性和可移

植性。在需要扩展或迁移微服务时，只需要将 JAR 文件部署到新环境即可。

总的来说，将微服务打包成 JAR 文件是一种简单、便捷、可移植的部署方式，有利于提高部署

---
**第 115 页**

109

效率、降低部署成本，并便于管理和维护微服务。

# 4.4.2 Idea 中打包jar 的基本步骤

在 IntelliJ IDEA 中，可以通过以下步骤将 Maven 项目打包成 jar 文件：

# 1.确保你的 Maven 项目已经正确配置了 pom.xml 文件，并且项目可以成功构建。

# 2.在 IDEA 的右侧 Maven Projects 面板中，展开你的 Maven 项目，找到 "Lifecycle "菜单。

# 3.在"Lifecycle"菜单中，双击"package" 命令，或者右键点击"package" 命令并选择"Run Maven

Build" 。

# 4.IDEA 将会执行 Maven 的 package 命令，该命令会将项目打包成一个 jar 文件。

打包成功后，你可以在 Maven Projects 面板的"target "目录下找到生成的 jar 文件。

# 4.5 微服务镜像的创建及保存

# 4.5.1 创建镜像仓库

使用私有仓库或远端公有仓库，用于存放创建的镜像。私有镜像仓库需要自己搭建，要做实验

也可以申请公有免费镜像仓库。下面以申请到的免费的阿里云镜像仓库为例。

登录到仓库后，创建名为my-stock 的镜像仓库。

图 4-5 阿里云上的私有镜像仓库

仓库地址：registry.cn-hangzhou.aliyuncs.com

命名空间：my-stock。

如果不想进行权限验证就可拉取镜像可以将默认仓库类型设为“公开”。

# 4.5.2 创建微服务镜像

Dockerfile 文件中的主要命令：

---
**第 116 页**

110

图 4-6 Dockerfile 中的主要命令

图 4-7 Dockerfile 文件基本结构

这里以eureka-service 为例进行微服务镜像制做。可以先按下面的方式在机器上测试编写的

Dockerfile 文件，在没有问题后将其放在eureka-service 项目中，放在eureka-service 项目根目录下即

可。这个文件在我们进行CI/CD 操作时还会用到。

在k8s-master 机器（或其它安装了docker 工具的机器）上分别给每个服务创建一个目录，然后

编写Dockerfile 文件。

下面是用于生成Eureka 镜像的Dockerfile。

FROM openjdk:8-jre-alpine
LABEL maintainer=me
ENV JAVA_ARGS="-Dfile.encoding=UTF8 -Duser.timezone=GMT+08"
ENV JAR_FILE="eureka-service.jar"
COPY ./${JAR_FILE} ./

---
**第 117 页**

111

EXPOSE 8888
CMD java -jar ${JAVA_ARGS} ${JAVA_OPTS} /${JAR_FILE}

将该Dockerfile 和将生成的jar 包eureka-service.jar 文件拷贝到eureka 文件夹。

进入eureka 文件夹，然后执行下面命令生成镜像，注意在命令的最后有个点“.”不要漏掉。

nerdctl build -t eureka:v1.0 .

运行时会显示创建过程：

图 4-8 用nerdctl 创建Eureka 镜像的过程

如果创建成功可以通过如下命令查看：nerdctl images

图 4-9 用nerdctl 显示已经在机器上的镜像

# 4.5.3 镜像推送到远端镜像仓库

登录镜像仓库。

sudo nerdctl login --username=仓库账号 registry.cn-hangzhou.aliyuncs.com

下面是登录成功后的信息。

图 4-10 登录到远程镜像仓库

然后把镜像推送到远端阿里云镜像仓库。要将镜像推送到镜像仓库的指定命名空间下，还需要

改一下本地镜像的标签（或者在创建镜像时加上与仓库命名空间对应的标签），命令格式：

nerdctl tag <本地镜像名称> registry.cn-hangzhou.aliyuncs.com/<命名空间>/<镜像名称>:<版本号>

修改标签命令具体命令：

nerdctl tag eureka:v1.0 registry.cn-hangzhou.aliyuncs.com/my-stock/eureka

推送镜像命令：

nerdctl push registry.cn-hangzhou.aliyuncs.com/my-stock/eureka

---
**第 118 页**

112

图 4-11 镜像推送到远端仓库的过程

命令执行完毕后可以在阿里云镜像仓库中看到保存的镜像。

图 4-12 远程镜像仓库中看到推送的镜像

测试：

nerdctl run -d --name=eureka -p 8888:8888 registry.cn-hangzhou.aliyuncs.com/my-stock/eureka:v1.0

容器启动后可以用下面命令检查是否正常启动，如果有信息返回则说明容器启动正常。

curl localhost:8888

测试后如果没有问题，请将启动的容器停掉，以免后面在用k8s 进行容器启动时产生端口冲突。

# 4.5.4 用脚本批量创建并推送镜像

前面创建和推送镜像是一个命令一个命令执行来完成的，如果有很多镜像都要创建和推送，前

面的方式操作起来很不方便，因此也可以编写一个脚本，这样可以更方便的进行操作。下面提供了

采用脚本方式，脚本文docker_nerdctl_build.sh 的内容如下：

#!/bin/bash
docker_registry=registry.cn-hangzhou.aliyuncs.com
service_list="eureka-service gateway-service product-service product-client"
service_list=${1:-${service_list}}
work_dir=$PWD
cd $work_dir
#mvn clean package -Dmaven.test.skip=true
for service in $service_list; do
cd $work_dir/$service
image_name=$docker_registry/my-stock/${service}
echo ${image_name}
nerdctl build -t ${image_name} .

---
**第 119 页**

113

nerdctl push ${image_name}
done

脚本具体说明如下：

使用该脚本时，要把目录组织成如下结构

其是docker_nerdctl_build.sh 为脚本文件。

├── docker_nerdctl_build.sh
├── eureka-service
│   ├── Dockerfile
│   └── eureka-service.jar
├── gateway-service
│   ├── Dockerfile
│   └── gateway-service.jar
├── product-client
│   ├── Dockerfile
│   └── product-client.jar
└── product-service
├── Dockerfile
└── product-service-biz.jar

这个 shell 脚本用于构建镜像并推送到阿里云容器镜像服务（Registry）。注意要要先登录到仓库

才能推送。

脚本中涉及以下变量和操作：

docker_registry=registry.cn-hangzhou.aliyuncs.com 为阿里云容器镜像服务的 Registry 地址，可以

改成你使用的镜像仓库地址。

service_list 是一个包含多个服务名称的字符串变量，表示需要构建镜像的服务清单。默认情况

下，服务列表为eureka-service config-service gateway-service product-service product-client。

如果在运行脚本时传入了参数，则使用传入的参数作为服务清单；否则使用默认的 service_list

变量定义的服务清单。

work_dir=$PWD 将当前工作目录保存到 work_dir 变量中。

current_dir=$PWD 同样将当前工作目录保存到 current_dir 变量中，预备进行切换操作。

cd $work_dir 切换到当前工作目录。

for service in $service_list; do... done 循环遍历服务列表，并对其中的每一个服务执行以下操作：

cd $work_dir/$service 切换到该服务所在的目录。

image_name=$docker_registry/my-stock/${service}:v1.0 拼接出镜像名称，其中包括 Registry 地

址、项目名、服务名及镜像版本号。

pwd 打印当前目录的路径，以便查看确认脚本运行的位置。

---
**第 120 页**

114

docker build -t ${image_name} . 基于 Dockerfile 在当前目录（即该服务所在的目录）中构建镜

像，并为该镜像取一个标签 ${image_name}。

docker push ${image_name} 用于把镜像推送到阿里云容器镜像仓库。

运行脚本

./docker_nerdctl_build.sh

会显示创建及推送过程。

图 4-13 批处理脚本执行过程

查看到镜像

nerdctl images -a

图 4-14 在本地生成的镜像

也可以登录远程仓库查看镜像

图 4-15 远程仓库的镜像

# 4.6 微服务的编排及部署

# 4.6.1 部署文件的语法格式

要在k8s 中部署一个微服务，要编写部署文件，K8s 微服务编排部署文件的语法是 YAML

（YAML Ain't Markup Language）。 使用 YAML 文件来定义微服务的部署、服务、配置等信息。

YAML（YAML Ain't Markup Language）语法要求如下：

⚫
缩进：使用缩进来表示层级关系，通常使用空格进行缩进，不允许使用制表符。

⚫
键值对：使用冒号（:）来表示键值对，键值对之间使用空格进行分隔。

---
**第 121 页**

115

⚫
列表：使用短横线（-）来表示列表项，列表项之间使用换行进行分隔。

⚫
注释：使用井号（#）表示注释

⚫
字符串：字符串可以使用单引号（'）或双引号（"）括起来，也可以不使用引号。

⚫
对象和数组：数组使用短横线和空格表示。

⚫
多行字符串：使用管道符号（|）表示多行字符串，使用大于符号（>）表示折叠的多行字符

串。

Kubernetes 中微服务部署文件必须的属性如下图所示：

v

图 4-16 k8s 部署文件必须的属性

在这些文件中，需要定义微服务的名称、镜像名称、端口、环境变量、资源请求等信息，以便

k8s 能够正确地部署和管理微服务。通过这些文件的配置，可以实现微服务在k8s 集群中的自动化部

署、扩展和管理。

# 4.6.2 k8s 中的一些重要概念

# 4.6.2.1 什么是POD

是K8s 最小调度单位。

K8s 使用 Pod 来管理容器，每个 Pod 可包含一个或多个容器。

Pod 内的多个容器共享网络和文件系统。

是一组紧密关联的容器集合，它们共享 PID、 IPC、Network 等。

K8s 通过这种简单高效的方式组合完成服务。

---
**第 122 页**

116

图 4-17 Pod 与容器的关系

# 4.6.2.2 什么是Service（服务）

Service 是 Kubernetes 中用来定义一组 Pod 的访问方式的抽象。Service 提供了一个稳定的网

络端点，以便其他应用程序可以通过该端点访问应用程序的 Pod。Service 可以通过标签选择器来选

择一组 Pod，并为它们提供统一的访问入口。

Service 可以通过 ClusterIP、NodePort、LoadBalancer 等不同类型来暴露应用程序。ClusterIP 类

型的 Service 仅在集群内部可访问，NodePort 类型的 Service 在每个节点上暴露一个端口，

LoadBalancer 类型的 Service 则可以使用云服务商的负载均衡器暴露服务。

# 4.6.2.3 什么是Deployment（部署）

Deployment 是 Kubernetes 中用来管理 Pod 副本数量、更新策略等的控制器。Deployment 可

以确保指定数量的 Pod 副本在集群中运行，并支持滚动更新和回滚操作。

Deployment 通过定义 Pod 模板和副本数量来创建和管理 Pod。当需要更新应用程序时，可以

通过修改 Deployment 的 Pod 模板或者更新策略来实现滚动更新，确保应用程序的高可用性和稳定

性。

# 4.6.2.4 三者的关系

总的来说，Service 主要用于暴露应用程序的访问入口，使其可以被其他应用程序访问，而

Deployment 则主要用于管理应用程序的 Pod 副本数量和更新策略，确保应用程序的正常运行和更

新。在实际应用中，通常会将 Service 和 Deployment 结合使用，通过 Service 提供稳定的访问入

口，通过 Deployment 管理应用程序的副本数量和更新。

虽然 Service 是 Kubernetes 中的重要概念，用于暴露应用程序的访问入口，但通常情况下，微

服务的部署更多地依赖于 Deployment 来管理 Pod 的生命周期和更新过程。因此，一般建议将自编

---
**第 123 页**

117

写的微服务以 Deployment 的形式部署到 Kubernetes 中，以便实现更灵活、自动化的管理和更新。

通过 Deployment 部署微服务，可以实现以下优势：

自动化管理：Deployment 提供了自动化的 Pod 管理功能，可以根据定义的 Pod 模板和副本数

量来创建、删除和更新 Pod，简化了部署和管理过程。

滚动更新：通过 Deployment 可以实现滚动更新，即逐步替换旧版本的 Pod，确保应用程序的

稳定性和可用性。这对于微服务的持续交付和更新非常重要。

健康检查：Deployment 支持对 Pod 的健康状态进行监控和检查，可以根据定义的健康检查策

略来自动处理不健康的 Pod，确保应用程序的正常运行。

水平扩展：Deployment 支持根据负载情况自动进行水平扩展，可以根据定义的 CPU 利用率、

内存使用率等指标来动态调整 Pod 的副本数量，提高应用程序的性能和弹性。

# 4.6.2.5 k8s 部署文件中几个端口的关系

port：这是 Service 暴露出去的端口，其他应用程序可以通过这个端口访问该 Service。

targetPort：这是 Service 路由到 Pod 的目标端口。当其他应用程序通过 Service 访问时，流量

会被路由到 Pod 的端口上。

nodePort：这是 Service 在每个 Node 上暴露出来的端口。通过 Node 的 IP 地址和这个端口，

可以从集群外部访问该 Service。比如配置中，NodePort 为 30001，意味着可以通过集群中任意一个

Node 的 IP 地址和端口 30001 来访问该 Service。

因此，port 是 Service 暴露出去的端口，targetPort 是路由到 Pod 的目标端口，而 nodePort 是

在 Node 上暴露出来的端口，用于从集群外部访问该 Service。如下图所示，为三种端口的关系。

---
**第 124 页**

118

图 4-18 port、nodePort 及targetPort 的关系

# 4.6.3 创建命名空间

Kubernetes 命名空间是一种资源，用于将Kubernetes 集群中的对象分组和隔离，作用类似于我

们磁盘上的文件夹。命名空间提供了一种将集群分割成多个虚拟集群的方法，使不同的团队和项目

可以共享同一个集群而不会互相干扰。在创建命名空间后，可以在其中创建其他Kubernetes 对象，

如Pod、Service、Deployment 等。

可以直接使用kubectl 命令创建命名空间，也可以使用yaml 文件创建命名空间，我们使用下面

命令来创建命名空间：

命令格式：kubectl create namespace my-namespace

其中 “my-namespace”更换为你实际使用的名字，这里我们创建的命名空间为“stockmgr”，则命

令为：

sudo kubectl create namespace stockmgr

查看命名空间可以使用如下命令：

sudo kubectl get ns

如下图所示，创建命令、查看命令和显示的命名空间。

---
**第 125 页**

119

图 4-19 创建、查看命名空间

# 4.6.4 项目microservice-demo 微服务的编排及部署

# 4.6.4.1 不同配置文件中端口的关系

application.yml 中的端口：微服务app 的port 是指应用程序监听的端口号，用于接收和处理传入

的请求。它通常在应用程序的代码中指定，并在应用程序启动时绑定到该端口。

Dockerfile 中的EXPOSE 的端口：是用来声明容器运行时需要暴露的端口，它并不会自动将容

器内部的端口映射到主机的端口。该指令主要用于帮助其他人了解该镜像的运行时依赖关系。

Deployment.yml 中的port：是指将应用程序容器化后，在Kubernetes 集群中部署和管理该容器

的配置文件。在该配置文件中，需要定义容器的端口映射关系，将容器内部的端口映射到集群的节

点上，以便可以从外部访问该容器。Deployment.yml 中的port 应该与app 中的port 一致。在

Deployment.yml 中，需要定义容器的端口映射关系，将容器内部的端口映射到集群的节点上，以便

可以从外部访问该容器。为了正确地将流量导向到容器内部的

Service 中的port：是指在Kubernetes 集群中创建的服务，它可以将外部的请求流量转发到部署

在集群中的容器中。Service 的port 与Deployment 中的port 有关联，它定义了Service 应该监听的端

口号，并将流量转发到该端口。同时，Service 还可以定义一些其他的特性，如负载均衡、服务发现

等。

综上所述，微服务app 的port 是应用程序监听的端口号，Dockerfile 中的EXPOSE 用于声明容

器运行时需要暴露的端口，Deployment.yml 中的port 定义了容器的端口映射关系，Service 中的port

定义了Service 应该监听的端口号，并将流量转发到该端口。它们之间有一定的关联，但各自承担不

同的功能。

# 4.6.4.2 公开镜像和私有镜像的部署

在进行微服务部署时，在远端镜像仓库中的微服务的镜像可以设置为公开镜像，也可以是私有

镜像。

公开镜像：公开镜像是指可以被任何人访问和使用的镜像，通常存储在公开的镜像仓库，或私

有仓库中将所用的某个镜像设为了公开。公开镜像适合用于开源项目或无需保密的应用程序部署，

任何人都可以在公开仓库中找到和下载这些镜像。

私有镜像：私有镜像是指只有授权用户才能访问和使用的镜像，通常存储在私有的镜像仓库。

私有镜像适合用于商业项目或需要保密的应用程序部署，只有具有权限的用户才能访问和使用这些

---
**第 126 页**

120

镜像。

它们的区别如下：

# 1.
安全性：私有镜像相对更安全，因为只有授权用户可以访问，可以更好地保护敏感信息和

代码。

# 2.
控制权：私有镜像提供了更多的控制权，可以限制访问权限、版本控制等。

# 3.
成本：私有镜像可能需要额外的费用来进行存储和管理，而公开镜像通常是免费的。

# 4.
可用性：公开镜像更容易被找到和使用，而私有镜像需要通过授权才能访问，可能会受到

限制。

# 5.
在实际应用中，根据具体的需求和安全考虑，可以选择使用公开镜像或私有镜像进行微服

务部署

使用公开镜像相对简单易用，适合用于开发和测试环境，而私有镜像需要更多的认证和授权操

作，适合用于生产环境或需要保密的应用程序部署。根据具体的需求和安全考虑，选择合适的镜像

部署方式进行操作。

如果是使用公有镜像则不需要下面的授权操作。

如果使用私有镜像进行微服务部署编排，需要进行认证和授权才能访问和使用，可以按照以下

步骤进行：

创建一个Secret 对象，用于存储私有镜像仓库的认证信息。

kubectl create secret docker-registry my-registry-secret --docker-server=REGISTRY_SERVER --
docker-username=REGISTRY_USERNAME
--docker-password=REGISTRY_PASSWORD
--docker-
email=REGISTRY_EMAIL

其中，REGISTRY_SERVER
是私有镜像仓库的地址，REGISTRY_USERNAME
和

REGISTRY_PASSWORD 是私有镜像仓库的用户名和密码，REGISTRY_EMAIL 是注册邮箱。

在Pod 的spec 中指定使用上一步创建的Secret 对象。可以在Pod 的spec 中添加如下字段：

spec:
containers:
- name: my-container
image: REGISTRY_SERVER/IMAGE_NAME:TAG
imagePullSecrets:
- name: my-registry-secret
其中，REGISTRY_SERVER 是私有镜像仓库的地址，IMAGE_NAME 是镜像名称，TAG 是镜像

标签。

部署Pod 时，Kubernetes 将使用上述Secret 对象中的认证信息来拉取私有镜像仓库中的镜像。

# 4.6.4.3 eureka-service 的部署

Eureka 是Netflix 开源的服务注册和发现框架，可以用来管理和发现微服务。

eureka-service 微服务的配置文件分为applycation.yml 和application-k8s.yml。将microservice-

demo 项目中的配置文件按如下内容修改，注意这里“active: k8s”，这样eureka-service 启动时才会使

用applycation-k8s.yml 环境变量配置文件。

改好配置文件后重新打包eureka-service，生成eureka-service.jar。然后按前面讲的方法生成名为

---
**第 127 页**

121

“registry.cn-hangzhou.aliyuncs.com/my-stock/eureka-service:latest”的镜像，并推送到镜像仓库。

applycation.yml 如下：

server:
port: 9999
spring:
application:
name: @artifactId@
profiles:
active: k8s
cloud:
gateway:
discovery:
locator:
#开启以服务id 去注册中心上获取转发地址
enabled: true
##小写serviceId
lower-case-service-id: true
routes:
- id: product-client
uri: lb://product-client
filters:
- StripPrefix=1
predicates:
- Path=/product/**
eureka:
instance:
prefer-ip-address: true
lease-renewal-interval-in-seconds: 5         #续约间隔时间
lease-expiration-duration-in-seconds: 15     #续约到期时间
client:
register-with-eureka: true
fetch-registry: true

上面配置文件中的application 的name 取值为“@artifactId@”，它是一个占位符，通常用于替换

实际的数值或字符串。这里的name 取值也可以给出具体的名字（后有的例子给出的是具体的名字）。

在上述配置文件中，“@artifactId@”是用来表示项目的Artifact ID，即项目的唯一标识符。在实际应

用中，该占位符会被项目的Artifact ID 值替换，从而使配置文件中的内容与实际项目信息对应起来。

在这个配置文件中，通过构建工具（如Maven 或Gradle）自动将@artifactId@替换为项目的实际

Artifact ID。这样做的好处是可以使配置文件具有通用性，同时保护敏感信息（如项目名称）不被泄

露。

application-k8s.yml 内容如下：

eureka:
client:
service-url:
defaultZone: http://eureka-
service.stockmgr.svc.cluster.local:8888/eureka

本示例中服务名为eureka-service，镜像名为registry.cn-hangzhou.aliyuncs.com/my-stock/eureka-

service:v1.0， 将其部署为服务，通过NodePort 进行服务暴露，端口号为30001，命名空间为ms，映

---
**第 128 页**

122

射到主机的端口为30001。

下面是编排部署文件：eureka-service.yaml

apiVersion: v1
kind: List
items:
- apiVersion: v1
kind: Service
metadata:
name: eureka-service
namespace: stockmgr
spec:
type: NodePort
ports:
- port: 8888
targetPort: 8888
nodePort: 30001
selector:
app: eureka
- apiVersion: apps/v1
kind: Deployment
metadata:
name: eureka-deployment
namespace: stockmgr
spec:
replicas: 1
selector:
matchLabels:
app: eureka
template:
metadata:
labels:
app: eureka
spec:
containers:
- name: eureka
imagePullPolicy: IfNotPresent
image: registry.cn-hangzhou.aliyuncs.com/my-stock/eureka-service:v1.0
ports:
- containerPort: 8888

该部署文件中：

指定了一个名为"eureka-service"的服务，将其命名空间设置为"stockmgr"，并使用了NodePort 方

式暴露了容器的8888 端口，并将其映射到宿主机的30001 端口上。

定义了一个名为"eureka-deployment"的 deployment，并将副本数设置为1。在该 deployment 中，

我们指定了容器的镜像以及端口号，这里的镜像名称为"registry.cn-hangzhou.aliyuncs.com/my-

stock/eureka"，端口号为8888。在这个 deployment 中，使用了标签选择器"app: eureka"来绑定部署

的 Pod 和服务。

部署成功后可以通过主机IP:30001 访问Eureka 的web 界面，查看微服务。

部署时使用如下命令：

sudo kubectl create -f eureka-service.yaml

---
**第 129 页**

123

# 4.6.4.4 product-service 的部署

product-service 是项目中自编的一个微服务，提供产品的管理。

product-service 微服务的配置文件分为applycation.yml 和application-k8s.yml。将microservice-

demo 项目中的配置文件按如下内容修改，注意这里“active: k8s”，这样eureka-service 启动时才会使

用applycation-k8s.yml 环境变量配置文件。

改好配置文件后重新打包product-service，生成product-service.jar。然后按前面讲的方法生成名

为“registry.cn-hangzhou.aliyuncs.com/my-stock/product-service:latest”的镜像，并推送到镜像仓库。

applycation.yml 如下：

server:
port: 8010
spring:
application:
name: product-service
profiles:
active: k8s
eureka:
instance:
prefer-ip-address: true #如果为false 可能出错找不到服务的错误
client:
register-with-eureka: true
fetch-registry: true
instance:
prefer-ip-address: true   #使用ip 地址向eureka server 进行注册
lease-renewal-interval-in-seconds: 5         #续约间隔时间
lease-expiration-duration-in-seconds: 15     #续约到期时间

application-k8s.yml 内容如下：

spring:
datasource:
url: jdbc:mysql://mysql.default:3306/tb_product?characterEncoding=utf-8& allowPublicKeyRetrieval
=true &useSSL=false
username: root
password: root
driver-class-name: com.mysql.jdbc.Driver
eureka:
client:
service-url:
defaultZone: http://eureka-service.stockmgr.svc.cluster.local:8888/eureka

本示例中服务名为eureka，镜像名为registry.cn-hangzhou.aliyuncs.com/my-stock/product-service。

下面是product 微服务的部署文件，该文件包含了三个 Kubernetes 对象：

Namespace：命名空间，它的名称为 "stockmgr"。

Service：服务，它的名称为 "product-service"，属于 "stockmgr" 命名空间，类型为 ClusterIP，

在端口 8010 上监听 HTTP 请求，并将它们转发到应用标签为 "product-service" 的 Pod 上。

Deployment：部署，它的名称为 "product-service"，属于 "stockmgr" 命名空间，创建了一个 Pod，

相应的容器镜像为 "registry.cn-hangzhou.aliyuncs.com/my-stock/product-service"，并且在容器内暴露

了端口 8010。这个 Pod 是通过 Selector 选择应用标签为 "product-service" 的 Pod 进行创建和管

---
**第 130 页**

124

理的。

apiVersion: apps/v1
kind: Deployment
metadata:
name: product-service
namespace: stockmgr
spec:
replicas: 2
selector:
matchLabels:
project: stockmgr
app: product-service
template:
metadata:
labels:
project: stockmgr
app: product-service
spec:
imagePullSecrets:
- name: registry-pull-secret
containers:
- name: product-service
image: registry.cn-hangzhou.aliyuncs.com/my-stock/product-service:v1.0
imagePullPolicy: IfNotPresent
ports:
- protocol: TCP
containerPort: 8010
env:
- name: JAVA_OPTS
value: "-Xmx1g"
resources:
requests:
cpu: 0.5
memory: 256Mi
limits:
cpu: 1
memory: 1Gi
readinessProbe:
tcpSocket:
port: 8010
initialDelaySeconds: 60
periodSeconds: 10
livenessProbe:
tcpSocket:
port: 8010
initialDelaySeconds: 60
periodSeconds: 10

product-service 中要访问mysql 数据库，因此在该服务的applycation.yaml 中要将jdbc 改为如下

形式：

jdbc:mysql://[service-name]:[port]/[database-name]?characterEncoding=utf-8&useSSL=false

前面我们部署的mysql 微服务名字为mysql，如果mysql 服务与product-service 不在一个命名空

间，则需要加上命名空间。比如mysql 服务的命名空间为default，则应写成mysql.default 端口为3306，

---
**第 131 页**

125

数据库名为tb_product。刚jdbc 连接如下：

jdbc:mysql://mysql.default:3306/tb_product?characterEncoding=utf-
8&allowPublicKeyRetrieval=true&useSSL=false

部署完毕后可以Master 机器上，用如下命令测试是否可以访问数据，其中IP 是Product 这个

pod 的clusterIP，可以通过 kubectl get pod -A -o wide 查看。

图 4-20 显示pod

执行下面命令可以检查结果：

sudo curl ip:8010/queryAllProduct

将ip 换为上图中查到的地址：curl 10.244.194.82:8010/queryAllProduct

下图是执行的结果：

执行上面命令如果有数据返回，说明product-service 这个服务的部署没有问题。否则可以通过下

面命令查看日志，来查找问题原因：

首先查看product-service 所在的pod:

sudo kubectl get pod -a

sudo crictl logs  容器ID

sudo kubectl logs -n 命名空间(NAMESPACE) 容器名(NAME)

示例：

sudo kubectl logs -n stockmgr  product-service-7bc767cfd4-m8zbw
上面的IP 地址只能在网络内部访问，集群之外是无法访问到的，因此在集群之外，比如web 前

---
**第 132 页**

126

端要使用该服务，一般是通过反向代理或网关将该服务暴露给外部。在我们这个案例中product-

service 只在集群内部使用，web 前端要通过网关路由到product-client 服务，再通过product-client 使

用product-service 服务。product-client 有负载均衡、熔断等功能。

# 4.6.4.5 product-client 的部署

product-client 是基于product-service 的客户端，前端在使用product-service 提供的服务时，不是

直接访问product-service，而是通过网关（gateway-service）访问product-client，关于product-client 前

面已经给出了详细的实现过程，现在需要将其部署到集群中。

首先要修改配置文件，然后重新打包product-client，生成product-client.jar。然后按前面讲的方

法生成名为“registry.cn-hangzhou.aliyuncs.com/my-stock/product-client:latest”的镜像，并推送到镜像仓

库。

配置文件修改如下：

applycation.yml：

server:
port: 8018
spring:
profiles:
active: k8s
application:
name: product-client
ribbon:
eureka:
enable: true
feign:
hystrix:
enabled: true    #在feign 中开启hystrix 熔断机制
eureka:
instance:
prefer-ip-address: true
client:
register-with-eureka: true
#下面一定要写true
fetch-registry: true
initial-instance-info-replication-interval-seconds: 10
registry-fetch-interval-seconds: 5
instance:
prefer-ip-address: true
lease-renewal-interval-in-seconds: 5         #续约间隔时间
lease-expiration-duration-in-seconds: 15     #续约到期时间

application-k8s.yml 内容如下：

eureka:
client:
serviceUrl:
defaultZone: http://eureka-service.stockmgr.svc.cluster.local:8888/eureka

生成镜像后，使用下面的部署文件将其部署的k8s 集群。

文件名product-client.yaml

apiVersion: apps/v1
kind: Deployment

---
**第 133 页**

127

metadata:
name: product-client
namespace: stockmgr
spec:
replicas: 1
selector:
matchLabels:
project: stockmgr
app: product-client
template:
metadata:
labels:
project: stockmgr
app: product-client
spec:
imagePullSecrets:
- name: registry-pull-secret
containers:
- name: product-client
image: registry.cn-hangzhou.aliyuncs.com/my-stock/product-client:v1.0
imagePullPolicy: IfNotPresent
ports:
- protocol: TCP
containerPort: 8018
env:
- name: JAVA_OPTS
value: "-Xmx1g"
resources:
requests:
cpu: 0.5
memory: 256Mi
limits:
cpu: 1
memory: 1Gi
readinessProbe:
tcpSocket:
port: 8018
initialDelaySeconds: 60
periodSeconds: 10
livenessProbe:
tcpSocket:
port: 8018
initialDelaySeconds: 60
periodSeconds: 10

部署命令：

sudo kubectl create -f product-client.yaml

查看部署结果：

sudo kubectl get pods -A -o wide

---
**第 134 页**

128

加上 “-o wide” 后会显示更多的信息，包括IP 、pod 所在的机器等。部署需要一定的时间，所

以如果执行完部署命令后，一般pod 不会立即变成ready 将状态，因为一般需要从远端服务器拉取

镜像，如果镜像比较大、网速双较慢，可能会等待几分种才能最终变成ready 状态。

# 4.6.4.6 gateway-service 的部署

Spring Cloud Gateway 是一个基于Spring 生态系统的API 网关，它提供了一种简单而有效的方

法来管理路由请求，以及执行一些常见的边缘服务逻辑，如安全、监控/指标和限流等。

按首前面部署服务的方式，修改配置文件、编译项目生成jar 包、打包成镜像、推送到镜像仓库、

最后执行部署gateway-service.yaml 文件进行部署。

配置文件修改如下：

applycation.yml：

server:
port: 9999
spring:
application:
name: @artifactId@
profiles:
active: k8s
cloud:
gateway:
discovery:
locator:
#开启以服务id 去注册中心上获取转发地址
enabled: true
##小写serviceId
lower-case-service-id: true
routes:
- id: product-client
uri: lb://product-client
filters:
- StripPrefix=1
predicates:
- Path=/product/**
eureka:
instance:
prefer-ip-address: true

---
**第 135 页**

129

lease-renewal-interval-in-seconds: 5         #续约间隔时间
lease-expiration-duration-in-seconds: 15     #续约到期时间
client:
register-with-eureka: true
fetch-registry: true

其中name 可以直接写成“gateway-service”，这里“@artifactId@”取的是项目的“artifactId”项目中

我们设的也是“gateway-service”。

application-k8s.yml 内容如下：

seureka:
client:
service-url:
defaultZone: http://eureka-service.stockmgr.svc.cluster.local:8888/eureka

生成镜像后，使用下面的部署文件将其部署的k8s 集群。

文件名product-client.yaml

kind: List
apiVersion: v1
items:
- apiVersion: v1
kind: Service
metadata:
name: gateway-service
namespace: stockmgr
spec:
type: NodePort
selector:
app: gateway-service
ports:
- name: http
port: 9999
targetPort: 9999
nodePort: 30099
- apiVersion: apps/v1
kind: Deployment
metadata:
name: gateway-service
namespace: stockmgr
spec:
replicas: 1
selector:
matchLabels:
app: gateway-service
template:
metadata:
labels:
app: gateway-service
spec:
containers:
- name: gateway-service
imagePullPolicy: IfNotPresent
image: registry.cn-hangzhou.aliyuncs.com/my-stock/gateway-service:v1.0

---
**第 136 页**

130

ports:
- containerPort: 9999

下面是gateway 的部署文件，该文件包含了三个 Kubernetes 对象：

Namespace：命名空间，它的名称为 "stockmgr"。

Service：服务，它的名称为 "gateway-service"，属于 "stockmgr" 命名空间，类型为 ClusterIP，

在端口 9999 上监听 HTTP 请求，并将它们转发到应用标签为 "gateway-service" 的 Pod 上。

Deployment：部署，它的名称为 "gateway-service"，属于 "stockmgr" 命名空间，创建了一个 Pod，

相应的容器镜像为 "registry.cn-hangzhou.aliyuncs.com/my-stock/gateway-service"，并且在容器内暴露

了端口 9999。这个 Pod 是通过 Selector 选择应用标签为 "gateway-service" 的 Pod 进行创建和管

理的。

kind: List
apiVersion: v1
items:
- apiVersion: v1
kind: Service
metadata:
name: gateway-service
namespace: stockmgr
spec:
type: NodePort
selector:
app: gateway-service
ports:
- name: http
port: 9999
targetPort: 9999
nodePort: 30099
- apiVersion: apps/v1
kind: Deployment
metadata:
name: gateway-service
namespace: stockmgr
spec:
replicas: 1
selector:
matchLabels:
app: gateway-service
template:
metadata:
labels:
app: gateway-service
spec:
containers:
- name: gateway-service
image: registry.cn-hangzhou.aliyuncs.com/my-stock/gateway-service:v1.0
ports:
- containerPort: 9999

---
**第 137 页**

131

# 4.6.4.7 项目测试

当项目全部部署完毕后,可以在k8s-master 结点上，通过下面命令查看pod 部署结果。

sudo kubectl get pod -A -o wide

图 4-21 部署后所有pod 的运行情况

信息主要包括：

(1). NAMESPACE：Pod 所属的命名空间。

(2). NAME：Pod 的名称。

(3). READY：Pod 中容器的就绪状态。

(4). STATUS：Pod 的状态，例如Running、Pending 等。

(5). RESTARTS：Pod 中容器的重启次数。

(6). AGE：Pod 的创建时间。

(7). IP：Pod 的IP 地址。

(8). NODE：Pod 所在的节点。

从中可以看到我们部署的服务在stockmgr 命名空间下，服务都已经正确运行了。

通过下面命令可以查看部署的服务。

sudo kubectl get svc -a -o wide

其中type 为NodePort 的，表示可以通过结点IP 进行访问的服务，比如gateway-service 它的type

为Nodeport，PORT(S)为9999:30099 表示k8s 内部可以通过9999 这个端口访问gateway-service,30099

表示将内部的9999 映射到了结点IP 的30099 上，外部可以通过k8s-master 的IP 访问该服务。

我们的主结点k8s-master 的IP 为192.168.79.133，因此我们可能通过下面的url 查询产品信息：

http://192.168.79.133:30099/product/queryAllProduct?token=1

输入后显示如下内容。

---
**第 138 页**

132

图 4-22 通过网关查询到的商品信息

由于我们在product-service 程序内部加了token 校验且必须等于1，所以在上面的url 中最后要

加上“?token=1”，否则访问会被拒绝。实际应用中token 应该是动态获取到的，即在用户登录后生成，

并且一般在一定时间后会过期，要申请新的token。

# 5 K8S 可视化工具部署

使用K8S 可视化工具可以轻松管理和操作集群中的各种资源。目前大约有几十种这类工具比如：

K8s Dashboard、Kuboard、KubeSphere、Rancher 等等。下面介绍一下K8s Dashboard 的安装，在

Kubernetes 中使用 Dashboard 可以方便地监视和管理集群。以下是部署 Kubernetes Dashboard 的步

骤：

# 5.1 下载 Kubernetes Dashboard 部署文件并进行配置

# 1.下载文件

可以使用以下命令下载 Kubernetes Dashboard YAML 文件：

wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

如果无法访问可以使用镜像网站下载部署文件：

wget https://raw.gitmirror.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

# 2.修改配置

配置从外部访问dashboard

设置 NodePort，编辑recommended.yaml，修改dashboard 的Service。

---
**第 139 页**

133

图 5-1 DashBoard 的配置

按上面的方式直接安装，安装过程中要从固定的镜像仓库下载镜像，有时网速比较慢，需要很

长时间才能安装完成。因此也可以先从网上下载镜像文件，导入到集群中，然后再安装。通常这样

会更快一些。

具体做法如下：

先修改一下recommended.yaml 中的镜像下载策略，将其改为如果本地已经有镜像文件则不再从

仓库拉取。修改方法如下：

找到这个位置（大约196 行左右），将Always 改为IfNotPresent。

然后下载镜像文件，主要有两个：

kubernetesui/metrics-scraper:v1.0.8 和kubernetesui/dashboard:v2.7.0。

可以参考前面k8s 的第二种安装方法，将这两个文件的导出包( 下载地址：

https://pan.hit.edu.cn/l/8FsAU9 提取码:(cfqz) ，文件夹名称：DashboardImages)，分别导入集群中的所

有结点中。然后再执行下面命令进行部署：

# 5.2 部署 Kubernetes Dashboard

可以在Master 节点上运行以下命令，来实现部署 Kubernetes Dashboard：

sudo kubectl apply -f recommended.yaml

查看运行状态：

---
**第 140 页**

134

kubectl get pod,svc -n kubernetes-dashboard

图 5-2 查看dashboard 是否运行

通过上面显示的信息可以看出，dashboard 的服务在kubernetes-dashboard 的命名空间中，可以

通过Master 机器的IP 和30010 端口访问，但在访问前要先创建账号。

创建账号

创建管理员用户：为了以管理员身份登录 Kubernetes Dashboard，需要使用以下命令创建一个具

有管理员权限的 Kubernetes 用户：

sudo kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard
sudo
kubectl
create
clusterrolebinding
dashboard-admin-rb
--clusterrole=cluster-admin
--
serviceaccount=kubernetes-dashboard:dashboard-admin

# 5.3 获取令牌并登录Dashboard

# 5.3.1 获取访问令牌

为了访问 Kubernetes Dashboard，需要获取管理员用户的访问令牌，可以用下面命令创建token:

sudo kubectl -n kubernetes-dashboard create token dashboard-admin
下面是创建的密钥

eyJhbGciOiJSUzI1NiIsImtpZCI6InFlS29mbDVfYUFZRGd0NmEwcHplT3BieUllTDZOVWF2QnNJ
Rjc3cDlqYTgifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIub
G9jYWwiXSwiZXhwIjoxNzA0MjcyNzE3LCJpYXQiOjE3MDQyNjkxMTcsImlzcyI6Imh0dHBzOi8va3Vi
ZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3B
hY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJkYXNoYm
9hcmQtYWRtaW4iLCJ1aWQiOiJkMzkxMjcxZC04NWE1LTRiOGItYmE5ZC1iYzcwMGIwMGUzZDAif
X0sIm5iZiI6MTcwNDI2OTExNywic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXM
tZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.1Qjm4_eH_noprsrECx5RtxHRXy4vaGUPbeUgozgyEp
UmmdrjwgzZtm2EdqjZYu7i2t87mhYFp20H3ELRs2pf47r_JZsmhk5MRuQ-Ch3WwLqoXPqYW-
ytaYIIkx_1dsH2uq5Q7xw9_MmXeiVlZTxmslE0DrfAVUR2ciEYiWs0y1CUjYsf4NWj2vMCWHjd8xx8r
EGPeuMRoHY5KkzfhCZZdkLB84g2z4zP8TBYWXNIS4Fd7U7orxX2tMzrcCvWLAEHJM_wZmVgb3p
6nShsdNOKkKyEwdoqiwkFv5j-m1YNeOsRhA8zcbxkEPnLlRxEgXFGOeUIHo_qD2uVICCJJ9PBGw

执行此命令后，将在输出中看到一个名为 token 的字符串。将此字符串复制下来，稍后将用作

登录 Kubernetes Dashboard 时的访问令牌。

# 5.3.2 登录Dashboard

如果要在另一台物理机器上登录虚拟机上安装的dashboard，则要进行端口映射，然后在另外的

主机上登录。如果想要宿主机登录虚拟机上的dashboard,则不需要做端口映射，直接输入虚拟机的ip

和端口就要以直接登录到虚拟机上的dashboard。

下面是直接在宿主机上登录的方法，假设虚拟机中master 结点的ip 为192.168.79.133 则在浏览

---
**第 141 页**

135

器上输入https://192.168.79.133:30010/#/login，（注意前缀输入的是https 不是http）出现如下界面，

将前面保存的Token 拷贝进来。

图 5-3 登录Dashboard

登录后。

图 5-4 登录后的dashboard 的主界面

注：也可能通过启动代理的方式：

启动代理：为了在本地计算机上访问 Kubernetes Dashboard，可以使用以下命令启动代理：

sudo kubectl proxy

访问 Kubernetes Dashboard：现在可以在本地计算机上访问 Kubernetes Dashboard。在浏览器中

打开以下 URL：

http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-
dashboard:/proxy/

登录 Kubernetes Dashboard：在登录页面上，选择“令牌”登录，并使用前面获取的访问令牌登录。

---
**第 142 页**

136

以上步骤将在您的 Kubernetes 集群中成功部署和启动 Kubernetes Dashboard，并设置一个具有

管理员权限的用户。请注意，Kubernetes Dashboard 默认情况下不受身份验证保护，因此只能使用

HTTPS 进行访问。建议您在生产环境中使用其他身份验证方法来保障 Kubernetes Dashboard 的安

全。

# 6 云原生应用的自动化部署和持续集成/持续交付（CI/CD）

自动化部署工具可以帮助实现应用的自动化构建、测试、部署和发布。常见的技术包括Jenkins、

GitLab CI、Spinnaker 等。本课程将使用GitOps 工具Jenkins 实现CI/CD。

# 6.1 GitOps 及Jenkins 简介

GitOps 是一种基于Git 的DevOps 实践，它将代码、应用程序配置和基础设施描述都存储在Git

存储库中，并使用CI/CD 工具自动将更改应用于生产系统。GitOps 旨在实现以代码为中心的、可重

复的和可预测的基础设施管理和应用程序交付。

GitOps 是一个 DevOps 的子领域，它强调了将 Git 作为声明性基础设施或应用程序资源的单

一来源，是一种基于Git 的持续交付模式，它可以使得应用程序和基础设施的管理更加高效和可靠。

GitOps 的核心思想是将所有部署的配置、代码和基础结构作为Git 代码管理，并通过自动化工

具在集群中实时同步这些更改。

以下是使用Gitee 和Jenkins 实现CI/CD 的过程：

1).在Gitee（或其代码管理平台）上创建代码仓库

2). 在Jenkins 中安装插件

3). 在Jenkins 中创建一个Pipline 项目

4). 编写Jenkinsfile

在Jenkins 中，可以手动触发自动化构建，也可以设置Webhooks 来接收Git 存储库中发生的事

件。Webhooks 可以自动触发Jenkins 的构建，从而自动执行CI/CD 流程。

通过上述步骤，您可以使用Gitee 和Jenkins 实现CI/CD，并自动构建和部署您的应用程序，从

而提高交付效率和可靠性。

下面是CI/CD 各步骤涉及的主要工具和技术。

---
**第 143 页**

137

图 6-1 CI/CD 各步骤涉及的技术

完整流程：

# 1.
开发者将最新代码提交到Git 仓库；

# 2.
Git WebHook 触发Jenkins 构建流水线：

# 3.
拉取最新代码；

# 4.
Maven 打包，打包过程中会先进行单元测试；

# 5.
单元测试通过，构建Docker 镜像；

# 6.
将最新镜像推送到Harbor（或其它镜像仓库）；

# 7.
更新Kubernetes 相关配置镜像版本。

# 8.
Kubernetes 感知到镜像更新，从Harbor（或其它镜像仓库）拉取最新镜像，滚动升级；

# 9.
开发者看到最新的代码效果。

本实验我们不要求大家实现全部流程，按下面指导书完成相应内容即可。

图 6-2 CI/CD 基本过程

---
**第 144 页**

138

# 6.2 创建代码仓库

在Gitee 上创建代码仓库，然后将项目上传到代码仓库。以下是如何在Gitee 上创建代码仓库的

步骤：

首先，登录你的Gitee 账号。

在页面上方的导航栏中，找到“代码托管”选项，将鼠标悬停在上面，然后选择“新建仓库”。

在“新建仓库”页面中，填写你的仓库名称、描述和其他相关信息。

选择你的仓库的可见性，可以选择公开或私有。

选择你的仓库的初始化方式，你可以选择从本地上传或者从其他代码托管平台导入。

点击“创建仓库”按钮，你的代码仓库就创建成功了。

以下是创建代码仓库的代码示例：

git init
git add .
git commit -m "initial commit"
git remote add origin [gitee repository url]
git push -u origin master
其中，[gitee repository url]是你在Gitee 上创建的仓库的URL。

如图所示是gitee 上创建好的仓库。

图 6-3 Gitee 上创建好的仓库

# 6.3 部署Jenkins

---
**第 145 页**

139

# 6.3.1 安装Jenkins

可以直接使用软件包管理工具apt 进行安装。使用软件包管理工具进行安装时可能会遇到无法

安装到最新版本的情况。这是因为软件包管理工具通常会从官方软件源中获取软件包，而有时官方

软件源更新速度不及第三方开发者发布新版本的速度。

如果需要安装最新版本的软件，可以考虑通过直接从软件官方网站下载最新版本并手动安装，

或者添加第三方软件源进行安装。

# 1.
使用软件包管理工具进行安装

如果没安装JRE 要先执行下面命令安装JRE，因为Jenkins 要运行在Jre 环境中。

sudo apt install fontconfig openjdk-17-jre

然后，依次运行下面命令完成安装：

sudo wget -O /usr/share/keyrings/jenkins-keyring.asc https://pkg.jenkins.io/debian-stable/jenkins.io-
2023.key

echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] https://pkg.jenkins.io/debian-stable
binary/ | sudo tee /etc/apt/sources.list.d/jenkins.list > /dev/null

第一条命令 sudo wget -O /usr/share/keyrings/jenkins-keyring.asc https://pkg.jenkins.io/debian-

stable/jenkins.io-2023.key 的作用是从 https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key 下载

Jenkins 软件的密钥文件，并将其保存到 /usr/share/keyrings/jenkins-keyring.asc。

第
二
条
命
令
echo
deb
[signed-by=/usr/share/keyrings/jenkins-keyring.asc]

https://pkg.jenkins.io/debian-stable binary/ | sudo tee /etc/apt/sources.list.d/jenkins.list > /dev/null 的作用

是将 Jenkins 软件的软件源信息添加到系统的 apt 源列表文件 /etc/apt/sources.list.d/jenkins.list 中。

这样系统就能够通过 apt 包管理器从 Jenkins 软件源中获取并安装 Jenkins 软件。

执行下面命令开始安装。

sudo apt-get update

sudo apt-get install jenkins

安装时如果提示如下错误：

The following information may help to resolve the situation:
The following packages have unmet dependencies:
jenkins : PreDepends: init-system-helpers (>= 1.54~) but 1.51 is to be installed
E: Unable to correct problems, you have held broken packages.
这是本机的 init-system-helpers 版本不满足要求导致，安装指定版本即可，解决方法如下：

#查看可用版本

apt-cache policy init-system-helpers

#选择一个安装

apt-get install init-system-helpers=1.56+nmu1~ubuntu18.04.1

基后再次运行：

---
**第 146 页**

140

sudo apt-get install jenkins

# 2.
下载Jenkins 的war 包直接运行

这是第二种安装Jenkins 的方法，就是先到官网下载jenkins.war，将其放在某个目录中，比如放

在/opt/jenkins 中，如果没有该目录可以创建一个，并创建/var/log/jenkins，即执行如下命令：

sudo mkdir -p /opt/jenkins

sudo mkdir -p /var/log/jenkins

sudo mkdir -p /var/lib/jenkins

然后进行如下操作，使其做为一个服务启动运行。

使用systemd 服务来启动jenkins,这样其启动用户是root。以下是在Ubuntu 中使用systemd 服务

启动Jenkins 的步骤：

(1) 创建一个新的systemd 服务单元文件，比如jenkins.service：

sudo vim /etc/systemd/system/jenkins.service

在文件中添加以下内容

[Unit]
Description=Jenkins Service
After=network.target
[Service]
User=root
Group=root
WorkingDirectory=/opt/jenkins
ExecStart=/usr/bin/java -jar /opt/jenkins.war --httpListenAddress=127.0.0.1
[Install]
WantedBy=multi-user.target

(2) 启动Jenkins 服务，并设置开机自启动

systemctl daemon-reload

sudo systemctl start jenkins

sudo systemctl enable jenkins

启动时如果提示如下错误：

Failed to start jenkins.service: Unit jenkins.service is masked

意思是这个服务锁定了，可执行如下命令开启：

systemctl unmask jenkins.service

可以使用下面命令查看启动状态，如果看到active（running）则表示启动成功，否则就要根据提

示查找原因。

---
**第 147 页**

141

systemctl status jenkins

# 6.3.2 修改Jenkins 启动用户

使用apt 安装的jenkins ，需要修改一下jenkins 的启动用户，而下载Jenkins 的war 包运行的方

式，按前面的方法起动后就可以了，不需要这步操作。

在默认情况下，Jenkins 在Ubuntu 下由"jenkins"用户启动。这种默认设置可能会带来一些局限性，

包括：

权限限制：Jenkins 使用的"jenkins"用户可能没有足够的权限来执行某些操作，例如访问特定目

录或文件，或执行特定的系统命令。

网络访问限制："jenkins"用户可能受到网络访问限制，例如无法访问互联网或特定的网络资源。

系统资源限制："jenkins"用户可能受到系统资源限制，例如无法使用特定的硬件设备或执行高级

系统操作。

并且在Jenkins 中编写pipeline 时，可能由于权限的限制，在使用nerdctl 创建镜像时，必须安装

安装Rootless Containers 并且要进行一些必要的配置才能正常执行。在学习时为了突出重点简化操

作，我们把jenkins 软件的启动用户改为root，当然如果在生产环境还是要严格管理好权限。

通过修改服务文件来修改启动用户：

老版本文件为vim /etc/sysconfig/jenkins

新版本文件为/usr/lib/systemd/system/jenkins.service

找到这个文件，用编辑器打开

比如是新版本的jenkins,

vim /usr/lib/systemd/system/jenkins.service

将User 和Group 改为root。

图 6-4 修改jenkins.service 的启动用户

然后重启动服务

systemctl daemon-reload
systemctl start jenkins.service

启动服务会有点慢，等服务启动成功后可以用下面命令查看启动用户：

ps -elf | grep jenkins

---
**第 148 页**

142

图 6-5

# 6.3.3 登录Jenkins

如果在Vmware 所在的宿主机上，则可以浏览器中直接输入Master 结点的ip:8080,就可进入

Jenkins 的登录界面。如果在其它机器上登录，就需要进行端口映射才行。

现在我们在Vmware 所在的宿主机上直接登录，登录后显示如下登录界面：

图 6-6 初次登录Jenkins

按界面上的提示找到初始密码，注意提示的路径，有可能与上面界面有区别，登录进入Jenkins

系统。进入系统后可以设置用户和密码，这里我们设置为 admin/admin。

链接地址为：http://192.168.79.133:8080/ 。192.168.79.133 是k8s-master 的地址。

如果登录时卡在这里“Please wait while Jenkins is getting ready to work …”解决方案如下：

进入Jenkins 的工作目录（/var/lib/jenkins），编辑hudson.model.UpdateCenter.xml 文件

完整命令：

vim /var/lib/jenkins/hudson.model.UpdateCenter.xml

将
https://updates.jenkins.io/update-center.json
修
改
为

http://mirror.xmission.com/jenkins/updates/update-center.json

或https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json

下面是修改后的截图：

---
**第 149 页**

143

图 6-7 更改插件库

保存后重启Jenkins

systemctl restart jenkins

# 6.3.4 安装插件并修改升级点

# 6.3.4.1 修改插件源和升级点

在浏览器中打开 Jenkins 的 UI，按照提示进行配置。

图 6-8 插件安装过程

修改插件源

插件更新源替换为清华源，在k8s-master 结点上进入jenkins 目录下的/updates 目录。

执行如下替换命令：

sed
-i
's/https:\/\/updates.jenkins.io\/download/https:\/\/mirrors.tuna.tsinghua.edu.cn\/jenkins/g'
default.json

修改升级点

将升级点修改为国内站点，这样可以更快的下载升级文件。

在浏览器的url 上输入http://ip:8080/restart 重启jenkins，然后进入系统管理-插件管理--高级

（Advanced settings）--最下面的url 地址填写为：

https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json

---
**第 150 页**

144

图 6-9 更改升级站点

# 6.3.4.2 安装常用插件

为了实现 CI/CD，Jenkins 中需要安装一些必要的插件。在 Jenkins 的 UI 中，进入 系统管理

（Manage Jenkins） ->插件管理（Manage Plugins），安装需要的插件。

图 6-10 选择插件管理

进入后选择“Available plugins”：

---
**第 151 页**

145

图 6-11 显示可用插件

下面是一些常用的插件（相似名字的插件有很多，一定要安装与下面名字完整匹配的插件）：

# 1. Git Plugin：用于 Jenkins 与 Git 代码库进行集成，可以从 Git 存储库中拉取代码，在构建和

部署过程中使用。

# 2. Publish Over SSH：用于 Jenkins 在构建过程中通过 SSH 协议将文件传输到远程服务器。这

个插件可用于将构建产物或其他文件部署到远程服务器上。

# 3. SSH：提供了支持 SSH 连接的 Jenkins 构建步骤和功能。它可以用于执行远程命令、传输文

件、设置代理等。

# 4. Generic Webhook Trigger：Jenkins 通过 Webhook 接收来自不同服务的 HTTP 请求，并触发

构建或其他操作。这个插件可以用于与各种外部服务（如代码托管平台、CI/CD 工具等）进行集成。

# 5. Blue Ocean：提供了一个现代化的、可视化的界面来管理和监控 Jenkins 构建流水线。它提供

了一种更直观和用户友好的来创建和管理 Jenkins 构建作业。

# 6. Pipeline：Pipeline 插件是 Jenkins 的核心插件，它引入了基于脚本的构建和部署管道的概念。

使用 Pipeline，你可以以代码的方式定义构建过程，并以可视化方式查看和管理整个构建流水线。

# 7. Kubernetes：Kubernetes 插件允许 Jenkins 与 Kubernetes 集群进行集成。它提供了一种在

Kubernetes 上运行 Jenkins 构建代理的方式，以及将构建产物部署到 Kubernetes 中的容器环境的能

力。

# 8. gitee：gitee 插件是为了与 gitee 代码托管平台进行集成而开发的。它提供了拉取代码、触发

构建等功能，以便与 gitee 上的项目进行持续集成。

# 9. SSH AGENT：SSH AGENT 插件允许 Jenkins 在构建过程中使用 SSH 密钥进行身份验证和

访问远程服务器。它为构建过程中的 SSH 操作提供了便利和安全性。

以上是一些常用的插件，当然还有很多其他的插件可以根据需求进行安装和配置。

如果重新安装了插件，需要重启 Jenkins 才能使插件生效。可以通过下面方法重启 Jenkins：

http://ip:port/restart

ip:port 是Jenkins 的Ip 地址和端口号。

---
**第 152 页**

146

# 6.3.4.3 解决插件按装失败

如果自动安装插件时，由于插件与jenkins 版本不匹配导致无法安装成功，如何解决呢？比如出

现类似如下的提示，则是Jenkins 版本太低，与所安装的插件不匹配。

java.io.IOException: Failed to load: Pipeline: Job (workflow-job 1426.v2ecb_a_a_42fd46)
- Jenkins (2.454) or higher required

在安装插件时，要确认插件与Jenkins 版本是否兼容：在Jenkins 官方插件站点上查看插件的兼

容性列表，确保插件与当前Jenkins 版本兼容。

如果遇到不兼容可以采用下面两种方法之一来解决。

一种方法是手动安装插件：如果由于操作系统原因无法安装更高版本的Jenkins,则需要从Jenkins

官方插件站点下载对应版本的插件文件，并将其上传到Jenkins 服务器的插件目录中（一般为

$JENKINS_HOME/plugins 目录）。

重启Jenkins：在安装完插件后，重启Jenkins 服务以使插件生效。

另一种方法是更新Jenkins 版本：如果操作系统允许安装更高版本的Jenkins,则可以考虑升级

Jenkins 至与插件兼容的版本。

# 6.3.4.4 手动安装插件方法

Step1：进入Jenkins 官网的插件下载页面，在搜索框中输入插件名称，比如Git。

Step2： 在搜索结果列表中选择对应的插件。

---
**第 153 页**

147

点击进入后，选择“Releases”标签，显示如下界面，点击“How to install”按钮会显示安装方

法。

---
**第 154 页**

148

安装方法有如下几种：

可以使用命令行工具进行安装，这种安装方法要先下载命令行工具，点击上面页面中的“the CLI

tool”即可下载，安装工具jenkins-plugin-manager-xxx.jar，然后使用该工具进行安装，命令格式如上

图中显示的：

jenkins-plugin-cli --plugins workflow-job:1426.v2ecb_a_a_42fd46

其中“workflow-job:1426.v2ecb_a_a_42fd46”是你要安装的插件的标签名。

另外一种方法是下载与你的Jenkins 版本对应的插件，此时会得到.hpi 文件。然后进入Jenkins 插

件管理页面（Manage Jenkins->Manage Plugins），选择高级设置页面，将刚刚下载的.hpi 文件上传，

然后点击“部署”即可安装。值得注意的是，插件与插件之间经常存在依赖关系，如果安装失败，则

---
**第 155 页**

149

可以根据日志查看依赖的插件，将依赖插件用同样方法手动下载安装即可。

# 6.4 Jenkins 其它配置

# 6.4.1 Jdk 安装及配置

如果所开发的项目不是jdk17 版本的，我们就需要再安装一个与项目所用版本一样的JDK。如

果选自动安装通常只有在新建的job 中有使用到java 命令，才会触发去下载jdk 安装。这有可能给

我们造成错觉，以为没有安装上，所以这里选择手动安装，然后在jenkins 中进行配置。

安装

下载所需版本的JDK 压缩包，我们选择jdk8 的版本jdk-8u381-linux-x64.tar.gz。

打开终端，进入下载目录。

解压JDK 压缩包到/usr/lib/jvm 目录中，例如sudo tar -xzf jdk-8u381-linux-x64.tar.gz -C /usr/lib/jvm。

解压后会将文件解压到/usr/lib/jvm/jdk1.8.0_381 中。

全局工具配置

选择“系统管理”→“全局工具配置”，找到JDK 安装，进行如图所示的配置。

---
**第 156 页**

150

图 6-12 Jdk 配置

配置环境变量

如果不配置环境变量，在Jenkins 流水线中可能会找不到Java。

配置环境变量，“系统管理”→“系统配置”。

图 6-13 系统配置

进入“系统配置”后向下滚动找到“全局属性”，勾选“环境变量”。

输入键值对：

JAVA_HOME
/usr/lib/jvm/jdk1.8.0_381

---
**第 157 页**

151

图 6-14 Java 环境变量配置

# 6.4.2 Maven 安装及配置

Maven 的安装也选择手动安装，方法与前面的Jdk 安装过程类似。

⚫
安装

下载：https://archive.apache.org/dist/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz

将其拷贝到入Master 结点的/nfs/data/jenkins/目录下，进入目录下解压：

sudo tar -zxvf apache-maven-3.6.3-bin.tar.gz -C /usr/lib
解压后maven 会安装在/usr/lib/apache-maven-3.6.3

⚫
全局工具配置

选择“系统管理”→“全局工具配置”，找到Maven 安装，进行如图所示的配置。

图 6-15 Maven 配置

配置maven 源为阿里源，这样在maven 进行编译打包时可以更快的下载依赖。

---
**第 158 页**

152

进入后台maven 目录下的conf 目录，即/usr/lib/apache-maven-3.6.3/conf 编辑setting.xml。

vim settings.xml

新增以下内容

<!--本地仓库-->

<localRepository>/data/software/repository</localRepository>
<mirrors>

<!--阿里云镜像-->

<mirror>
<id>aliyun-maven</id>
<mirrorOf>central</mirrorOf>
<name>aliyun maven mirror</name>
<url>http://maven.aliyun.com/nexus/content/groups/public/</url>
</mirror>
</mirrors>

图 6-16 配置maven 依赖下载站点

如果想要在系统里直接使用maven，还要对ubuntu 系统的环境变时进行配置，具体配置如下：

⚫
编辑 /etc/profile 文件：

vim /etc/profile

最后写入如下内容：

export MAVEN_HOME=/usr/lib/apache-maven-3.6.3
export CLASSPATH=${MAVEN_HOME}/lib:$CLASSPATH
export PATH=${MAVEN_HOME}/bin:$PATH

执行如下命令：source /etc/profile

执行命令 source /etc/profile 的作用是重新加载 /etc/profile 文件中定义的环境变量和配置。通过

source 命令执行 /etc/profile 文件，可以使得其中定义的环境变量、别名和其他配置在当前 Shell 会

话中生效，而不需要注销并重新登录系统。

---
**第 159 页**

153

通常，/etc/profile 文件用于系统范围的环境变量和配置的设置，例如全局 PATH 变量、系统默

认的别名等。执行 source /etc/profile 命令后，这些配置将被重新加载到当前 Shell 中，使得任何新

开启的终端窗口或执行的命令都能够使用到这些定义的环境变量和配置

总而言之，执行 source /etc/profile 命令可以使得系统范围的环境变量和配置在当前 Shell 中生

效，而无需重新登录系统。

⚫
配置环境变量

如果不配置环境变量，在Jenkins 流水线中可能会找不到maven。

配置环境变量，“系统管理”→“系统配置”。进入“系统配置”后向下滚动找到“全局属性”，勾选“环

境变量”。

输入键值对：

M2_HOME: /var/jenkins_home/apache-maven-3.6.3
PATH+EXTRA: $M2_HOME/bin

如图所示：

图 6-17 Maven 环境变量配置

# 6.4.3 配置 Jenkins Agent（可选的）

配置 Jenkins Agent 的主要目的是为了实现分布式构建和部署，以提高构建和部署的效率和可靠

性。具体来说，配置 Jenkins Agent 的好处包括：

并行构建：通过配置多个 Jenkins Agent，可以同时执行多个构建任务，从而加快构建的速度。

每个 Agent 可以运行在不同的机器上，充分利用硬件资源。

负载均衡：通过将构建任务分配给不同的 Agent，可以均衡负载，避免单个 Agent 被过度使用

而导致性能下降。

灵活性：每个 Agent 可以配置不同的环境和工具，以满足不同的构建需求。例如，可以配置一

---
**第 160 页**

154

个 Agent 来构建 Java 项目，另一个 Agent 来构建 Android 应用。

高可用性：通过配置多个 Agent，即使某个 Agent 发生故障，其他 Agent 仍然可以继续执行构

建任务，确保构建的可用性。

安全性：通过配置 Agent 的权限和访问控制，可以限制 Agent 可以执行的操作和访问的资源，

保护系统的安全性。

总之，配置 Jenkins Agent 可以提高构建和部署的效率、可靠性和安全性，使得整个 CI/CD 流

程更加灵活和可控。

在 Jenkins 的 UI 中，进入 Manage Jenkins -> Manage Nodes and Clouds（节点和云管理），配置

Jenkins Agent。

# 6.5 创建并配置任务

主要步骤包括：

# 1.
添加凭据，通过“添加凭据”来配置代码仓库或镜像仓库等访问的用户名和密码，凭据会

在编写的流水线中用到。

# 2.
配置Jenkins 连接到K8S。

# 3.
创建新任务：在 Jenkins 控制台页面，点击“新建任务”或“新建项目”按钮。然后输入任务的

名称，并选择任务类型（例如自由风格项目、Pipeline 等）。

# 4.
配置任务：进入任务配置页面后，进行任务配置和流水线的编写。

# 6.5.1 添加凭据

在Jenkins 的pipeline 脚本中，可以通过“添加凭据”来配置代码仓库或镜像仓库等访问的用户名

和密码，凭据会在编写的流水线中用到。

# 6.5.1.1 配置代码仓库凭据

这里我们以码云（Gitee）仓库为例，在Jenkins 中配置Gitee 的用户名和密码凭据。

在左侧导航栏中，点击"系统管理"→"凭据管理"。

图 6-18 添加凭据

如上图所示，在"凭据存储"部分，点击"全局"→"添加凭据"，选择"Username with password"。

---
**第 161 页**

155

在"用户名"字段中输入Gitee 的用户名。

在"密码"字段中输入Gitee 的密码。

在"ID"字段中输入一个唯一的凭据ID，例如"gitee-credentials"。其中"ID"如果不输入系统会自动

生成一个，为便于记忆，可以手动输入一个，但要确保不要与其它凭据重复。

在"描述"字段中输入一个描述，例如"Gitee 用户名和密码"。

然后输入代码仓库的用户名及密码然后点击“create”，用同样的方式配置镜像仓库的用户名和密

码。

图 6-19 输入Gitee 访问凭据信息

点击"Create"保存凭据。

# 6.5.1.2 配置镜像仓库访问凭据

在Jenkins 中配置远程镜像仓库的用户名及密码

按上面的方法在编辑框中输入相应信息，然后点“Create”。

---
**第 162 页**

156

图 6-20 输入远程镜像仓库访问凭据

# 6.5.2 配置Jenkins 连接 K8S

如果是k8s 集群则使和本节介绍的方法进行配置。

配置Jenkins 连接Kubernetes（K8s）是为了实现Jenkins 的自动化部署到Kubernetes 集群的功

能。

Jenkins 连接Kubernetes 后，可以通过Jenkins 的Pipeline 或者Job 来进行自动化部署。可以配置

Jenkins 根据代码仓库的变化、定时触发等条件来自动触发部署流程，简化了手动部署的繁琐操作。

# 6.5.2.1 创建kubernetes cloud 并输入基本信息

“系统管理”→“Clouds”→“New Cloud”。填加相应内容：

图 6-21 创建k8s cloud

---
**第 163 页**

157

# 6.5.2.2 生成连接K8S 集群的验证文件

查看k8s 的 /root/.kube/config 文件（如果是k3s 则是这个文件/etc/rancher/k3s/k3s.yaml），其中红

色的部分，省略了大部分内容，这几部分是我们下面生成验证文件要用到的。

cat /root/.kube/config
apiVersion: v1
clusters:
- cluster:
certificate-authority-data: 0tLS1...
server: https://192.168.79.133:6443
name: kubernetes
contexts:
- context:
cluster: kubernetes
user: kubernetes-admin
name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
user:
client-certificate-data: LS0tLS1CRUdJTi...
client-key-data: LS0tLS1...Qo=

生成ca.crt 文件

获取/root/.kube/config 中certificate-authority-data 的内容并转化成base64 encoded 文件，保存在

ca.crt 中

echo  这里是certificate-authority-data 的内容  | base64 -d > ./ca.crt

生成client.crt 和client.key 文件

获取/root/.kube/config 中client-certificate-data 和client-key-data 的内容并转化成base64 encoded

文件

echo  这里是client-certificate-data 的内容  |base64 -d > ./client.crt

echo  这里是client-key-data 的内容  |base64 -d > ./client.key

生成Client P12 认证文件cert.pfx

由于文件生成在k8s 服务器上，要下载至本地才能上传至jenkins 中。

openssl pkcs12 -export -out ./cert.pfx -inkey ./client.key -in ./client.crt -certfile ./ca.crt
Enter Export Password:
Verifying - Enter Export Password:

---
**第 164 页**

158

显示生成的文件：

ll
total 442256
-rw-r--r--  1 root    root         1029 Aug  8 14:30 ca.crt
-rw-r--r--  1 root    root         3125 Aug  8 14:35 cert.pfx
-rw-r--r--  1 root    root         1086 Aug  8 14:33 client.crt
-rw-r--r--  1 root    root         1679 Aug  8 14:34 client.key

# 6.5.2.3 配置jenkins 中生成的k8s 云

在云kubernetes 中添加凭据

图 6-22 在cloud k8s 中填加访问凭据

图 6-23 在cloud k8s 选择证书文件

Upload PKCS#12 certificate 是前面生成并下载至本地的cert.pfx 文件，Password 值添加生成

cert.pfx 文件时输入的密钥

---
**第 165 页**

159

需要安装插件：Credentials Binding Plugin

填加完毕后可以进行连接测试，如果显示了类似下面蓝框中的信息说明连接成功。

图 6-24 测试连接

如果集群是k3s，一定要将文件/etc/rancher/k3s/k3s.yaml 拷贝到/root/.kube 下面，改名为config，

否则连接测试会失败,，通常是类似如下的错误：

... ... IOException : DER input, Integer tag error
执行如下命令进行拷贝：

sudo cp /etc/rancher/k3s/k3s.yaml /root/.kube/config

# 6.5.3 创建新任务

在 Jenkins 控制台页面，点击“新建任务”或“新建项目”按钮。然后输入任务的名称，并选择任务

类型（例如自由风格项目、Pipeline 等）。

这里我们选择“流水线”类型。

---
**第 166 页**

160

图 6-25 创建流水线

# 6.5.4 配置任务

进入所创建的项目后，如下图所示，选择“配置”，打开配置界面进行必要的配置。

图 6-26 配置流水线

“流水线”类型项目配置比较简单，关建是流水线脚本的编写。

# 6.5.4.1 构建触发器

这里要根据具体情况进行相应选择。

---
**第 167 页**

161

图 6-27 选定激活pipeline 的方式

Generic Webhook Trigger: 这是一个通用的Webhook 触发器，用于在收到Webhook 请求时触发

相应操作。

其他工程构建后触发: 当其他工程构建完成后，可以通过此选项触发当前工程的构建操作。

定时构建: 定时构建是指在预设的时间间隔内自动触发构建操作，可以根据固定的时间表或者

Cron 表达式进行设置。

轮询 SCM: SCM (源代码管理) 轮询是指定期间内定时检查源代码管理工具（如Git）以查看是

否有新的提交，并在有新提交时触发构建操作。

静默期: 静默期是指在构建完成后，在一段时间内禁止再次触发构建操作。这个时间段可以用来

防止频繁构建，例如构建结果还没有稳定下来时。

触发远程构建 (例如,使用脚本): 可以通过远程调用的方式触发构建操作，使用脚本或者其他远

程命令来触发构建。

如果你的代码仓库可以访问你的Jenkins 所在机器，那么可以将“Generic Webhook Trigger”。勾

选，实现当代码库有新的提交时，可以通过Webhook 触发Jenkins 构建并执行自动化测试，从而实

现快速反馈和持续交付的流程。

如果Jenkins 是在自己机器上创建的虚拟机上，则不必勾选此项。

Generic Webhook Trigger 是Jenkins 插件中的一种触发器，作用是通过监听和解析Webhook 请求

来触发Jenkins 的构建。

Webhook 是一种机制，用于在特定事件发生时向指定的URL 发送HTTP 请求。通过配置Generic

Webhook Trigger 插件，可以将Jenkins 的构建与外部系统的事件关联起来，当外部系统发送Webhook

请求时，Jenkins 会根据请求的内容触发相应的构建操作。

具体来说，Generic Webhook Trigger 可以实现以下功能：

• 监听外部系统的特定事件，如代码提交、Issue 更新等。

• 解析Webhook 请求中的参数和数据，可以根据请求内容进行条件判断或提取关键信息。

• 根据条件判断结果触发Jenkins 的构建，可以执行编译、测试、部署等操作。

• 将请求中的数据传递给构建过程，实现构建与外部系统的数据交互。

通过使用Generic Webhook Trigger，可以将Jenkins 与其他系统或工具集成，实现自动化构建和

持续集成的需求。例如，当代码库有新的提交时，可以通过Webhook 触发Jenkins 构建并执行自动

化测试，从而实现快速反馈和持续交付的流程。

---
**第 168 页**

162

# 6.5.4.2 组织和配置项目中的文件

为了将微服务项目中的Dockerfile、Kubernetes 的YAML 文件等组织在一起并让Jenkins 流水线

能够访问到，可以按照以下方式进行组织和配置项目中的文件：

# 1.
在每个微服务模块下创建一个名为Dockerfile 的文件，用于定义Docker 镜像的构建规则。

# 2.
在与每个微服务同级的目录下创建一个名为k8s 的文件夹，用于存放部署微服务用的yaml

文件。

# 3.
编写好每个微服务的在k8s 环境中使用的application-k8s.yml。

# 4.
将每个微服务中的application.yml 中的active: dev 改为active: k8s

项目基本结构如下图所示。后面我们编写流水线时就基于这样一个结构来访问所需要的文件。

所有这些文件都要推送到你的代码仓库（比如gitee 或github 等）。

图 6-28 项目结构

---
**第 169 页**

163

# 6.5.4.3 流水线编写

流水线是指一系列的连续操作或任务按照特定的顺序和规则执行的过程。在软件开发中，流水

线常用于描述持续集成和持续交付的过程。

图 6-29 流水线编辑窗口

软件开发中的流水线通常包括以下步骤：

# 1.
获取源代码：从代码仓库中获取最新的源代码。

代码将被拉取到Jenkins 工作目录中的一个默认目录中，该目录的位置取决于你的Jenkins 配置。

一般情况下，默认的工作目录位于
Jenkins
服务器的主文件系统中的

$JENKINS_HOME/workspace/<Pipeline Job Name>目录下，其中<Pipeline Job Name>是你的Pipeline

作业的名称。

例如，如果你的
Pipeline
作业的名称是
my-pipeline ，那么代码将被拉取到

$JENKINS_HOME/workspace/my-pipeline 目录中。

在这个目录中，你将找到你从Gitee 仓库拉取的代码。你可以在后续的构建步骤中使用这些代码

来进行构建、测试或其他操作。例如，你可以使用sh 命令运行Shell 脚本来处理这些代码。

请注意，如果你在Pipeline 脚本中使用了dir 步骤来更改工作目录，那么代码将被拉取到新的工

作目录中。你可以根据自己的需求进行相应的调整。

# 2.
编译：将源代码编译成可执行的程序或库文件。

# 3.
测试：运行单元测试、集成测试和系统测试等各种测试来验证代码的质量和功能性。

# 4.
静态代码分析：使用静态代码分析工具检查代码中的潜在问题和错误。

# 5.
打包：将编译后的代码打包成可部署的软件包或容器镜像。

# 6.
部署：将打包好的软件包或容器镜像部署到目标环境中进行运行。

# 7.
集成和验收测试：在目标环境中运行集成和验收测试，确保软件在真实环境中的运行正常。

# 8.
上线发布：将通过所有测试的软件版本发布到生产环境中供用户使用。

---
**第 170 页**

164

流水线的目的是自动化和标准化软件开发和部署过程，提高开发效率和软件质量，并减少人为

错误的发生。常见的流水线工具包括Jenkins、Travis CI、GitLab CI/CD 等。

下面我们实现其中的几个步骤，包括代码拉取（Checkout）、编译生成jar 包（Build）、生成镜像

（BuildImages）、镜像推送到镜像仓库（Pushimage）、在k8s 系统中部署服务（Deploy），其中给出了

微服务eureka-service、 gateway-service、 product-service 和 product-client 的完整过程。

由于流水线有多个步骤，因此我们可以一个步骤一个步骤的调试，当前面某个步骤已经调试通

过后，在调试后续的步骤时，可以跳已经调试通过的步骤。为了实现这一目的，我们首先设置一些

参数。

选中我们前面创建的项目，然后点击“配置”。

图 6-30 项目操作菜单

进入“配置”后找到“参数化构建过程”，勾选此项，然后点击添加参数。

图 6-31 勾选“参数化构建过程”

---
**第 171 页**

165

针对流水线中的每个阶段定义一个参数，注意：这个参数名称我们在后面的流水线中将要用到，

并根据这个参数的值来决定是否运行该流水线。

我们流水线共有5 个阶段码拉取（Checkout）、编译生成jar 包（Build）、生成镜像（BuildImages）、

镜像推送到镜像仓库（Pushimage）、在k8s 系统中部署服务（Deploy），因此添加5 个参数。

图 6-32 添加参数

下面是Pipeline 示例，其中when{}中的代码是用于决定是否运行该步骤的。

pipeline{
agent any
parameters {

booleanParam(name: 'Checkout', defaultValue: true, description: '拉取代码')

booleanParam(name: 'Build', defaultValue: true, description: '生成jar')

booleanParam(name: 'BuildImages', defaultValue: true, description: '生成镜像')

booleanParam(name: 'Pushimage', defaultValue: true, description: '推送镜像')

booleanParam(name: 'Deploy', defaultValue: true, description: '部署服务')

}
environment {
docker_registry = 'registry.cn-hangzhou.aliyuncs.com'
service_list = 'eureka-service gateway-service product-service product-client'
//version = "${env.BUILD_NUMBER}"

//如果每次根据版本生成镜像，则将下面的去掉，用上面那个

version = "latest"

---
**第 172 页**

166

}
stages{
stage('Checkout'){
when {
expression { params.Checkout == true }
}
steps{
git
credentialsId:
'247b5c50-6d38-4f75-aaef-a5ab2db8dedf',
url:
'https://gitee.com/frankgy/microservice-demo-n.git'

}
}
stage('Build'){
when {
expression { params.Build == true }
}
steps{
sh 'mvn clean package'
}
}
stage('BuildImages') {
when {
expression { params.BuildImages == true }
}
steps {
script {
def work_dir = pwd()
for (service in service_list.split()) {
dir("$work_dir/$service") {
def
image_name
=
"${docker_registry}/my-
stock/${service}:${version}"

echo "${image_name}"
sh "nerdctl build -f Dockerfile -t ${image_name} ."
}
}
}
}
}

---
**第 173 页**

167

stage("Pushimage"){
when {
expression { params.Pushimage == true }
}
steps{
withCredentials([usernamePassword(credentialsId: '898423f1-9aa1-4450-
b8e0-d61f6f204b1e',
passwordVariable:
'ALIYUN_PASSWORD',
usernameVariable:
'ALIYUN_USERNAME')]) {

sh
"nerdctl
login
--username=$ALIYUN_USERNAME
--
password=$ALIYUN_PASSWORD registry.cn-hangzhou.aliyuncs.com"

sh
"nerdctl
push
registry.cn-hangzhou.aliyuncs.com/my-
stock/gateway-service:${version}"

sh "nerdctl push registry.cn-hangzhou.aliyuncs.com/my-stock/eureka-
service:${version}"

sh
"nerdctl
push
registry.cn-hangzhou.aliyuncs.com/my-
stock/product-service:${version}"

sh
"nerdctl
push
registry.cn-hangzhou.aliyuncs.com/my-
stock/product-client:${version}"

}
}
}
stage("Deploy"){
when {
expression { params.Deploy == true }
}
steps{
script{
withCredentials([certificate(aliasVariable: '', credentialsId: '59f161bf-6325-
4a2a-a2f6-38c39275a406', keystoreVariable: 'CERTIFICATE_KEYSTORE', passwordVariable:
'CERTIFICATE_PASSWORD')]) {

def work_dir = pwd()
sh 'echo $work_dir'
dir("$work_dir/k8s"){
sh 'ls'
sh 'kubectl apply -f gateway-service.yaml'
sh 'kubectl apply -f eureka-service.yaml'
sh 'kubectl apply -f product-service.yaml'
sh 'kubectl apply -f product-client.yaml'

---
**第 174 页**

168

}
}
}
}
}
}
}

上面的脚本在运行过程中，路径位于项目的根目录下，通过使用“dir”命令对子目录下的文件进

行操作。

脚本中在创建镜像时使用的文件是
Dockerfile ，其中拷贝文件使用的是

“COPY ./target/${JAR_FILE} ./”，因为在执行命令时当前路径与与target 目录同级，所在要将trget 下

的jar 拷贝到镜像，需要加上路径。

eureka-service 服务的生成镜像的Dockerfile 内容如下，其它几个服务的Dockerfile 与此类似。

FROM openjdk:8-jre-alpine
LABEL maintainer=me
ENV JAVA_ARGS="-Dfile.encoding=UTF8 -Duser.timezone=GMT+08"
ENV JAR_FILE="eureka-service.jar"
COPY ./target/${JAR_FILE} ./
EXPOSE 8888
CMD java ${JAVA_ARGS} ${JAVA_OPTS} -
Deureka.instance.hostname=${MY_POD_NAME}.eureka.stockmgr -jar ${JAR_FILE}

# 6.5.4.4 调试运行流水线

pipeline 脚本编写后，点击保存，回到如下图所示界面，由于前面我们勾选了“参数化构建过程”，

因此操作菜单中的“立即构建”变成了“Build with Parameters”,点击该菜单。

---
**第 175 页**

169

图 6-33 流水线执行过程

点击“Build with Parameters”后出现如下界面：

图 6-34 参数选择界面

此时可以有选择的执行某些步骤，如图我们选择了“Checkout”和“Build”,即运行流水线中的

代码拉取和编译打包。

点击“Build”,稍等片刻后会显示阶段视图。

---
**第 176 页**

170

图 6-35 阶段视图

如果某个阶段出错会显示红色，可以点击“Blue Ocean”查看详细信息。如图会列出每次执行的

信息，点击某一条，可以进入详细信息页面。

图 6-36 选择要查看的某次构建过程

如图 6-37 所示，显示出执行的阶段，可以点击某个阶段，会正下方显示该阶段的执行日志，展

开后可以看到详细信息。如果有错误可以在此查看并分析原因。

---
**第 177 页**

171

图 6-37 构建过程日志信息

或者回到“dashboard”界面，在此界面会显示所有项目，可以直接点击如图所示的按钮运行指定

的项目，来执行流水线。

图 6-38 流水线列表

如图图 6-39 所示可以看到每个stage 的执行情况，其中“Pushimage”这个stage 出错了，下面显

示出了错误原因，找到原因后，重新修改脚本，然后按上面的步骤执行，直到没有错误为止。

---
**第 178 页**

172

图 6-39 显示流水线中Pushimage 错误原因

在 Jenkins 的 UI 中，创建 Pipeline，并编写 Pipeline 的脚本，参考链接：

- https://www.jenkins.io/doc/book/installing/kubernetes/
- https://www.digitalocean.com/community/tutorials/how-to-install-jenkins-on-kubernetes
- https://www.jenkins.io/doc/book/installing/kubernetes/#persistent-storage
上面是手动触发自动化构建的过程。

在Jenkins 中，可以手动触发自动化构建，也可以设置Webhooks 来接收Git 存储库中发生的事

件。Webhooks 可以自动触发Jenkins 的构建，从而自动执行CI/CD 流程。

# 7 前端开发及部署（选做）

云原生项目并不仅限于后端，它是一种综合性的方法和理念，涵盖了整个应用的开发、部署和

运维过程。云原生项目旨在充分利用云计算的优势，提高应用的可扩展性、弹性和可靠性。

云原生项目涉及到后端的容器化、微服务架构和自动化部署等方面，同时它也包括了前端的部

分，比如前端应用的容器化和部署，以及与后端API 接口的交互等。因此，云原生项目是一个全面

的概念，涵盖了前端和后端的方方面面

前端是指用户在浏览器中直接与之交互的部分。它主要包括用户界面的设计和开发，以及与用

户交互的功能实现。前端开发涵盖了HTML、CSS 和JavaScript 等技术，用来构建网页、样式和交互

效果。

在前端部署过程中，需要将前端代码上传到服务器，并配置服务器环境，以便能够通过域名或

IP 地址访问到前端页面。

下面以vue 开发的项目为例，讲一下前端的部署。

# 7.1 vue 项目的创建

---
**第 179 页**

173

# 7.1.1 环境准备

# 7.1.1.1 安装node.js

下载地址：https://nodejs.org/en/download/

界面显示如下，选择要安装的版本，下载并安装。

图 7-1 node.js 下载地址

安装后可以用下面查node.js 版本

node -v

为了提高我们的效率，可以使用淘宝的镜像源：

输入：npm install -g cnpm --registry=https://registry.npm.taobao.org 即可。

安装npm 镜像源以后再用到npm 的地方直接用cnpm 来代替就好了，因为没有镜像源的话，安

装速度比较慢。

检查是否安装成功：cnpm -v

# 7.1.1.2 搭建vue 环境

全局安装vue-cli

这里注意：安装vue-cli 对node.js 的版本是有要求的。

输入cmd 命令，从镜像源下载。

cnpm install -g @vue/cli

查看安装的版本（显示版本号说明安装成功）

vue --version

如果想升级可以用如下命令：

npm update -g @vue/cli

---
**第 180 页**

174

# 7.1.2 vue 项目的创建及项目结构

# 7.1.2.1 vue 项目的创建

vue 项目的创建，可以使用vue 资源管理器创建，也可以直接通过命令创建，下面使用命令来创

建vue 项目。

打开命令行界面，进入想要存放项目的文件夹，输入：

vue create vue01

选择配置信息

通过上下方向键选择对应配置，然后回车

按空格键选择要安装的配置资源，带 * 号说明被选上了。

选择版本

上下方向键选择版本，这里我们选择vue2，然后回车

路径模式选择

这里可以不用管，直接输入 no/n

---
**第 181 页**

175

语法代码格式检查

代码检查，选标准的就行，方向键切换，空格键选择然后回车

代码检查时间，方向键切换，空格键选择然后回车

第三方文件存在的方式：独立的文件、综合的。

是否保存本次配置信息（保存预设）

这里如果选择保存的话，就要输入名字，默认就是文件夹的名字，下次配置的时候就会显示这

个文件配置的选项就像上面的一样，在配置的时候会多出来一个

创建成功

Successfully created project vue01 出现这个说明创建成功

---
**第 182 页**

176

运行：使用cd 命令进入项目文件夹。

cd vue01

输入代码运行文件

npm run serve

启动项目后，在浏览器输入对应的网址就可以看到界面程序运行界面

http://localhost:8080/

---
**第 183 页**

177

停止程序：

按Ctrl+C 然后Y

# 7.1.2.2 vue 项目基本结构

Vue-cli 工程中每个文件夹和文件的用处如下：

dist 文件夹：默认 npm run build 命令打包生成的静态资源文件，用于生产部署

node_modules：存放npm 命令下载的开发环境和生产环境的依赖包

public：有的叫assets：存放项目中需要用到的资源文件，css、js、images 以及index

src 文件夹：存放项目源码及需要引用的资源文件

src-api 文件夹：放ajax 或axios 相关操作的代码文件:index.js(相关的接口),ajax.js(封装的axios,

拦截器)。有的叫service：自己配置的vue 请求后台接口方法

src-components 文件夹：存放vue 开发中抽离的一些公共组件

src-mock 文件夹：mock 数据存放文件及mock 模拟接口（没有后台接口或接口不完整情况下可

以模拟后台接口）

src-pages 文件夹：涉及到路由的组件

src-router 文件夹:vue-router，路由器及路由的配置

src-store 文件夹：存放 vue 中的状态数据，用vuex 集中管理

App.vue 文件：使用标签渲染整个工程的.vue 组件

main.js 文件：vue-cli 工程的入口文件

package.json 文件：用于 node_modules 资源部 和 启动、打包项目的 npm 命令管理

build 文件夹：用于存放 webpack 相关配置和脚本。开发中仅 偶尔使用 到此文件夹下

webpack.base.conf.js 用于配置 less、sass 等css 预编译库，或者配置一下 UI 库

config 文件夹：主要存放配置文件，用于区分开发环境、线上环境的不同，常用到此文件夹下

config.js 配置开发环境的 端口号、是否开启热加载 或者 设置生产环境的静态资源相对路径、是否

开启gzip 压缩、npm run build 命令打包生成静态资源的名称和路径等

# 7.1.2.3 vue 项目如何使用axios 实现前后端通信

要在Vue 项目中使用axios 实现前后端通信，可以按照以下步骤进行：

在项目中安装axios：可以使用npm 命令来安装axios，例如：npm install axios。

在Vue 组件中引入axios：可以在需要使用axios 的Vue 组件中引入axios，例如：import axios

from 'axios'。

发送HTTP 请求：使用axios 发送HTTP 请求到后端API。可以使用axios 提供的方法，如

axios.get(url)、axios.post(url, data)等来发送GET 或POST 请求。例如：

发送get 请求：

---
**第 184 页**

178

axios.get('/product/findByProductId/88')
.then(response => {

// 处理后端返回的数据

})
.catch(error => {

// 处理错误

});
axios.get('/product/queryAllProduct')
.then(response => {

// 处理后端返回的数据

})
.catch(error => {

// 处理错误

});

处理后端返回的数据：在then 回调函数中处理后端返回的数据。可以根据后端API 的返回数据

格式进行处理，例如解析JSON 数据、处理错误信息等。

向后端发送POST 请求：如果需要向后端发送POST 请求，可以使用axios.post(url, data)方法。

其中url 为后端API 的URL 地址，data 为要发送的数据。

发送post 请求

axios.post('/product/queryAllProduct', {
token: mytoken
})
.then(response => {

// 处理后端返回的数据

})
.catch(error => {

// 处理错误

});

在Java 后端中，可以使用Spring Boot 来处理HTTP 请求并与前端进行通信。具体的代码编写可

以按照以下步骤进行：

创建Controller 类：在后端项目中创建一个Controller 类，用于处理前端发送的HTTP 请求。可

以使用@RestController 注解来标识该类为Controller，并使用@RequestMapping 注解来指定URL 路

径。

例如：

@AllArgsConstructor
@RestController
@Slf4j
@RequestMapping("/product")

---
**第 185 页**

179

public class ProductServiceClientController {
@Autowired
private final ProductServiceClient prodServiceClient;
/**

* 根据商品id 查询商品

*/
@GetMapping("/findByProductId/{productId}")
public Product findByProductId(@PathVariable Long productId) {
Product product = prodServiceClient.findByProductId(productId);
return product;
}
/**

* 查询所有商品

*/
@GetMapping("/queryAllProduct")
public List<Product> findByProductId() {
List<Product> productList = prodServiceClient.queryAllProduct();
return productList;
}
}

注意：如果在网关Gateway 中已经将请求前缀"/product"过滤掉了，那么上面的代码中要去掉这

条语句：@RequestMapping("/product")。

配置CORS 跨域请求：如果前端和后端部署在不同的域名或端口下，可能需要进行CORS 跨域

请求配置。

跨域可以在后端通过配置解决，也可以在前端解决，下面是后端解决跨域的方法。

可以在后端的配置类中添加@CrossOrigin 注解来实现。例如：

@Configuration
public class CorsConfig implements WebMvcConfigurer {
@Override
public void addCorsMappings(CorsRegistry registry) {
registry.addMapping("/product/**")
.allowedOrigins("http://localhost:8080")
.allowedMethods("GET", "POST");
}
}

---
**第 186 页**

180

# 7.2 vue 项目在云原生环境中的部署

# 7.2.1 前端的部署方式的选择

在云原生项目中，前端应用的容器化是可选的，而不是必需的。前端应用通常是一个静态的网

页，可以直接部署到云服务器上，而无需使用容器技术。

然而，在某些情况下，将前端应用容器化可能会带来一些好处：

(1). 环境一致性：通过将前端应用容器化，可以确保在不同的环境中具有一致的运行环境，减

少了由于环境差异导致的问题。

(2). 扩展性和弹性：使用容器技术可以实现前端应用的水平扩展和负载均衡，以应对高流量和

并发访问。

(3). 便于部署和管理：容器化使得前端应用可以和其他部分一起使用容器编排工具（如

Kubernetes）进行自动化部署和管理。

因此，前端应用的容器化在云原生项目中并非必需，但在某些情况下它可能是有益的。具体是

否将前端应用容器化，取决于项目的需求和具体情况。

# 7.2.2 前端非容器方式部署

# 7.2.2.1 在部署的机器上安装nodejs

指定安装装12.x 版本，nodejs 版本最好与开发环境的一致。

curl -sL https://deb.nodesource.com/setup_12.x | sudo bash -
sudo apt update
sudo apt install nodejs

查看版本：

node -v

如果安装的nodejs 有问题可以采用下面方式进行卸载：

执行如下命令：

sudo apt remove nodejs

此命令会卸载 nodejs，但是会保留配置文件，方便你以后再次安装 nodejs。

如果不想保留配置文件，继续执行：

sudo apt purge nodejs

这将会卸载 nodejs 和其相关的配置文件。

最后，你还可以移除和 nodejs 一起安装但是现在没有被使用的包：

sudo apt autoremove

# 7.2.2.2 在部署机器上安装nginx

执行如下命令安装nginx：

apt install nginx

目录说明

---
**第 187 页**

181

/usr/sbin/nginx：主程序，启动文件

/etc/nginx：存放配置文件

/var/www/html：存放项目目录，vue 项目放在此处

/var/log/nginx：存放日志

一般自动安装配置文件目录和主程序目录不变，因为版本原因，其它目录可能会变，但是都可

以从配置文件里ngxin.conf 里找到对应的位置。

nginx 管理命令：

查看版本：nginx -v

启动服务：service nginx start

重新启动nginx 服务：service nginx restart

停止nginx:service nginx stop

# 7.2.2.3 部署vue 项目

(1). 构建 Vue 项目

在 Vue 项目的根目录下执行以下命令，将项目构建为静态文件：

npm run build
这将生成一个 dist 目录，里面包含了构建后的静态文件。

在/var/www/html下创建部署文件存放目录stock，然后将dist目录下的所有文件考入stock目录，

注意是dist 目录内的文件，不包含目录名dist。

目录结构为：

├─var
│  ├─www
│  │  ├─html
│  │  │  └─stock
│  │  │      ├─index.html
│  │  │      ├─static
│  │  │           └─...

(2). 配置 Nginx

打开 Nginx 的配置文件，通常是 /etc/nginx/nginx.conf 在 server 块中将配置修改为如下形式：

server {
listen 80 ;
root /var/www/html/stock;
index index.html;
location / {
try_files $uri $uri/ /index.html;

---
**第 188 页**

182

}
}
如果涉及跨域，则需要进行进一步设置，详细设置如下：

这个配置是在nginx 中设置一个路径为/product 的location，作用是将访问路径为/product 的请求

转发给http://192.168.79.133:30099 这个地址上的服务器处理，并在响应头中添加Access-Control-

Allow-Origin 字段，允许跨域访问。

具体配置解释如下：

add_header ‘Access-Control-Allow-Origin’ '';：这一行配置是在响应头中添加一个Access-Control-

Allow-Origin 字段，并设置其值为，表示允许任何来源的请求访问该路径下的资源。这个配置是为了

解决跨域访问的问题。

proxy_pass http://192.168.79.133:30099;：这一行配置是将所有访问该路径下的请求转发给后端服

务器，即将请求代理到http://192.168.79.133:30099 这个地址上处理。这个配置是用来实现反向代理

的功能，将请求转发给其他服务器处理。

---
**第 189 页**

183

修改配置后要重新启动nginx 才能使配置生效。

执行以下命令重启 Nginx 服务，使配置生效：

sudo service nginx restart

或者

sudo systemctl restart nginx

现在，你可以通过访问服务器的 IP 地址或域名来查看部署的 Vue 项目了。

# 7.2.3 前端容器方式的部署

在Kubernetes 的云原生环境中，将Vue 前端代码进行打包部署可以按照以下步骤进行：

(1). 构建前端应用：在Vue 项目的根目录下，运行npm run build 命令来构建前端应用。这将生

成一个用于生产环境的静态文件目录（默认为dist 目录），包含所有打包后的HTML、CSS 和JavaScript

文件等。

(2). 创建Docker 镜像：在项目的根目录下创建一个名为Dockerfile 的文件，用于定义前端应用

的容器化构建规则一个简单的Dockerfile 示例下：

# 使用nginx 作为基础镜像

nginx:1.21.1

# 删除默认的nginx 配置

RUN rm /etc/nginx/conf.d/default.conf

# 将打包后的静态文件复制到nginx 的默认静态文件目录

COPY dist /usr/share/html

# 复制自定义nginx 配置文件到容器中

COPY nginx.conf /etc/nginx.d

# 暴露容器的80 端口

EXPOSE 80

# 启动nginx 服务

CMD ["nginx", "-g",daemon off;"]
在个例子中，我们使用了NGX 作为容器的基镜像，并将打包后的静态文件复制到NGINX 默认

静态文件目录。同时，我们还可以自定义NGINX 的配置文件（nginx.conf）并将其复制到容器中。

(3). 构建Docker 镜像：在终端中进入项目的根目录，运行以下命令来构建Docker 镜像。将your-

image-name 替换为你希望的镜像名称。

docker build -t your-image-name .

(4). 推送镜像到镜像仓库（可选）：如果你的Kubernetes 集群使用了镜像仓库，你可以将构建好

的Docker 镜像推送到镜像仓库中，以便在Kubernetes 中使用。具体的镜像推送方式可以参考你所使

用的镜像仓库的文档。

---
**第 190 页**

184

(5). 部署前端应用：在Kubernetes 中部署前端应用可以使用Kubernetes 的资源配置文件（如

Deployment、Service 等）来描述应用的部署和暴露方式。一个简单的Deployment 示例如下：

apiVersion: apps/v1
kind: Deployment
metadata:
name: your-deployment-name
spec:
replicas: 1
selector:
matchLabels:
app: your-app-label
template:
metadata:
labels:
app: your-app-label
spec:
containers:
- name: your-container-name
image: your-image-name
ports:
- containerPort: 80

在这个例子中，我们定义了一个Deployment 用于部署前端应用。其中，your-deployment-name

是Deployment 的名称，your-app-label 是应用的标签，your-container-name 是容器的名称，your-image-

name 是之前构建的Docker 镜像的名称。

(6). 应用部署：使用kubectl 或其他Kubernetes 管理工具，执行以下命令来部署前端应用。

kubectl apply -f your-deployment-file.yaml

your-deployment-file.yaml 是前面定义的Deployment 配置文件的路径。

通过以上步骤，你就可以将Vue 前端应用打包成Docker 镜像，并在Kubernetes 的云原生环境中

进行部署。需要注意的是，具体的配置和命令可能会根据你的项目和环境有所调整。

---
**第 191 页**

185

# 8 附录

# 8.1 K8S 卸载重装流程

kubeadm reset (所有结点都要执行)

rm -rf /etc/kubernetes
rm -rf /var/lib/etcd/
rm -rf $HOME/.kube

# 8.2 批量导出镜像脚本export_images.sh：

要使用nerdctl 导出不带<none>标签的镜像，可以使用以下脚本：

# 列出所有镜像

images=$(nerdctl images -a --format "{{.Repository}}:{{.Tag}}")

# 循环遍历镜像列表

for image in $images
do

# 判断镜像是否带有none 标签

if [[ $image != *"<none>"* ]]; then

# 导出镜像

echo $image
nerdctl image save $image -o $(echo $image | tr ':/' '_').tar
fi
done

执行脚本后，将会导出所有不带<none>标签的镜像，并以镜像名称和标签命名对应的tar 文件。

例如，calico/apiserver:v3.27.0 的镜像将被导出为calico_apiserver_v3.27.0.tar 文件。

请确保已经安装并正确配置了nerdctl。

# 8.3 出错的解决

# 8.3.1 删除k8s

注意，所有节点都要执行：

删除 $HOME./kube/config 文件

执行kubeadm reset 命令

# 8.3.2 Jenkins 中Git 拉取代码出错

出现类似如下的错误时，进入
Jenkins
的工作空间找到项目目录，如下图

/var/lib/jenkins/workspace 是jenkins 的工作空间，把其下面的项目目录（这里是StockMgr）以及有引

命字的相关目录全部删除，然后再重新执行流水线。

---
**第 192 页**

186

> git rev-parse --resolve-git-dir /var/lib/jenkins/workspace/StockMgr/.git # timeout=10
Fetching changes from the remote Git repository
> git config remote.origin.url https://gitee.com/frankgy/cloudnativetest.git # timeout=10
ERROR: Error fetching remote repo 'origin'
... ...
Caused
by:
hudson.plugins.git.GitException:
Command
"git
config
remote.origin.url
https://gitee.com/frankgy/cloudnativetest.git" returned status code 128:

stdout:
stderr: fatal: not in a git directory

# 8.3.3 常用命令

# 1. kubectl get pods: 获取集群中的所有pod 列表

示例：kubectl get pods

# 2. kubectl describe pod : 查看某个pod 的详细信息

示例：kubectl describe pod my-pod

# 3. kubectl create deployment --image=: 创建一个部署（deployment）

示例：kubectl create deployment nginx-deployment --image=nginx:latest

# 4. kubectl scale deployment --replicas=: 缩放部署的副本数

示例：kubectl scale deployment nginx-deployment --replicas=3

# 5. kubectl expose deployment --port= --target-port=: 暴露一个部署的服务

示例：kubectl expose deployment nginx-deployment --port=80 --target-port=8080

# 6. kubectl delete pod : 删除一个pod

示例：kubectl delete pod my-pod

如果删除不掉，可以强制删除，确定要删除的 Pod 的名称pod-name 和命名空间namespace 后，

使用以下命令强制删除该 Pod：

kubectl delete pod <pod-name> -n <namespace> --grace-period=0 –force

# 7. kubectl logs : 查看一个pod 的日志

示例：kubectl logs my-pod

# 8. kubectl exec -it – : 在一个pod 上执行命令

示例：

kubectl exec -it my-pod – bash
kubectl exec -it nginx-test-795d659f45-bw7qz bash -n kube-public

# 9. kubectl apply -f : 从一个YAML 文件中创建或更新资源

示例：kubectl apply -f my-deployment.yaml

# 10. kubectl get nodes: 获取集群中的所有节点列表

示例：kubectl get nodes

# 11.使用以下命令列出所有镜像的id 和标签

docker images --format "{{.ID}} {{.Repository}}:{{.Tag}}"

---
**第 193 页**

187

其它：

查看kubelet 状态：systemctl status kubelet

查看安装日志：journalctl -xeu kubelet

如果安装出错可以用kubeadm reset 复位后再新安装。

可以通过下面命令查看所安装k8s 相关组件版本。

kubeadm config images pull
kubectl describe pod -n namespacename coredns-66f779496c-f6gn7

# 8.3.4 镜像仓库操作

如果不想将仓库公开，可以点击上图中左侧的“访问凭证”进行设置，设置后还要做如下工作才

能拉取镜像。

（1）.在master 结点创建secret

命令如下：

kubectl create secret docker-registry regcred --docker-server=your-registry-server --docker-
username=your-username  --docker-password=your-password  --docker-email=your-email

请替换 your-registry-server、your-username、your-password 和 your-email 为你私有镜像仓库的

相关信息。

regcred 为创建的secret 的名字。这个名字要与部署pod 的yaml 文件中“imagePullSecrets:”下面

指定的名字一致。

（2）.在部署的yaml 文件中指定设置好的secret

进行必要的配置，否则会出现权限不足的错误。

# 8.3.5 项目基本结构

示例项目基本结构如下图所示：

---
**第 194 页**

188

测试：

# 127.0.0.1:9999/product/queryAllProduct?token=1 中127.0.0.1:9999 为gateway 地址，加了认证token=1

---
**第 195 页**

189

注意修改：portal-service 下的orderList.js 和productList.js 中的网关地址。

---
**第 196 页**

190

# 8.3.6 k8s 安装时如何重新生成证书？

在Kubernetes 中，证书是用于认证和加密通信的重要组件。如果需要重新生成证书，可以按照

以下步骤操作：

停止 Kubernetes 集群中的所有组件，包括 kube-apiserver、kube-controller-manager、kube-

scheduler、kubelet 和 kube-proxy。

在每个节点上删除旧的证书文件。证书文件通常位于 /etc/kubernetes/pki 目录下，可以使用以下

命令删除：
sudo rm /etc/kubernetes/pki/*.crt
sudo rm /etc/kubernetes/pki/*.key

重新生成证书。可以使用工具如 kubeadm、kubeadm-certs 或 cfssl 进行证书生成。以下是使

用 kubeadm 命令重新生成证书的示例：
sudo kubeadm init phase certs all --apiserver-advertise-address=<apiserver_address>

其中 <apiserver_address> 是你的 Kubernetes API Server 的地址。

将新生成的证书文件复制到每个节点的 /etc/kubernetes/pki 目录下。可以使用以下命令复制：
sudo cp /etc/kubernetes/pki/*.crt /etc/kubernetes/pki/*.key /etc/kubernetes/pki/etcd/
sudo chown root:root /etc/kubernetes/pki/etcd/*.crt /etc/kubernetes/pki/etcd/*.key

启动 Kubernetes 集群中的所有组件。可以使用以下命令启动：
sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler kubelet kube-
proxy

重新生成证书后，Kubernetes 集群中的组件将使用新的证书进行通信和认证。请确保在重新生

成证书之前备份原有证书，以防需要恢复。

# 8.3.7 YAML 常见资源对象的字段

当编写Kubernetes 中的YAML 文件时，不同资源对象可能有不同的字段，以下是一些常见资
源对象的字段以及它们的含义：

# 1. 通用字段（metadata 字段通常适用于所有资源对象）：

o
apiVersion: 指定使用的Kubernetes API 版本

o
kind: 指定资源对象的类型，如Pod、Service、Deployment 等

o
metadata: 元数据，包括名称、命名空间、标签、注释等

▪
name: 资源对象的名称

▪
namespace: 资源对象所属的命名空间

▪
labels: 用于标识和选择资源对象的标签

▪
annotations: 元数据的注释信息

# 2. Pod 对象的字段：

o
spec: 规格，包括容器、卷、调度策略等

▪
containers: 容器列表，定义了Pod 中的一个或多个容器

▪
name: 容器的名称

---
**第 197 页**

191

▪
image: 容器的镜像

▪
ports: 容器的端口配置

▪
containerPort: 容器监听的端口号

▪
volumeMounts: 挂载到容器内部的卷配置

▪
name: 卷的名称

▪
mountPath: 卷在容器内的挂载路径

▪
volumes: 定义Pod 的卷配置

▪
name: 卷的名称

▪
emptyDir: 空目录卷

▪
hostPath: 主机路径卷

▪
secret: 密文卷

▪
configMap: 配置映射卷

# 3. Service 对象的字段：

o
spec: 规格，包括端口、选择器等

▪
ports: 定义Service 暴露的端口

▪
port: Service 暴露的端口号

▪
targetPort: 用于接收流量的后端Pod 的端口号

▪
selector: 用于选择目标Pod 的标签选择器

# 4. Deployment 对象的字段：

o
spec: 规格，包括副本数、模板、策略等

▪
replicas: 副本数量

▪
selector: 用于选择要控制的Pod 的标签选择器

▪
template: 定义创建的Pod 的模板

▪
metadata: Pod 的元数据

▪
spec: Pod 的规格，包括容器、卷等配置

这些字段是Kubernetes 资源对象中常见的字段，但实际使用时可能会根据具体的需求和资源对
象类型有所不同。在编写YAML 文件时，需要根据所需的配置和资源对象类型来正确使用这些字
段。

---
**第 198 页**

192

# 8.3.8 Feign 服务调用时的错误

# 8.3.8.1 找不到服务错误信息如下

This application has no explicit mapping for /error, so you are seeing this as a fallback.
Mon Jan 29 09:23:33 CST 2024
There was an unexpected error (type=Internal Server Error, status=500).
Request
processing
failed;
nested
exception
is
java.lang.RuntimeException:
com.netflix.client.ClientException: Load balancer does not have available server for client: product-service

原因1：服务“product-service”没有正常启动

原因2：服务“product-service”名字错误，没有这个服务

原因3：配置文件错误，Client 中就为fetch-registry: true ，如查写忝false 则不行。

原因4：服务没有问题，只是Eureka 注册延时。

# 8.3.8.2 Eureka 服务注册延时问题解决（新服务上线了，Eureka-Client 服务不能及时获取到）

1、Eureka_Client 应用实例异常挂掉了，但是没有能在挂掉前告知Eureka-Server 服务，所以

Eureka-Server 服务没有下线挂掉的Eureka-Client 服务实例信息。解决该问题可以依赖Eureka-Server

的 EvictionTask 去剔除已下线的Eureka-client 服务实例信息。

可以在Eureka-Server 服务中调整EvictionTask 的调度频率，比如将调度间隔从默认的60 秒，调

整为5 秒，即添加以下配置：

eureka:
server:

---
**第 199 页**

193

eviction-interval-timer-in-ms: 5000

# 2.Eureka-Client 应用实例下线时告知Eureka-Server 了，但是 Eureka-Server 的 REST API 有

response cache 缓存，所以需要等待缓存过期后才能更新。

解决方案：

可以根据情况考虑在Eureka-Server 服务中关闭readOnlyCacheMap，即修改或添加以下配置：

eureka:
server:
use-read-only-response-cache: false

或者调整 readWriteCacheMap 的过期时间，即修改或添加以下配置：

eureka:
server:
response-cache-auto-expiration-in-seconds: 60

# 3.Eureka-Server 服务由于开启了Self Preservation 模式（自我保护模式），导致注册列表（registry）

的信息不会因为过期而被剔除，直到退出自我保护模式（Self Preservation）。

解决方案：

在测试环境中可在Eureka-Server 服务中关闭自我保护模式，即修改或添加以下配置：

eureka:
server:
enable-self-preservation: false

在生产环境下可以在Eureka-Server 服务中把leaseRenewalIntervalInSeconds 和 renewal-percent-

threshold 参数调小，从而提高触发自我保护机制的门槛，即修改或添加以下配置：

eureka:
server:

renewal-percent-threshold: 0.49  ## 指定每分钟需要收到的续约次数的阀值，默认值为

# 0.85

eureka:
instance:

leaseRenewalIntervalInSeconds: 10  # 默认值为30

# 1.新服务上线了，Eureka-Client 服务不能及时获取到。

Eureka 客户端的设置

原因1：Eureka Client 注册延迟

Eureka Client 启动后，不是立即向Eureka Server 注册的，而是有一个延迟向服务端注册的时间。

通过跟踪源码，可以发现默认的延迟时间为40 秒。

解决方案：

将实例信息变更同步到 Eureka Server 的初始延迟时间，从默认的40 秒修改到10 秒，即修改或

添加以下配置：

eureka:
client:

---
**第 200 页**

194

## InstanceInfoReplicator 将实例信息变更同步到 Eureka Server 的初始延迟时间 ，默认为40

秒

initial-instance-info-replication-interval-seconds: 10

原因2： Eureka Client 缓存

Eureka Client 保留注册表信息的缓存。该缓存每30 秒更新一次。故Eureka Client 刷新本地缓存

并发现其他新注册的实例可能需要30 秒。

解决方案：

在测试环境下，可以在Eureka-Client 服务中适当提高 Eureka-Client 端拉取 Server 注册信息的

频率，比如将默认频率由30 秒改为5 秒，即修改或添加以下配置：

eureka:
client:
registry-fetch-interval-seconds: 5

# 8.3.9 Could not get lock /var/lib/dpkg/lock-frontend. It is held by proc

这个错误通常是由于另一个进程正在使用apt 或dpkg 工具，因此无法获得对/var/lib/dpkg/lock-

frontend 文件的锁定。

这种情况可能是因为正在进行软件包管理操作，如安装、升级或删除软件包。这可能是由于正

在运行的软件包管理工具或后台的自动更新进程导致的。

为了解决这个问题，可以尝试等待一段时间，让其他软件包管理操作完成，然后再次尝试执行

相同的操作。如果确定没有其他软件包管理操作在运行，可以尝试手动删除锁定文件，然后再次运

行命令。命令如下

sudo rm /var/lib/dpkg/lock-frontend
sudo rm /var/lib/dpkg/lock
sudo dpkg --configure -a

# 8.3.10 本指导书所用的镜像版本

calico/apiserver:v3.27.0
calico/cni:v3.27.0
calico/csiv:3.27.0
calico/kube-controllers:v3.27.0
calico/node-driver-registrar:v3.27.0
calico/node:v3.27.0
calico/pod2daemon-flexvol:v3.27.0
registry.aliyuncs.com/google_containers/coredns:v1.10.1
registry.aliyuncs.com/google_containers/etcd:3.5.9-0
registry.aliyuncs.com/google_containers/kube-apiserver:v1.28.2
registry.aliyuncs.com/google_containers/kube-controller-manager:v1.28.2
registry.aliyuncs.com/google_containers/kube-proxy:v1.28.2
registry.aliyuncs.com/google_containers/kube-scheduler:v1.28.2
registry.aliyuncs.com/google_containers/pause:3.9

---
**第 201 页**

195

导入镜像脚本：

# 定义tar 文件列表和对应的镜像名和标签列表

tar_files=(
"calico_apiserver_v3.27.0.tar"
"calico_cni_v3.27.0.tar"
"calico_csi_v3.27.0.tar"
"calico_kube-controllers_v3.27.0.tar"
"calico_node-driver-registrar_v3.27.0.tar"
"calico_node_v3.27.0.tar"
"calico_pod2daemon-flexvol_v3.27.0.tar"
"calico_typha_v3.27.0.tar"
"mysql_8.0.26.tar"
"quay.io_tigera_operator_v1.32.3.tar"
"registry.aliyuncs.com_google_containers_coredns_v1.10.1.tar"
"registry.aliyuncs.com_google_containers_kube-proxy_v1.28.2.tar"
"registry.aliyuncs.com_google_containers_pause_3.9.tar"
)
image_tags=(
"calico/apiserver:v3.27.0"
"calico/cni:v3.27.0"
"calico/csiv:3.27.0"
"calico/kube-controllers:v3.27.0"
"calico/node-driver-registrar:v3.27.0"
"calico/node:v3.27.0"
"calico/pod2daemon-flexvol:v3.27.0"
"registry.aliyuncs.com/google_containers/coredns:v1.10.1"
"registry.aliyuncs.com/google_containers/etcd:3.5.9-0"
"registry.aliyuncs.com/google_containers/kube-apiserver:v1.28.2"
"registry.aliyuncs.com/google_containers/kube-controller-manager:v1.28.2"
"registry.aliyuncs.com/google_containers/kube-proxy:v1.28.2"
"registry.aliyuncs.com/google_containers/kube-scheduler:v1.28.2"
"registry.aliyuncs.com/google_containers/pause:3.9"
)

# 循环处理tar 文件列表

for ((i=0; i<${#tar_files[@]}; ++i)); do

# 导入镜像

nerdctl load -i ${tar_files[i]}
done

---
**第 202 页**

196

# 8.3.11 基础镜像设置

注：基础镜像是harbor 仓库引用需要创建如下配置才能构建镜像

1、mkdir -p /etc/buildkit/

2、cat /etc/buildkit/buildkitd.toml

[registry."10.66.1.10"]
http = true
insecure = true

3、重启服务

systemctl restart buildkitd

# 8.3.12 Eureka 启动异常，提示Freemarker Template Error

FreeMarker template error (DEBUG mode; use RETHROW in production!): The following has
evaluated to null or missing: ==> replica.key [in template "eureka/navbar.ftlh" at line 68, column 62] ---- Tip:
It's the step after the last dot that caused this error, not those before it. ---- Tip: If the failing expression is
known to legally refer to something that's sometimes null or missing, either

更多的异常就不写了，总之是 Freemarker 的异常。

这个应该是 spring boot 的配置文件 ，euraka 的 defaultZone 这个配置写的不规范。

注意： 这个 defaultZone 要求绝对规范才行，否则就会报 Freemarker 的异常。

曾经发现的两个错误写法：

# 1  defaultZone: localhost:10000/eureka

应该是   defaultZone: http://localhost:10000/eureka

# 2  defaultZone: http://eurekaServer2:8762/eureka,http://eurekaServer3:8763/eureka,

多个 Zone ，最后多写了一个 逗号， 去掉最后的逗号就好了。

# 8.3.13 Ubuntu 关机异常慢的解决方法

提示 a stop job is running for session 3 of user ubuntu (1 min 30 s)

解决：

编辑/etc/systemd/system.conf

---
**第 203 页**

197

把 #DefaultTimeoutStopSec=90s 先去掉 #

然后改成想要的时间，如5s，即 DefaultTimeoutStopSec=5s，保存退出

最后重新加载服务的配置文件

sudo systemctl daemon-reload

# 8.3.14 springcloud gateway 找不到服务

查看日志提示这种错误，java.net.UnknownHostException: product-client-5b94d857bb-dn28n。

是主机名有DNS 问题。在kubernetes 中，POD 的DNS 名称的格式通常如下：pod-ip-

address.deployment-name.my-namespace.svc.cluster-domain.example
（
参
见

https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pods）

可以通过使用IP 地址来解决这个问题：将eureka.instance.preferIpAddress 设置为true

# 8.3.15 如何暂时停止k8s

在master 结点运行：

sudo systemctl stop kube-apiserver
sudo systemctl stop kube-controller-manager
sudo systemctl stop kube-scheduler
sudo systemctl stop kube-proxy

在worker 结点上运行：

sudo systemctl stop kubelet
sudo systemctl stop kube-proxy

# 8.3.16 如何重启动k8s

在master 结点运行：

systemctl start kube-apiserver
sudo systemctl start kube-controller-manager
sudo systemctl start kube-scheduler
sudo systemctl start kubelet
sudo systemctl start kube-proxy

在worker 结点上运行：

---
**第 204 页**

198

sudo systemctl start kubelet
sudo systemctl start kube-proxy

# 8.3.17 Jenkins 忘记密码|密码重置

⚫
修改配置文件#

vim  /var/lib/jenkins/config.xml

把字段 useSecurity 的值修改为 false，如下

<useSecurity>false</useSecurity>

⚫
重启 Jenkins 服务

systemctl restart jenkins

⚫
然后重新进入再 修改用户密码#

打开前台首页，依次进入系统管理 -> 安全 -> 全局安全配置，在“认证（Authentication）”->

安全域 -> 选择“Jenkins 专有用户数据库”，取消勾选“允许用户注册”，在授权策略 -> 选择“登

录用户可以做任何事”，取消“匿名用户具有可读权限”，完成后点“保存”。

---
**第 205 页**

199

# 8.4 其它

# 8.4.1 清除镜像

清理无用数据,这个指令十分可怕，它不是和Docker 那样只是把标签为"none"的镜像清理掉，而

是把所有没有"正在使用"的镜像清理了，比如我用来编译的mave 和node 镜像。至于清理volumes 这

个指令，我暂时还没有勇气执行它。

nerdctl system prune -h
--------------------------
Remove unused data
Usage: nerdctl system prune [flags]
Flags:
-a, --all       Remove all unused images, not just dangling ones
-f, --force     Do not prompt for confirmation
-h, --help      help for prune
--volumes   Prune volumes

# 8.4.2 buildkitd 帐号配置

buildkitd 的registry 的帐号密码配置在 ~/.docker/config.json 文件中 , 沿用了Docker 的配置,虽

然我们并没有装Docker

{
"auths": {

---
**第 206 页**

200

"docker.io": {
"auth": "base64(username:password)"
}
}
}

# 8.4.3 nerdctl 登录帐号

nerdctl login --insecure-registry harbor.cncf.net

# 8.4.4 配置buildkitd 配置文件，添加镜像仓库使用http 访问

root@master1:/dockerfile# mkdir /etc/buildkit/
root@master1:/dockerfile# vim /etc/buildkit/buildkitd.toml
[registry."harbor.cncf.net"]
http = true
insecure = true

# 8.4.5 containerd 配置国内镜像源

配置国内镜像源

vi /etc/containerd/config.toml

https://------.mirror.aliyuncs.com 私有镜像仓库位址请自行阿里云获取

[plugins."io.containerd.grpc.v1.cri".registry]
[plugins."io.containerd.grpc.v1.cri".registry.mirrors]
[plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
endpoint = ["registry.aliyuncs.com/google_containers"]

使用私有镜像仓库

[plugins."io.containerd.grpc.v1.cri".registry]
[plugins."io.containerd.grpc.v1.cri".registry.mirrors]
[plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]

endpoint = ["https://registry-1.docker.io"] //到此为配置文件默认生成，之后为需

要添加的内容

[plugins."io.containerd.grpc.v1.cri".registry.mirrors."192.168.66.4"]
endpoint = ["https://192.168.66.4:443"]
[plugins."io.containerd.grpc.v1.cri".registry.configs]
[plugins."io.containerd.grpc.v1.cri".registry.configs."192.168.66.4".tls]
insecure_skip_verify = true
[plugins."io.containerd.grpc.v1.cri".registry.configs."192.168.66.4".auth]
username = "admin"
password = "Harbor12345"

---
**第 207 页**

201

systemctl daemon-reload
systemctl restart containerd
systemctl status containerd

# 8.4.6 排错命令

# 8.4.6.1  查看日志：kubectl logs

您可以查看 pod 的所有容器的日志：

kubectl logs mypod --all-containers
或者指定的容器：

kubectl logs mypod -c mycontainer

日志可能会显示有用的信息。

# 8.4.6.2 查看事件：kubectl get events

可以列出相关的事件：

kubectl get events
或者，您可以使用以下命令列出单个 Pod 的所有事件：

kubectl get events --field-selector involvedObject.name=mypod

请注意，此信息也出现在describe pod 输出的底部。

# 8.4.6.3 检查部署：kubectl describe deployment

您可以通过以下方式获取此信息：

kubectl describe deployment mydeployment

如果deployment 定义了所需的 Pod 状态，它可能包含导致 CrashLoopBackOff 的错误配置。

结合起来看

在下面的示例中，您可以看到如何挖掘日志，在其中发现命令参数中的错误。

获取配置文件:

---
**第 208 页**

202

kubectl get deployment -n stockmgr    gateway-service -o yaml > nt.yaml

# 8.4.7 MySQL8 报错：Public Key Retrieval is not allowed

mysql 8.0 默认使用 caching_sha2_password 身份验证机制 （即从原来mysql_native_password

更改为 caching_sha2_password。）

从 5.7 升级 8.0 版本的不会改变现有用户的身份验证方法，但新用户会默认使用新的

caching_sha2_password 。 客户端不支持新的加密方式。 修改用户的密码和加密方式。

方案一：

在命令行模式下进入mysql，输入以下命令:

ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'root';

或者

ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY 'root';

然后就可以正常连接了。

方案二：

在配置数据源的时候直接将属性allowPublicKeyRetrieval 设置为true 即可

# 8.4.8 清除pod 日志（不太好用）

kubectl logs <pod 名称> --tail=0 > /dev/null

# 8.4.9 pom 标签

这是一个 Maven 的 Project Object Model (POM) 文件，它是 Maven 项目的基本单元，包含了

项目的基本信息和项目的构建信息。下面是各个标签的作用：

<modelVersion>：POM 模型的版本，当前版本是4.0.0。

<groupId>：项目的组织或者公司的唯一标识符，通常使用公司的域名反转。

<artifactId>：项目的唯一标识符，通常是项目的名称。

<version>：项目的版本。

<modules>：包含的子项目列表。

<name>：项目的名称。

---
**第 209 页**

203

<packaging>：项目的打包方式，如 jar、war、pom 等。

<description>：项目的描述信息。

<properties>：定义一些属性，可以在 POM 文件中引用。

<dependencyManagement>：用于集中管理项目中的依赖版本，避免在子模块中重复定义。

<dependencies>：项目的依赖列表。

<build>：构建项目的相关信息，如最终名、资源、插件等。

<repositories>：项目依赖的仓库列表，Maven 会从这些仓库中下载依赖的 jar 包。

<pluginManagement>：用于集中管理插件的版本和配置。

<plugins>：项目使用的插件列表。

在这个 POM 文件中，还使用了一些变量，如 ${springboot.version}、${project.name}、

${maven.compiler.target}、${maven.compiler.source} 等，这些变量可以在 <properties> 标签中定义，

也可以在命令行中传入。

# 8.4.10 配置文件属性含义

spring.cloud.gateway.discovery.locator.enabled: true 表示开启从注册中心获取转发地址的功能。

spring.cloud.gateway.discovery.locator.lower-case-service-id: true 表示将服务id 转换为小写。

spring.cloud.gateway.routes 是一个路由配置的列表，每个路由配置包括以下信息：

id：路由的唯一标识符。

uri：指定转发的目标地址，lb://product-client 表示负载均衡到名为 product-client 的服务实例。

filters：过滤器列表，用于对请求进行预处理或后处理。

predicates：断言列表，用于匹配请求的条件。

eureka.instance.prefer-ip-address: true 表示在Eureka 注册中心上注册时，使用IP 地址而不是主机

名。

eureka.client.register-with-eureka: true 表示将应用程序自身注册到Eureka 服务器。

eureka.client.fetch-registry: true 表示从Eureka 服务器获取注册表信息。

eureka.client.service-url.defaultZone: http://127.0.0.1:8888/eureka 是Eureka 服务器的地址。

# 8.4.11 控制器的作用

具体来说，控制器的作用包括：

接收请求：控制器通过定义不同的请求路径和请求方法来接收不同的HTTP 请求。

参数解析：控制器可以从请求中获取参数，并将其解析为合适的Java 对象，以便后续的处理。

调用服务：控制器可以调用其他服务、业务逻辑或数据访问层的方法，完成具体的业务处理。

返回结果：控制器将处理结果封装为HTTP 响应，并返回给前端或其他服务。这可以是HTML

页面、JSON 数据、文件等不同的形式。

异常处理：控制器可以处理请求过程中可能出现的异常，返回相应的错误信息给前端或其他服

---
**第 210 页**

204

务。

总之，控制器在Spring Cloud 项目中起到了连接前端与后端服务的作用，负责接收和处理请求，

调用相应的服务完成业务逻辑，并返回处理结果。

# 8.4.12 系统启动时很慢提示A start job is running for wait for network to be Configured

cd /etc/systemd/system/network-online.target.wants/

查看该文件夹，该文件夹下有个文件，systemd-networkd-wait-online.service

加上TimeoutStartSec=2sec

如下图所示：

# 8.4.13 错误现像ContainerCreating

创建pod 时卡再某些节点创建Pod 会一直卡在ContainerCreating 的状态无法顺利创建并且就绪

Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for
sandbox...error getting ClusterInformation: connection is unauthorized: Unauthorized

解决方式：

进去删除异常节点的calico-node 容器，让它拉起重新同步数据即可修复

# 8.4.14 IOException : DER input, Integer tag error

问题原因

私钥格式存在问题。

解决方案

RSA 私钥没有转 pkcs8 或者配置的私钥格式不对。

Java 语言需要使用到 pkcs8 私钥，如果单独调试 app 支付客户端 demo，客户端上配置的私钥也是

需要 pkcs8 私钥。

其他编程语言例如 php、.net、python 都使用 pkcs1 私钥。详情请参见 如何区分密钥格式类型。

---
**第 211 页**

205

# 8.4.15 镜像源

网易：http://hub-mirror.c.163.com

中科大镜像地址：http://mirrors.ustc.edu.cn/

中科大github 地址：https://github.com/ustclug/mirrorrequest

Azure 中国镜像地址：http://mirror.azure.cn/

Azure 中国github 地址：https://github.com/Azure/container-service-for-azure-china

DockerHub 镜像仓库: https://hub.docker.com/

阿里云镜像仓库： https://cr.console.aliyun.com

google 镜像仓库： https://console.cloud.google.com/gcr/images/google-containers/GLOBAL

coreos 镜像仓库： https://quay.io/repository/

RedHat 镜像仓库： https://access.redhat.com/containers

渡渡鸟镜像同步站 (aityp.com) ：https://docker.aityp.com/

三个chart 时的目录结构：

my-parent-chart/
|__ Chart.yaml
|__ values.yaml
|__ charts/
|   |__ my-first-subchart/
|   |   |__ Chart.yaml
|   |   |__ values.yaml
|   |   |__ templates/
|   |   |   |__ deployment.yaml
|   |   |   |__ service.yaml
|   |__ my-second-subchart/
|   |   |__ Chart.yaml
|   |   |__ values.yaml
|   |   |__ templates/
|   |   |   |__ deployment.yaml
|   |   |   |__ service.yaml
|   |__ my-third-subchart/
|   |   |__ Chart.yaml
|   |   |__ values.yaml
|   |   |__ templates/
|   |   |   |__ deployment.yaml
|   |   |   |__ service.yaml
|__ templates/
|__ parent-deployment.yaml

|__ parent-service.yaml

---
**第 212 页**

206

在这个目录结构中：

my-parent-chart/ 是父chart 的根目录。

Chart.yaml 文件包含了父chart 的元数据信息。

values.yaml 文件包含了全局的配置参数。

charts/ 目录包含了各个子chart。

每个子chart 目录中包含了 Chart.yaml 文件和 values.yaml 文件，用于定义子chart 的元数据和

特有的配置参数。

每个子chart 目录中的 templates/ 目录中包含了子chart 的部署文件模板。

parent-deployment.yaml 和 parent-service.yaml 是父chart中的模板文件，用于渲染和配置子chart

的部署文件。

通过这样的目录结构，可以清晰地组织和管理多个子chart，方便地定义全局和特有的配置参数，

并实现各个子chart 的部署和配置。在部署整个chart 时，Helm 将会按照依赖关系逐个安装和配置

每个子chart，实现整个应用的部署。

要将涉及 PV、PVC、ConfigMap、Secret、Deployment、Service 等 YAML 文件组织到一个

Helm Chart 中，可以按照以下步骤进行：

创建一个新的 Helm Chart 项目，可以使用 helm create 命令生成一个基本的 chart 结构。

在 templates/ 目录下创建各种资源的模板文件，例如：

deployment.yaml：用于定义 MySQL 的 Deployment。

service.yaml：用于定义 MySQL 的 Service。

pv.yaml：用于定义 PersistentVolume。

pvc.yaml：用于定义 PersistentVolumeClaim。

configmap.yaml：用于定义 ConfigMap。

secret.yaml：用于定义 Secret。

在每个模板文件中，使用 Helm 模板语言来引用配置参数和值。可以从 values.yaml 文件中获

取全局配置参数，也可以在每个资源的模板文件中定义特定的配置参数。

在 values.yaml 文件中定义全局的配置参数，包括 MySQL 的镜像名称、数据库用户名密码、

存储配置等。也可以在 values.yaml 文件中定义 PV、PVC、ConfigMap、Secret 的相关参数。

在 Chart.yaml 文件中定义 Chart 的元数据信息，包括 Chart 的名称、版本、描述等。

---
**第 213 页**

207

可以在 templates/ 目录下创建一个 helpers.tpl 文件，用于存放一些公用的模板函数或者变量，

以便在各个模板文件中复用。

最后，使用 Helm 命令将整个 Chart 打包，并通过 helm install 命令部署到 Kubernetes 集群中。

通过以上步骤，可以将涉及 PV、PVC、ConfigMap、Secret、Deployment、Service 等 YAML 文

件组织到一个 Helm Chart 中，实现一键部署 MySQL 服务，并且可以通过配置参数定制化部署过

程。这样可以简化部署流程，提高部署的一致性，并方便管理依赖关系。

Helm Visualizer：Helm Visualizer 是一个开源的工具，提供了一个交互式可视化界面，可以帮助

用户查看和编辑 Helm Charts。用户可以通过界面拖拽操作，添加和编辑 Charts 的各种属性，同时

还可以查看 Charts 之间的依赖关系。

Helm UI：Helm UI 是 Helm 的官方用户界面，提供了一个基于 Web 的图形化界面，可以方便

地管理 Helm Charts。用户可以通过 Helm UI 来安装、升级、回滚 Charts，并查看 Charts 的详细信

息。

Octant：Octant 是一个开源的 Kubernetes Dashboard 工具，也提供了对 Helm Charts 的支持。

用户可以在 Octant 中查看 Helm Charts 的状态、配置参数，并管理 Helm Releases。

Lens：Lens 是一个功能强大的 Kubernetes IDE 工具，支持 Helm Charts 的管理和编辑。用户

可以在 Lens 中直观地查看 Helm Charts 的结构、配置参数，并方便地部署和管理 Helm Releases。

这些可视化编辑工具为用户提供了更直观、更便捷的方式来创建和管理 Helm Charts，有助于加

快开发和部署流程。用户可以根据自己的需求选择适合的工具来提高工作效率。

